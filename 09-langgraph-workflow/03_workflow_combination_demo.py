"""
LangGraph Workflow ç»„åˆå½¢å¼æ¼”ç¤º

è¿™ä¸ªæ–‡ä»¶å±•ç¤ºäº† LangGraph ä¸­å¤šç§ workflow ç»„åˆå½¢å¼ï¼ŒåŒ…æ‹¬ï¼š
1. å¹¶è¡Œå·¥ä½œæµ
2. ä¸²è¡Œå·¥ä½œæµé“¾
3. æ¡ä»¶åˆ†æ”¯å·¥ä½œæµ
4. å¾ªç¯å·¥ä½œæµ
5. åµŒå¥—å·¥ä½œæµ
6. é”™è¯¯æ¢å¤å·¥ä½œæµ
7. è¶…æ—¶æ§åˆ¶å·¥ä½œæµ
"""

import asyncio
from typing import Dict, List, Any, Optional, Annotated
from datetime import datetime, timedelta
from dataclasses import dataclass
from enum import Enum
import random

from langgraph.graph import StateGraph, END
from langgraph.checkpoint.memory import MemorySaver


class WorkflowType(Enum):
    """å·¥ä½œæµç±»å‹æšä¸¾"""
    PARALLEL = "parallel"
    SEQUENTIAL = "sequential"
    CONDITIONAL = "conditional"
    LOOP = "loop"
    NESTED = "nested"
    ERROR_RECOVERY = "error_recovery"
    TIMEOUT = "timeout"


@dataclass
class WorkflowState:
    """å·¥ä½œæµçŠ¶æ€å®šä¹‰"""
    workflow_type: Annotated[WorkflowType, "å·¥ä½œæµç±»å‹"]
    input_data: Annotated[Dict[str, Any], "è¾“å…¥æ•°æ®"]
    intermediate_results: Annotated[Dict[str, Any], "ä¸­é—´ç»“æœ"]
    current_step: Annotated[str, "å½“å‰æ­¥éª¤"]
    execution_history: Annotated[List[Dict[str, Any]], "æ‰§è¡Œå†å²"]
    error_info: Annotated[Optional[Dict[str, Any]], "é”™è¯¯ä¿¡æ¯"] = None
    is_completed: Annotated[bool, "æ˜¯å¦å®Œæˆ"] = False
    timeout_at: Annotated[Optional[datetime], "è¶…æ—¶æ—¶é—´"] = None


class WorkflowCombinationDemo:
    """å·¥ä½œæµç»„åˆæ¼”ç¤ºç±»"""
    
    def __init__(self):
        self.workflows = {}
    
    def create_parallel_workflow(self) -> StateGraph:
        """åˆ›å»ºå¹¶è¡Œå·¥ä½œæµ - ç®€åŒ–ä¸ºä¸²è¡Œæ‰§è¡Œä»¥é¿å…å¹¶å‘å†²çª"""
        print("ğŸ”„ åˆ›å»ºå¹¶è¡Œå·¥ä½œæµ...")
        
        graph = StateGraph(WorkflowState)
        
        # æ·»åŠ èŠ‚ç‚¹
        graph.add_node("data_processing", self._data_processing_node)
        graph.add_node("model_inference", self._model_inference_node)
        graph.add_node("external_api_call", self._external_api_node)
        
        # è®¾ç½®å…¥å£ç‚¹
        graph.set_entry_point("data_processing")
        
        # ä¸²è¡Œæ‰§è¡Œä»¥é¿å…å¹¶å‘å†²çª
        graph.add_edge("data_processing", "model_inference")
        graph.add_edge("model_inference", "external_api_call")
        graph.add_edge("external_api_call", END)
        
        return graph.compile()
    
    def create_sequential_workflow(self) -> StateGraph:
        """åˆ›å»ºä¸²è¡Œå·¥ä½œæµé“¾ - ä»»åŠ¡æŒ‰é¡ºåºæ‰§è¡Œ"""
        print("ğŸ”„ åˆ›å»ºä¸²è¡Œå·¥ä½œæµé“¾...")
        
        graph = StateGraph(WorkflowState)
        
        # æ·»åŠ ä¸²è¡ŒèŠ‚ç‚¹
        graph.add_node("step_1_preprocessing", self._preprocessing_node)
        graph.add_node("step_2_analysis", self._analysis_node)
        graph.add_node("step_3_enhancement", self._enhancement_node)
        graph.add_node("step_4_finalization", self._finalization_node)
        
        # è®¾ç½®ä¸²è¡Œæ‰§è¡Œé¡ºåº
        graph.set_entry_point("step_1_preprocessing")
        graph.add_edge("step_1_preprocessing", "step_2_analysis")
        graph.add_edge("step_2_analysis", "step_3_enhancement")
        graph.add_edge("step_3_enhancement", "step_4_finalization")
        graph.add_edge("step_4_finalization", END)
        
        return graph.compile()
    
    def create_conditional_workflow(self) -> StateGraph:
        """åˆ›å»ºæ¡ä»¶åˆ†æ”¯å·¥ä½œæµ - æ ¹æ®æ¡ä»¶é€‰æ‹©ä¸åŒè·¯å¾„"""
        print("ğŸ”„ åˆ›å»ºæ¡ä»¶åˆ†æ”¯å·¥ä½œæµ...")
        
        graph = StateGraph(WorkflowState)
        
        # æ·»åŠ èŠ‚ç‚¹
        graph.add_node("initial_analysis", self._initial_analysis_node)
        graph.add_node("complex_processing", self._complex_processing_node)
        graph.add_node("simple_processing", self._simple_processing_node)
        graph.add_node("final_output", self._final_output_node)
        
        # è®¾ç½®å…¥å£ç‚¹
        graph.set_entry_point("initial_analysis")
        
        # æ¡ä»¶åˆ†æ”¯
        def route_based_on_complexity(state: WorkflowState) -> str:
            """æ ¹æ®æŸ¥è¯¢å¤æ‚åº¦è·¯ç”±"""
            complexity = state.intermediate_results.get("complexity_score", 0)
            
            if complexity > 0.7:
                return "complex"
            else:
                return "simple"
        
        graph.add_conditional_edges(
            "initial_analysis",
            route_based_on_complexity,
            {
                "complex": "complex_processing",
                "simple": "simple_processing"
            }
        )
        
        # åˆå¹¶è·¯å¾„
        graph.add_edge("complex_processing", "final_output")
        graph.add_edge("simple_processing", "final_output")
        graph.add_edge("final_output", END)
        
        return graph.compile()
    
    def create_loop_workflow(self) -> StateGraph:
        """åˆ›å»ºå¾ªç¯å·¥ä½œæµ - é‡å¤æ‰§è¡Œç›´åˆ°æ»¡è¶³æ¡ä»¶"""
        print("ğŸ”„ åˆ›å»ºå¾ªç¯å·¥ä½œæµ...")
        
        graph = StateGraph(WorkflowState)
        
        # æ·»åŠ èŠ‚ç‚¹
        graph.add_node("iteration_step", self._iteration_node)
        graph.add_node("final_result", self._final_result_node)
        
        # è®¾ç½®å…¥å£ç‚¹
        graph.set_entry_point("iteration_step")
        
        # å¾ªç¯æ§åˆ¶
        def check_iteration_completion(state: WorkflowState) -> str:
            """æ£€æŸ¥è¿­ä»£æ˜¯å¦å®Œæˆ"""
            iteration_count = state.intermediate_results.get("iteration_count", 0)
            max_iterations = state.input_data.get("max_iterations", 5)
            
            if iteration_count >= max_iterations:
                return "complete"
            else:
                return "continue"
        
        graph.add_conditional_edges(
            "iteration_step",
            check_iteration_completion,
            {
                "continue": "iteration_step",  # å¾ªç¯å›åˆ°è‡ªèº«
                "complete": "final_result"
            }
        )
        
        graph.add_edge("final_result", END)
        
        return graph.compile()
    
    def create_nested_workflow(self) -> StateGraph:
        """åˆ›å»ºåµŒå¥—å·¥ä½œæµ - å·¥ä½œæµä¸­åŒ…å«å­å·¥ä½œæµ"""
        print("ğŸ”„ åˆ›å»ºåµŒå¥—å·¥ä½œæµ...")
        
        graph = StateGraph(WorkflowState)
        
        # æ·»åŠ ä¸»å·¥ä½œæµèŠ‚ç‚¹
        graph.add_node("main_processing", self._main_processing_node)
        graph.add_node("sub_workflow_executor", self._sub_workflow_executor_node)
        graph.add_node("result_aggregation", self._result_aggregation_node)
        
        # è®¾ç½®æ‰§è¡Œé¡ºåº
        graph.set_entry_point("main_processing")
        graph.add_edge("main_processing", "sub_workflow_executor")
        graph.add_edge("sub_workflow_executor", "result_aggregation")
        graph.add_edge("result_aggregation", END)
        
        return graph.compile()
    
    def create_error_recovery_workflow(self) -> StateGraph:
        """åˆ›å»ºé”™è¯¯æ¢å¤å·¥ä½œæµ - è‡ªåŠ¨å¤„ç†é”™è¯¯å¹¶é‡è¯•"""
        print("ğŸ”„ åˆ›å»ºé”™è¯¯æ¢å¤å·¥ä½œæµ...")
        
        graph = StateGraph(WorkflowState)
        
        # æ·»åŠ èŠ‚ç‚¹
        graph.add_node("main_task", self._main_task_node)
        graph.add_node("error_handler", self._error_handler_node)
        graph.add_node("fallback_task", self._fallback_task_node)
        graph.add_node("final_output", self._final_output_node)
        
        # è®¾ç½®å…¥å£ç‚¹
        graph.set_entry_point("main_task")
        
        # é”™è¯¯æ£€æµ‹å’Œè·¯ç”±
        def check_for_errors(state: WorkflowState) -> str:
            """æ£€æŸ¥æ˜¯å¦æœ‰é”™è¯¯"""
            if state.error_info:
                return "error"
            else:
                return "success"
        
        graph.add_conditional_edges(
            "main_task",
            check_for_errors,
            {
                "error": "error_handler",
                "success": "final_output"
            }
        )
        
        # é”™è¯¯å¤„ç†è·¯å¾„
        def decide_recovery_strategy(state: WorkflowState) -> str:
            """å†³å®šæ¢å¤ç­–ç•¥"""
            error_type = state.error_info.get("error_type", "unknown")
            
            if error_type == "retryable":
                return "retry"
            else:
                return "fallback"
        
        graph.add_conditional_edges(
            "error_handler",
            decide_recovery_strategy,
            {
                "retry": "main_task",  # é‡è¯•ä¸»ä»»åŠ¡
                "fallback": "fallback_task"
            }
        )
        
        graph.add_edge("fallback_task", "final_output")
        graph.add_edge("final_output", END)
        
        return graph.compile()
    
    def create_timeout_workflow(self) -> StateGraph:
        """åˆ›å»ºè¶…æ—¶æ§åˆ¶å·¥ä½œæµ - é™åˆ¶ä»»åŠ¡æ‰§è¡Œæ—¶é—´"""
        print("ğŸ”„ åˆ›å»ºè¶…æ—¶æ§åˆ¶å·¥ä½œæµ...")
        
        graph = StateGraph(WorkflowState)
        
        # æ·»åŠ èŠ‚ç‚¹
        graph.add_node("timed_task", self._timed_task_node)
        graph.add_node("timeout_handler", self._timeout_handler_node)
        graph.add_node("normal_completion", self._normal_completion_node)
        
        # è®¾ç½®å…¥å£ç‚¹
        graph.set_entry_point("timed_task")
        
        # è¶…æ—¶æ£€æŸ¥
        def check_timeout(state: WorkflowState) -> str:
            """æ£€æŸ¥æ˜¯å¦è¶…æ—¶"""
            if state.timeout_at and datetime.now() > state.timeout_at:
                return "timeout"
            else:
                return "normal"
        
        graph.add_conditional_edges(
            "timed_task",
            check_timeout,
            {
                "timeout": "timeout_handler",
                "normal": "normal_completion"
            }
        )
        
        graph.add_edge("timeout_handler", END)
        graph.add_edge("normal_completion", END)
        
        return graph.compile()
    
    # ========== èŠ‚ç‚¹å‡½æ•°å®ç° ==========
    
    def _data_processing_node(self, state: WorkflowState) -> WorkflowState:
        """æ•°æ®å¤„ç†èŠ‚ç‚¹"""
        print("ğŸ“Š æ‰§è¡Œæ•°æ®å¤„ç†...")
        state.intermediate_results["data_processed"] = True
        state.execution_history.append({"step": "data_processing", "timestamp": datetime.now()})
        return state
    
    def _model_inference_node(self, state: WorkflowState) -> WorkflowState:
        """æ¨¡å‹æ¨ç†èŠ‚ç‚¹"""
        print("ğŸ¤– æ‰§è¡Œæ¨¡å‹æ¨ç†...")
        state.intermediate_results["model_inference_completed"] = True
        state.execution_history.append({"step": "model_inference", "timestamp": datetime.now()})
        return state
    
    def _external_api_node(self, state: WorkflowState) -> WorkflowState:
        """å¤–éƒ¨APIè°ƒç”¨èŠ‚ç‚¹"""
        print("ğŸŒ è°ƒç”¨å¤–éƒ¨API...")
        state.intermediate_results["api_call_completed"] = True
        state.execution_history.append({"step": "external_api_call", "timestamp": datetime.now()})
        return state
    
    def _aggregate_results_node(self, state: WorkflowState) -> WorkflowState:
        """èšåˆç»“æœèŠ‚ç‚¹"""
        print("ğŸ“ˆ èšåˆå¹¶è¡Œæ‰§è¡Œç»“æœ...")
        state.intermediate_results["results_aggregated"] = True
        state.execution_history.append({"step": "aggregate_results", "timestamp": datetime.now()})
        return state
    
    def _preprocessing_node(self, state: WorkflowState) -> WorkflowState:
        """é¢„å¤„ç†èŠ‚ç‚¹"""
        print("ğŸ”§ æ‰§è¡Œé¢„å¤„ç†...")
        state.intermediate_results["preprocessing_done"] = True
        state.execution_history.append({"step": "preprocessing", "timestamp": datetime.now()})
        return state
    
    def _analysis_node(self, state: WorkflowState) -> WorkflowState:
        """åˆ†æèŠ‚ç‚¹"""
        print("ğŸ” æ‰§è¡Œåˆ†æ...")
        state.intermediate_results["analysis_done"] = True
        state.execution_history.append({"step": "analysis", "timestamp": datetime.now()})
        return state
    
    def _enhancement_node(self, state: WorkflowState) -> WorkflowState:
        """å¢å¼ºèŠ‚ç‚¹"""
        print("âœ¨ æ‰§è¡Œå¢å¼ºå¤„ç†...")
        state.intermediate_results["enhancement_done"] = True
        state.execution_history.append({"step": "enhancement", "timestamp": datetime.now()})
        return state
    
    def _finalization_node(self, state: WorkflowState) -> WorkflowState:
        """æœ€ç»ˆåŒ–èŠ‚ç‚¹"""
        print("âœ… æ‰§è¡Œæœ€ç»ˆåŒ–...")
        state.intermediate_results["finalization_done"] = True
        state.execution_history.append({"step": "finalization", "timestamp": datetime.now()})
        return state
    
    def _initial_analysis_node(self, state: WorkflowState) -> WorkflowState:
        """åˆå§‹åˆ†æèŠ‚ç‚¹"""
        print("ğŸ” æ‰§è¡Œåˆå§‹åˆ†æ...")
        # æ¨¡æ‹Ÿè®¡ç®—å¤æ‚åº¦åˆ†æ•°
        complexity = random.uniform(0.1, 1.0)
        state.intermediate_results["complexity_score"] = complexity
        state.execution_history.append({"step": "initial_analysis", "timestamp": datetime.now()})
        return state
    
    def _complex_processing_node(self, state: WorkflowState) -> WorkflowState:
        """å¤æ‚å¤„ç†èŠ‚ç‚¹"""
        print("ğŸ§  æ‰§è¡Œå¤æ‚å¤„ç†...")
        state.intermediate_results["complex_processing_done"] = True
        state.execution_history.append({"step": "complex_processing", "timestamp": datetime.now()})
        return state
    
    def _simple_processing_node(self, state: WorkflowState) -> WorkflowState:
        """ç®€å•å¤„ç†èŠ‚ç‚¹"""
        print("âš¡ æ‰§è¡Œç®€å•å¤„ç†...")
        state.intermediate_results["simple_processing_done"] = True
        state.execution_history.append({"step": "simple_processing", "timestamp": datetime.now()})
        return state
    
    def _final_output_node(self, state: WorkflowState) -> WorkflowState:
        """æœ€ç»ˆè¾“å‡ºèŠ‚ç‚¹"""
        print("ğŸ“¤ ç”Ÿæˆæœ€ç»ˆè¾“å‡º...")
        state.intermediate_results["final_output_generated"] = True
        state.execution_history.append({"step": "final_output", "timestamp": datetime.now()})
        return state
    
    def _iteration_node(self, state: WorkflowState) -> WorkflowState:
        """è¿­ä»£èŠ‚ç‚¹"""
        iteration_count = state.intermediate_results.get("iteration_count", 0)
        iteration_count += 1
        state.intermediate_results["iteration_count"] = iteration_count
        print(f"ğŸ”„ æ‰§è¡Œç¬¬ {iteration_count} æ¬¡è¿­ä»£...")
        state.execution_history.append({"step": f"iteration_{iteration_count}", "timestamp": datetime.now()})
        return state
    
    def _final_result_node(self, state: WorkflowState) -> WorkflowState:
        """æœ€ç»ˆç»“æœèŠ‚ç‚¹"""
        print("ğŸ ç”Ÿæˆæœ€ç»ˆç»“æœ...")
        state.intermediate_results["final_result_generated"] = True
        state.execution_history.append({"step": "final_result", "timestamp": datetime.now()})
        return state
    
    def _main_processing_node(self, state: WorkflowState) -> WorkflowState:
        """ä¸»å¤„ç†èŠ‚ç‚¹"""
        print("ğŸ—ï¸ æ‰§è¡Œä¸»å¤„ç†...")
        state.intermediate_results["main_processing_done"] = True
        state.execution_history.append({"step": "main_processing", "timestamp": datetime.now()})
        return state
    
    def _sub_workflow_executor_node(self, state: WorkflowState) -> WorkflowState:
        """å­å·¥ä½œæµæ‰§è¡Œå™¨èŠ‚ç‚¹"""
        print("ğŸ”— æ‰§è¡Œå­å·¥ä½œæµ...")
        state.intermediate_results["sub_workflow_executed"] = True
        state.execution_history.append({"step": "sub_workflow_executor", "timestamp": datetime.now()})
        return state
    
    def _result_aggregation_node(self, state: WorkflowState) -> WorkflowState:
        """ç»“æœèšåˆèŠ‚ç‚¹"""
        print("ğŸ“Š èšåˆç»“æœ...")
        state.intermediate_results["results_aggregated"] = True
        state.execution_history.append({"step": "result_aggregation", "timestamp": datetime.now()})
        return state
    
    def _main_task_node(self, state: WorkflowState) -> WorkflowState:
        """ä¸»ä»»åŠ¡èŠ‚ç‚¹"""
        print("ğŸ¯ æ‰§è¡Œä¸»ä»»åŠ¡...")
        # æ¨¡æ‹Ÿéšæœºé”™è¯¯
        if random.random() < 0.3:  # 30% æ¦‚ç‡æ¨¡æ‹Ÿé”™è¯¯
            state.error_info = {
                "error_type": "retryable",
                "error_message": "æ¨¡æ‹Ÿå¯é‡è¯•é”™è¯¯",
                "timestamp": datetime.now()
            }
        state.intermediate_results["main_task_done"] = True
        state.execution_history.append({"step": "main_task", "timestamp": datetime.now()})
        return state
    
    def _error_handler_node(self, state: WorkflowState) -> WorkflowState:
        """é”™è¯¯å¤„ç†èŠ‚ç‚¹"""
        print("ğŸ› ï¸ å¤„ç†é”™è¯¯...")
        state.execution_history.append({"step": "error_handler", "timestamp": datetime.now()})
        return state
    
    def _fallback_task_node(self, state: WorkflowState) -> WorkflowState:
        """å¤‡ç”¨ä»»åŠ¡èŠ‚ç‚¹"""
        print("ğŸ”„ æ‰§è¡Œå¤‡ç”¨ä»»åŠ¡...")
        state.intermediate_results["fallback_task_done"] = True
        state.execution_history.append({"step": "fallback_task", "timestamp": datetime.now()})
        return state
    
    def _timed_task_node(self, state: WorkflowState) -> WorkflowState:
        """å®šæ—¶ä»»åŠ¡èŠ‚ç‚¹"""
        print("â° æ‰§è¡Œå®šæ—¶ä»»åŠ¡...")
        # è®¾ç½®è¶…æ—¶æ—¶é—´
        if not state.timeout_at:
            state.timeout_at = datetime.now() + timedelta(seconds=2)
        state.intermediate_results["timed_task_done"] = True
        state.execution_history.append({"step": "timed_task", "timestamp": datetime.now()})
        return state
    
    def _timeout_handler_node(self, state: WorkflowState) -> WorkflowState:
        """è¶…æ—¶å¤„ç†èŠ‚ç‚¹"""
        print("â±ï¸ å¤„ç†è¶…æ—¶...")
        state.intermediate_results["timeout_handled"] = True
        state.execution_history.append({"step": "timeout_handler", "timestamp": datetime.now()})
        return state
    
    def _normal_completion_node(self, state: WorkflowState) -> WorkflowState:
        """æ­£å¸¸å®ŒæˆèŠ‚ç‚¹"""
        print("âœ… æ­£å¸¸å®Œæˆä»»åŠ¡...")
        state.intermediate_results["normal_completion_done"] = True
        state.execution_history.append({"step": "normal_completion", "timestamp": datetime.now()})
        return state


def demo_workflow_combinations():
    """æ¼”ç¤ºå¤šç§å·¥ä½œæµç»„åˆå½¢å¼"""
    print("ğŸš€ LangGraph Workflow ç»„åˆå½¢å¼æ¼”ç¤º")
    print("=" * 60)
    
    demo = WorkflowCombinationDemo()
    
    # æ¼”ç¤º1: å¹¶è¡Œå·¥ä½œæµ
    print("\nğŸ“‹ æ¼”ç¤º1: å¹¶è¡Œå·¥ä½œæµ")
    print("-" * 40)
    parallel_workflow = demo.create_parallel_workflow()
    initial_state = WorkflowState(
        workflow_type=WorkflowType.PARALLEL,
        input_data={"query": "å¹¶è¡Œå¤„ç†æµ‹è¯•"},
        intermediate_results={},
        current_step="start",
        execution_history=[]
    )
    result = parallel_workflow.invoke(initial_state)
    print(f"âœ… å¹¶è¡Œå·¥ä½œæµæ‰§è¡Œå®Œæˆ")
    print(f"ğŸ“Š æœ€ç»ˆç»“æœ: {result}")
    
    # æ¼”ç¤º2: ä¸²è¡Œå·¥ä½œæµé“¾
    print("\nğŸ“‹ æ¼”ç¤º2: ä¸²è¡Œå·¥ä½œæµé“¾")
    print("-" * 40)
    sequential_workflow = demo.create_sequential_workflow()
    initial_state = WorkflowState(
        workflow_type=WorkflowType.SEQUENTIAL,
        input_data={"text": "è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•æ–‡æœ¬"},
        intermediate_results={},
        current_step="start",
        execution_history=[]
    )
    result = sequential_workflow.invoke(initial_state)
    print(f"âœ… ä¸²è¡Œå·¥ä½œæµæ‰§è¡Œå®Œæˆ")
    print(f"ğŸ“Š æœ€ç»ˆç»“æœ: {result}")
    
    # æ¼”ç¤º3: æ¡ä»¶åˆ†æ”¯å·¥ä½œæµ
    print("\nğŸ“‹ æ¼”ç¤º3: æ¡ä»¶åˆ†æ”¯å·¥ä½œæµ")
    print("-" * 40)
    conditional_workflow = demo.create_conditional_workflow()
    
    # æµ‹è¯•ä¸åŒæ¡ä»¶
    test_cases = [
        ("simple", "ç®€å•æŸ¥è¯¢"),
        ("complex", "å¤æ‚åˆ†æè¯·æ±‚"),
        ("unknown", "å…¶ä»–ç±»å‹æŸ¥è¯¢")
    ]
    
    for query_type, query_text in test_cases:
        initial_state = WorkflowState(
            workflow_type=WorkflowType.CONDITIONAL,
            input_data={"query": query_text, "query_type": query_type},
            intermediate_results={},
            current_step="start",
            execution_history=[]
        )
        
        result = conditional_workflow.invoke(initial_state)
        print(f"ğŸ” æŸ¥è¯¢ç±»å‹ '{query_type}': {result}")
    
    # æ¼”ç¤º4: å¾ªç¯å·¥ä½œæµ
    print("\nğŸ“‹ æ¼”ç¤º4: å¾ªç¯å·¥ä½œæµ")
    print("-" * 40)
    loop_workflow = demo.create_loop_workflow()
    
    initial_state = WorkflowState(
        workflow_type=WorkflowType.LOOP,
        input_data={"items": ["item1", "item2", "item3"], "processed_count": 0},
        intermediate_results={},
        current_step="start",
        execution_history=[]
    )
    
    result = loop_workflow.invoke(initial_state)
    print(f"âœ… å¾ªç¯å·¥ä½œæµæ‰§è¡Œå®Œæˆ")
    print(f"ğŸ“Š å¤„ç†ç»“æœ: {result}")
    
    # æ¼”ç¤º5: åµŒå¥—å·¥ä½œæµ
    print("\nğŸ“‹ æ¼”ç¤º5: åµŒå¥—å·¥ä½œæµ")
    print("-" * 40)
    nested_workflow = demo.create_nested_workflow()
    initial_state = WorkflowState(
        workflow_type=WorkflowType.NESTED,
        input_data={"query": "åµŒå¥—å·¥ä½œæµæµ‹è¯•"},
        intermediate_results={},
        current_step="start",
        execution_history=[]
    )
    result = nested_workflow.invoke(initial_state)
    print(f"âœ… åµŒå¥—å·¥ä½œæµæ‰§è¡Œå®Œæˆ")
    print(f"ğŸ“Š æœ€ç»ˆç»“æœ: {result}")
    
    # æ¼”ç¤º6: é”™è¯¯æ¢å¤å·¥ä½œæµ
    print("\nğŸ“‹ æ¼”ç¤º6: é”™è¯¯æ¢å¤å·¥ä½œæµ")
    print("-" * 40)
    error_recovery_workflow = demo.create_error_recovery_workflow()
    initial_state = WorkflowState(
        workflow_type=WorkflowType.ERROR_RECOVERY,
        input_data={"query": "é”™è¯¯æ¢å¤æµ‹è¯•"},
        intermediate_results={},
        current_step="start",
        execution_history=[]
    )
    result = error_recovery_workflow.invoke(initial_state)
    print(f"âœ… é”™è¯¯æ¢å¤å·¥ä½œæµæ‰§è¡Œå®Œæˆ")
    print(f"ğŸ“Š æœ€ç»ˆç»“æœ: {result}")
    
    # æ¼”ç¤º7: è¶…æ—¶æ§åˆ¶å·¥ä½œæµ
    print("\nğŸ“‹ æ¼”ç¤º7: è¶…æ—¶æ§åˆ¶å·¥ä½œæµ")
    print("-" * 40)
    timeout_workflow = demo.create_timeout_workflow()
    initial_state = WorkflowState(
        workflow_type=WorkflowType.TIMEOUT,
        input_data={"query": "è¶…æ—¶æ§åˆ¶æµ‹è¯•"},
        intermediate_results={},
        current_step="start",
        execution_history=[]
    )
    result = timeout_workflow.invoke(initial_state)
    print(f"âœ… è¶…æ—¶æ§åˆ¶å·¥ä½œæµæ‰§è¡Œå®Œæˆ")
    print(f"ğŸ“Š æœ€ç»ˆç»“æœ: {result}")
    
    print("\n" + "=" * 60)
    print("ğŸ‰ æ‰€æœ‰å·¥ä½œæµç»„åˆæ¼”ç¤ºå®Œæˆï¼")
    print("\nğŸ“Š æ¼”ç¤ºæ€»ç»“:")
    print("   â€¢ å¹¶è¡Œå·¥ä½œæµ: å¤šä¸ªä»»åŠ¡åŒæ—¶æ‰§è¡Œï¼Œæé«˜æ•ˆç‡")
    print("   â€¢ ä¸²è¡Œå·¥ä½œæµé“¾: ä»»åŠ¡æŒ‰é¡ºåºæ‰§è¡Œï¼Œç¡®ä¿ä¾èµ–å…³ç³»")
    print("   â€¢ æ¡ä»¶åˆ†æ”¯å·¥ä½œæµ: æ ¹æ®æ¡ä»¶é€‰æ‹©ä¸åŒæ‰§è¡Œè·¯å¾„")
    print("   â€¢ å¾ªç¯å·¥ä½œæµ: é‡å¤æ‰§è¡Œç›´åˆ°æ»¡è¶³æ¡ä»¶")
    print("   â€¢ åµŒå¥—å·¥ä½œæµ: å·¥ä½œæµä¸­åŒ…å«å­å·¥ä½œæµï¼Œæ”¯æŒå¤æ‚åœºæ™¯")
    print("   â€¢ é”™è¯¯æ¢å¤å·¥ä½œæµ: è‡ªåŠ¨å¤„ç†é”™è¯¯å¹¶é‡è¯•ï¼Œæé«˜é²æ£’æ€§")
    print("   â€¢ è¶…æ—¶æ§åˆ¶å·¥ä½œæµ: é™åˆ¶ä»»åŠ¡æ‰§è¡Œæ—¶é—´ï¼Œé˜²æ­¢æ— é™ç­‰å¾…")


if __name__ == "__main__":
    demo_workflow_combinations()