---
title: "LangChain å…¥é—¨æ•™ç¨‹ï¼šå­¦ä¹ è®°å¿†æ¨¡å—"
description: "åŸºäºæ¨¡å— 04-memory çš„å®æˆ˜ä»‹ç»ï¼Œå«ç¯å¢ƒå‡†å¤‡ã€ä¾èµ–ã€åŸºç¡€ç”¨æ³•ä¸ç¤ºä¾‹ç´¢å¼•ã€‚"
keywords:
  - LangChain
  - Memory
  - ChatMessageHistory
  - RunnableWithMessageHistory
  - MessagesPlaceholder
  - InMemoryChatMessageHistory
  - å†…å­˜å­˜å‚¨
  - æ–‡ä»¶å­˜å‚¨
  - å®ä½“è®°å¿†
  - æ‘˜è¦è®°å¿†
  - ä»¤ç‰Œå‹ç¼©è®°å¿†
  - é™åˆ¶å†å²çª—å£è®°å¿†
tags:
  - Tutorial
  - Memory
  - LLM
author: "langchain-learn"
date: "2025-10-12"
lang: "zh-CN"
canonical: "/blog/langchain-memory-tutorial"
audience: "åˆå­¦è€… / å…·å¤‡PythonåŸºç¡€çš„LLMå·¥ç¨‹å¸ˆ"
difficulty: "beginner-intermediate"
estimated_read_time: "12-18min"
topics:
  - LangChain Core
  - Memory
  - ChatMessageHistory
  - InMemoryChatMessageHistory
  - RunnableWithMessageHistory
entities:
  - LangChain
  - OpenAI
  - dotenv
  - DeepSeek
  - Ollama
---

# LangChain å…¥é—¨æ•™ç¨‹ï¼šå­¦ä¹ è®°å¿†æ¨¡å—

## æœ¬é¡µå¿«æ·è·³è½¬
- ç›®å½•ï¼š
  - [å¼•è¨€](#intro)
  - [è®°å¿†çš„åŸºç¡€ç”¨æ³•ï¼šå°†å†å²æ³¨å…¥æç¤ºè¯](#memory-basics)
  - [èŠå¤©æ¶ˆæ¯è®°å¿†å­˜å‚¨](#stores)
    - [AIè®°å¿†å†…å­˜å­˜å‚¨(çŸ­æœŸè®°å¿†)](#store-memory)
    - [AIè®°å¿†æ–‡æœ¬å­˜å‚¨(é•¿æœŸè®°å¿†)](#store-text-memory)
  - [èŠå¤©æ¶ˆæ¯è®°å¿†å‹ç¼©](#compression)
    - [AIè®°å¿†ä¿¡æ¯å®ä½“å‹ç¼©](#entity)
    - [AIè®°å¿†ä¿¡æ¯æ€»ç»“ï¼ˆæ‘˜è¦ï¼‰å‹ç¼©](#summary)
    - [èŠå¤©æ¶ˆæ¯è®°å¿†æ»‘çª—ä¿ç•™](#limited)
    - [èŠå¤©æ¶ˆæ¯è®°å¿†ä»¤ç‰Œæ»‘çª—å‹ç¼©ï¼ˆTokenï¼‰](#token-compression-window)
  - [å¸¸è§é”™è¯¯ä¸å¿«é€Ÿæ’æŸ¥ (Q/A)](#qa)
    - [å­˜å‚¨ç±»å‹é€‰æ‹©æŒ‡å—](#qa-storage-choice)
    - [æ»‘çª— vs ä»¤ç‰Œå‹ç¼©å¦‚ä½•å–èˆ](#qa-window-vs-compression)
    - [ä¸åŒæ¨¡å‹/ä¾›åº”å•†ä¸‹å¯ç”¨è®°å¿†](#qa-providers)
    - [é¿å…ä¸Šä¸‹æ–‡çˆ†ç‚¸çš„å®ç”¨å»ºè®®](#qa-context-bloat)
    - [å‹ç¼©æŸå¤±çš„è¯„ä¼°ä¸ç›‘æ§](#qa-eval)
  - [å‚è€ƒèµ„æ–™](#references)
  - [æ›´æ–°è®°å½•](#changelog)
  - [æ€»ç»“](#summary-final)

---

<a id="intro" data-alt="å¼•è¨€ æ¦‚è¿° ç›®æ ‡ å—ä¼—"></a>
## å¼•è¨€
æœ¬æ•™ç¨‹å›´ç»• LangChain çš„è®°å¿†æ¨¡å—ï¼Œå¸®åŠ©ä½ åœ¨å·¥ç¨‹å®è·µä¸­æ„å»ºå¯ç»´æŠ¤çš„å¯¹è¯è®°å¿†ï¼šè®©æ¨¡å‹â€œè®°ä½â€ä¸Šä¸‹æ–‡ã€ç†è§£é•¿æœŸç›®æ ‡ï¼Œå¹¶åœ¨å¤šè½®äº¤äº’ä¸­ä¿æŒä¸€è‡´æ€§ã€‚

<a id="what-is-ai-memory" data-alt="ä»€ä¹ˆæ˜¯ AI è®°å¿† å®šä¹‰ æ¦‚å¿µ"></a>
## ä»€ä¹ˆæ˜¯AIè®°å¿†

AI é»˜è®¤åªä¼šæ ¹æ®å½“å‰è¾“å…¥ç”Ÿæˆå›ç­”ï¼Œæ— æ³•è®°ä½ä¹‹å‰çš„å¯¹è¯ã€‚ä¸ºäº†è®© AI åœ¨å¤šè½®å¯¹è¯ä¸­ä¿æŒä¸Šä¸‹æ–‡è¿è´¯ï¼Œæˆ‘ä»¬éœ€è¦å°†å¯¹è¯å†å²è®°å½•æ³¨å…¥åˆ°æç¤ºè¯ä¸­ï¼Œè¿™ç§æœºåˆ¶å°±æ˜¯**è®°å¿†**ã€‚

<a id="why-memory" data-alt="ä¸ºä»€ä¹ˆ éœ€è¦ è®°å¿† ä½œç”¨"></a>
### 01ã€ä¸ºä»€ä¹ˆéœ€è¦è®°å¿†

ç”±äºæ¨¡å‹çš„ä¸Šä¸‹æ–‡çª—å£ï¼ˆè¾“å…¥ç»™AIçš„å†…å®¹ï¼‰æœ‰é™ï¼Œå¿…é¡»æ§åˆ¶æ³¨å…¥å†…å®¹çš„ä½“é‡ä¸è´¨é‡ã€‚å¦‚ä½•è®°å½•ä¸ä½¿ç”¨è®°å¿†ï¼Œç›´æ¥å½±å“å¯¹è¯çš„ä¸€è‡´æ€§ä¸æˆæœ¬ã€‚ä¸‹æ–‡å°†åŸºäº LangChain è®°å¿†æ¨¡å—ï¼ŒæŒ‰åœºæ™¯ç»™å‡ºå¯å¤ç”¨çš„ä½¿ç”¨æ–¹æ³•ä¸å®è·µã€‚

<a id="memory-basics" data-alt="åŸºç¡€ ç”¨æ³• ChatPromptTemplate MessagesPlaceholder RunnableWithMessageHistory"></a>
### 02ã€è®°å¿†çš„åŸºç¡€ç”¨æ³•ï¼šå°†å†å²æ³¨å…¥æç¤ºè¯

åœ¨ LangChain ä¸­æœ‰ 2 ç§æ–¹å¼å¯ä»¥å°†å†å²æ³¨å…¥æç¤ºè¯ï¼š
1. è®°å¿†é€šè¿‡ ChatPromptTemplate ä¸ MessagesPlaceholder æ³¨å…¥åˆ°æç¤ºè¯ä¸­ï¼Œå¹¶ç”¨ RunnableWithMessageHistory ç»Ÿä¸€ç®¡ç†ã€‚
2. æ‰‹åŠ¨æ‹¼æ¥å†å²åˆ° ç³»ç»Ÿæç¤ºè¯ / ç”¨æˆ·æç¤ºè¯ æ¶ˆæ¯ï¼ˆé€‚åˆæç®€æˆ–ä¸€æ¬¡æ€§è„šæœ¬ï¼‰ï¼›éœ€è‡ªè¡Œæ§åˆ¶æ ¼å¼ä¸ä»¤ç‰Œï¼ˆTokenï¼‰é¢„ç®—ï¼Œå¯èƒ½å‡ºç°é‡å¤æ³¨å…¥æˆ–è¶Šæƒå†…å®¹ã€‚

æ¥ä¸‹æ¥æˆ‘å°†ä»‹ç»ä¸åŒè®°å¿†ä½¿ç”¨æ–¹æ³•ã€‚
é¡¹ç›®é…ç½®ä¸ä¾èµ–å®‰è£…è¯·å‚è§ä¸‹æ–‡çš„ï¼Œè¯·å‚è€ƒå¦ä¸€ç¯‡æ•™ç¨‹ï¼š[LangChain å…¥é—¨æ•™ç¨‹ï¼šæ„å»ºä½ çš„ç¬¬ä¸€ä¸ªèŠå¤©æœºå™¨äºº](https://juejin.cn/post/7559428036514709554)ã€‚


<a id="stores" data-alt="ä¸åŒå­˜å‚¨ç±»å‹ å†…å­˜ æ–‡ä»¶"></a>
## èŠå¤©æ¶ˆæ¯è®°å¿†å­˜å‚¨

è®°å¿†å¯ä»¥åˆ†ä¸ºçŸ­æœŸè®°å¿†å’Œé•¿æœŸè®°å¿†:
- çŸ­æœŸè®°å¿†ï¼šå­˜å‚¨åœ¨å†…å­˜ä¸­çš„å¯¹è¯å†å²è®°å½•ï¼Œä»…åœ¨å½“å‰ä¼šè¯ä¸­æœ‰æ•ˆã€‚
- é•¿æœŸè®°å¿†ï¼šå­˜å‚¨åœ¨å¤–éƒ¨æ•°æ®åº“æˆ–æ–‡ä»¶ä¸­çš„å¯¹è¯å†å²è®°å½•ï¼Œå¯è·¨ä¼šè¯è®¿é—®ã€‚

<a id="store-memory" data-alt="å†…å­˜å­˜å‚¨ è¿›ç¨‹å†… æ— é…ç½®"></a>
### AIè®°å¿†å†…å­˜å­˜å‚¨(çŸ­æœŸè®°å¿†)

```python

import os
import langchain
from dotenv import load_dotenv
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_core.chat_history import InMemoryChatMessageHistory
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_openai import ChatOpenAI


# ä»å½“å‰æ¨¡å—ç›®å½•åŠ è½½ .env
def load_environment() -> None:
    """åŠ è½½ç¯å¢ƒå˜é‡é…ç½®"""
    try:
        load_dotenv(dotenv_path=os.path.join(os.path.dirname(__file__), ".env"), override=False)
        print("âœ… ç¯å¢ƒå˜é‡åŠ è½½æˆåŠŸ")
    except Exception as e:
        print(f"âŒ ç¯å¢ƒå˜é‡åŠ è½½å¤±è´¥: {e}")
        raise


def get_llm() -> ChatOpenAI:
    """åˆ›å»ºå¹¶é…ç½®è¯­è¨€æ¨¡å‹å®ä¾‹"""
    try:
        api_key = os.getenv("OPENAI_API_KEY")
        if not api_key:
            raise ValueError("OPENAI_API_KEY æœªè®¾ç½®ï¼Œè¯·åœ¨.envæ–‡ä»¶ä¸­é…ç½®")
        
        model = os.getenv("OPENAI_MODEL", "gpt-3.5-turbo")
        base_url = os.getenv("OPENAI_BASE_URL")
        temperature = float(os.getenv("OPENAI_TEMPERATURE", "0.7"))  # é»˜è®¤0.7ä»¥è·å¾—æ›´å¥½çš„å¯¹è¯æ•ˆæœ
        max_tokens = int(os.getenv("OPENAI_MAX_TOKENS", "512"))

        kwargs = {
            "model": model,
            "api_key": api_key,
            "temperature": temperature,
            "max_tokens": max_tokens,
            "timeout": 120,
            "max_retries": 3,
            "request_timeout": 120,
            "verbose": True,
            "base_url": base_url,
        }
        
        print(f"âœ… LLMé…ç½®æˆåŠŸ - æ¨¡å‹: {model}, æ¸©åº¦: {temperature}")
        return ChatOpenAI(**kwargs)
    except Exception as e:
        print(f"âŒ LLMé…ç½®å¤±è´¥: {e}")
        raise


# å†…å­˜å­˜å‚¨ï¼ˆä»…è¿›ç¨‹å†…ï¼Œä¸æŒä¹…åŒ–ï¼‰
_store: dict[str, InMemoryChatMessageHistory] = {}


def get_in_memory_history(session_id: str) -> InMemoryChatMessageHistory:
    """ä½¿ç”¨å†…å­˜å­˜å‚¨èŠå¤©å†å²è®°å½•"""
    if session_id not in _store:
        _store[session_id] = InMemoryChatMessageHistory()
        print(f"âœ… åˆ›å»ºæ–°çš„å†…å­˜ä¼šè¯å†å² - session_id: {session_id}")
    else:
        print(f"âœ… è·å–ç°æœ‰å†…å­˜ä¼šè¯å†å² - session_id: {session_id}, æ¶ˆæ¯æ•°: {len(_store[session_id].messages)}")
    return _store[session_id]


def create_conversation_chain() -> RunnableWithMessageHistory:
    """åˆ›å»ºå¸¦å†…å­˜å†å²è®°å½•çš„ä¼šè¯é“¾"""
    try:
        load_environment()
        llm = get_llm()

        prompt = ChatPromptTemplate.from_messages([
            ("system", "ä½ æ˜¯ä¸€ä¸ªå‹å¥½çš„AIåŠ©æ‰‹ï¼Œèƒ½å¤Ÿè®°ä½ç”¨æˆ·ä¹‹å‰è¯´è¿‡çš„è¯ã€‚"),
            MessagesPlaceholder(variable_name="history"),
            ("human", "{input}"),
        ])

        chain = prompt | llm

        conversation = RunnableWithMessageHistory(
            runnable=chain,
            get_session_history=get_in_memory_history,
            input_messages_key="input",
            history_messages_key="history",
            verbose=True,
        )
        
        print("âœ… å¯¹è¯é“¾åˆ›å»ºæˆåŠŸ")
        return conversation
    except Exception as e:
        print(f"âŒ å¯¹è¯é“¾åˆ›å»ºå¤±è´¥: {e}")
        raise


def run_conversation_example(session_id: str = "memory_demo") -> None:
    """
    è¿è¡Œå†…å­˜å­˜å‚¨çš„å¯¹è¯ç¤ºä¾‹
    
    Args:
        session_id: ä¼šè¯æ ‡è¯†ç¬¦ï¼Œé»˜è®¤ä¸º "memory_demo"
    
    Raises:
        Exception: å¯¹è¯è¿‡ç¨‹ä¸­å‘ç”Ÿçš„å¼‚å¸¸
    """
    try:
        langchain.debug = True
        conversation = create_conversation_chain()

        print(f"\nğŸš€ å¼€å§‹å¯¹è¯ç¤ºä¾‹ (å­˜å‚¨ç±»å‹: memory, ä¼šè¯ID: {session_id})")

        print("\nğŸ“ ç¬¬ä¸€è½®å¯¹è¯...")
        response = conversation.invoke({"input": "ä½ å¥½ï¼Œæˆ‘å«å¼ ä¸‰ã€‚"}, config={"configurable": {"session_id": session_id}})
        print(f"ğŸ¤– AI: {response.content}")

        print("\nğŸ“ ç¬¬äºŒè½®å¯¹è¯...")
        response = conversation.invoke({"input": "æˆ‘åˆšæ‰å‘Šè¯‰ä½ æˆ‘å«ä»€ä¹ˆåå­—ï¼Ÿ"}, config={"configurable": {"session_id": session_id}})
        print(f"ğŸ¤– AI: {response.content}")

        print("\nğŸ“ ç¬¬ä¸‰è½®å¯¹è¯...")
        response = conversation.invoke({"input": "èƒ½å¸®æˆ‘ç”Ÿæˆä¸€ä¸ªç®€çŸ­çš„è‡ªæˆ‘ä»‹ç»å—ï¼Ÿ"}, config={"configurable": {"session_id": session_id}})
        print(f"ğŸ¤– AI: {response.content}")

        print("\nâœ… å¯¹è¯ç¤ºä¾‹è¿è¡Œå®Œæˆ")
        langchain.debug = False
    except Exception as e:
        print(f"âŒ å¯¹è¯ç¤ºä¾‹è¿è¡Œå¤±è´¥: {e}")
        print("ğŸ’¡ è¯·æ£€æŸ¥ç¯å¢ƒé…ç½®å’Œä¾èµ–æ˜¯å¦æ­£ç¡®ã€‚")
        raise


def main() -> None:
    """ä¸»å‡½æ•°ï¼šè¿è¡Œå†…å­˜å­˜å‚¨ç¤ºä¾‹"""
    run_conversation_example()


if __name__ == "__main__":
    main()
```

è¦ç‚¹ï¼š
- ä½¿ç”¨ MessagesPlaceholder("history") å°†å†å²æ¶ˆæ¯åµŒå…¥æç¤ºè¯ã€‚
- ç”¨ RunnableWithMessageHistory ç®¡ç†ä¼šè¯ id ä¸å†å²è¯»å†™ï¼Œå…å»æ‰‹å·¥æ‹¼æ¥å†å²ã€‚
- ç”¨ InMemoryChatMessageHistory å­˜å‚¨åˆ°å†…å­˜ä¸­ï¼Œä¼šè¯ç»“æŸåæ¸…é™¤å†å²è®°å½•ã€‚
- æç¤ºè¯: ç³»ç»Ÿæç¤ºè¯+ç”¨æˆ·æç¤ºè¯1+AIå›ç­”1+ç”¨æˆ·æç¤ºè¯2+AIå›ç­”2+æœ€æ–°çš„ç”¨æˆ·æç¤ºè¯çš„é€»è¾‘

<a id="store-text-memory" data-alt="æ–‡æœ¬å†…å­˜å­˜å‚¨ æ–‡ä»¶å­˜å‚¨ JSON"></a>
### AIè®°å¿†æ–‡æœ¬å­˜å‚¨(é•¿æœŸè®°å¿†)

```python
import os
import langchain
from dotenv import load_dotenv
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_community.chat_message_histories import FileChatMessageHistory
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_openai import ChatOpenAI


# ä»å½“å‰æ¨¡å—ç›®å½•åŠ è½½ .env
def load_environment() -> None:
    """åŠ è½½ç¯å¢ƒå˜é‡é…ç½®"""
    try:
        load_dotenv(dotenv_path=os.path.join(os.path.dirname(__file__), ".env"), override=False)
        print("âœ… ç¯å¢ƒå˜é‡åŠ è½½æˆåŠŸ")
    except Exception as e:
        print(f"âŒ ç¯å¢ƒå˜é‡åŠ è½½å¤±è´¥: {e}")
        raise


def get_llm() -> ChatOpenAI:
    """åˆ›å»ºå¹¶é…ç½®è¯­è¨€æ¨¡å‹å®ä¾‹"""
    try:
        api_key = os.getenv("OPENAI_API_KEY")
        if not api_key:
            raise ValueError("OPENAI_API_KEY æœªè®¾ç½®ï¼Œè¯·åœ¨.envæ–‡ä»¶ä¸­é…ç½®")
        
        model = os.getenv("OPENAI_MODEL", "gpt-3.5-turbo")
        base_url = os.getenv("OPENAI_BASE_URL")
        temperature = float(os.getenv("OPENAI_TEMPERATURE", "0.7"))  # é»˜è®¤0.7ä»¥è·å¾—æ›´å¥½çš„å¯¹è¯æ•ˆæœ
        max_tokens = int(os.getenv("OPENAI_MAX_TOKENS", "512"))

        kwargs = {
            "model": model,
            "api_key": api_key,
            "temperature": temperature,
            "max_tokens": max_tokens,
            "timeout": 120,
            "max_retries": 3,
            "request_timeout": 120,
            "verbose": True,
            "base_url": base_url,
        }
        
        print(f"âœ… LLMé…ç½®æˆåŠŸ - æ¨¡å‹: {model}, æ¸©åº¦: {temperature}")
        return ChatOpenAI(**kwargs)
    except Exception as e:
        print(f"âŒ LLMé…ç½®å¤±è´¥: {e}")
        raise


def get_file_history(session_id: str) -> FileChatMessageHistory:
    """
    ä½¿ç”¨æ–‡ä»¶å­˜å‚¨èŠå¤©å†å²è®°å½•ï¼ˆJSONï¼‰
    
    Args:
        session_id: ä¼šè¯æ ‡è¯†ç¬¦ï¼Œç”¨äºåŒºåˆ†ä¸åŒç”¨æˆ·çš„å¯¹è¯å†å²
        
    Returns:
        FileChatMessageHistory: æ–‡ä»¶å­˜å‚¨çš„èŠå¤©å†å²è®°å½•å®ä¾‹
        
    Raises:
        Exception: å½“æ–‡ä»¶åˆ›å»ºæˆ–è¯»å–å¤±è´¥æ—¶æŠ›å‡ºå¼‚å¸¸
    """
    try:
        file_path = f"chat_history_{session_id}.json"
        history = FileChatMessageHistory(file_path, encoding="utf-8", ensure_ascii=False)
        print(f"âœ… æ–‡ä»¶å†å²è®°å½•åŠ è½½æˆåŠŸ - session_id: {session_id}, æ–‡ä»¶: {file_path}")
        return history
    except Exception as e:
        print(f"âŒ æ–‡ä»¶å†å²è®°å½•åŠ è½½å¤±è´¥: {e}")
        raise


def create_conversation_chain() -> RunnableWithMessageHistory:
    """
    åˆ›å»ºå¸¦æ–‡ä»¶å†å²è®°å½•çš„ä¼šè¯é“¾
    
    Returns:
        RunnableWithMessageHistory: é…ç½®å¥½çš„ä¼šè¯é“¾å®ä¾‹
        
    Raises:
        Exception: ä¼šè¯é“¾åˆ›å»ºè¿‡ç¨‹ä¸­å‘ç”Ÿçš„å¼‚å¸¸
    """
    try:
        load_environment()
        llm = get_llm()

        prompt = ChatPromptTemplate.from_messages([
            ("system", "ä½ æ˜¯ä¸€ä¸ªå‹å¥½çš„AIåŠ©æ‰‹ï¼Œèƒ½å¤Ÿè®°ä½ç”¨æˆ·ä¹‹å‰è¯´è¿‡çš„è¯ã€‚"),
            MessagesPlaceholder(variable_name="history"),
            ("human", "{input}"),
        ])

        chain = prompt | llm

        conversation = RunnableWithMessageHistory(
            runnable=chain,
            get_session_history=get_file_history,
            input_messages_key="input",
            history_messages_key="history",
            verbose=True,
        )
        print("âœ… æ–‡ä»¶å­˜å‚¨å¯¹è¯é“¾åˆ›å»ºæˆåŠŸ")
        return conversation
    except Exception as e:
        print(f"âŒ æ–‡ä»¶å­˜å‚¨å¯¹è¯é“¾åˆ›å»ºå¤±è´¥: {e}")
        raise


def run_conversation_example(session_id: str = "file_demo") -> None:
    """è¿è¡Œæ–‡ä»¶å­˜å‚¨çš„å¯¹è¯ç¤ºä¾‹ï¼ˆä¼šåœ¨å½“å‰ç›®å½•åˆ›å»º JSON æ–‡ä»¶ï¼‰"""
    try:
        langchain.debug = True
        conversation = create_conversation_chain()

        print(f"\n=== å¼€å§‹å¯¹è¯ç¤ºä¾‹ (å­˜å‚¨ç±»å‹: file, ä¼šè¯ID: {session_id}) ===")

        response = conversation.invoke({"input": "ä½ å¥½ï¼Œæˆ‘å«å¼ ä¸‰ã€‚"}, config={"configurable": {"session_id": session_id}})
        print(f"AI: {response.content}")

        response = conversation.invoke({"input": "æˆ‘åˆšæ‰å‘Šè¯‰ä½ æˆ‘å«ä»€ä¹ˆåå­—ï¼Ÿ"}, config={"configurable": {"session_id": session_id}})
        print(f"AI: {response.content}")

        response = conversation.invoke({"input": "èƒ½å¸®æˆ‘ç”Ÿæˆä¸€ä¸ªç®€çŸ­çš„è‡ªæˆ‘ä»‹ç»å—ï¼Ÿ"}, config={"configurable": {"session_id": session_id}})
        print(f"AI: {response.content}")

        print("\n=== å¯¹è¯ç¤ºä¾‹ç»“æŸ ===")
        langchain.debug = False
    except Exception as e:
        print(f"å‘ç”Ÿé”™è¯¯ï¼š{e}")
        print("è¯·æ£€æŸ¥è¿è¡Œç›®å½•å†™å…¥æƒé™ï¼Œæˆ–æ£€æŸ¥ç¯å¢ƒé…ç½®ä¸ä¾èµ–ã€‚")


def main() -> None:
    """ä¸»å‡½æ•°ï¼šè¿è¡Œæ–‡ä»¶å­˜å‚¨ç¤ºä¾‹"""
    run_conversation_example()


if __name__ == "__main__":
    main()
```

è¦ç‚¹ï¼š
- ä½¿ç”¨ MessagesPlaceholder("history") å°†å†å²æ¶ˆæ¯åµŒå…¥æç¤ºè¯ã€‚
- ç”¨ RunnableWithMessageHistory ç®¡ç†ä¼šè¯ id ä¸å†å²è¯»å†™ï¼Œå…å»æ‰‹å·¥æ‹¼æ¥å†å²ã€‚
- ç”¨ FileChatMessageHistory å­˜å‚¨åˆ°æœ¬åœ°æ–‡ä»¶ä¸­ï¼Œéœ€è¦æ³¨æ„ç¼–ç é—®é¢˜ï¼Œé¿å…ä¸­æ–‡ä¹±ç ã€‚
- æç¤ºè¯: ç³»ç»Ÿæç¤ºè¯+ç”¨æˆ·æç¤ºè¯1+AIå›ç­”1+ç”¨æˆ·æç¤ºè¯2+AIå›ç­”2+æœ€æ–°çš„ç”¨æˆ·æç¤ºè¯çš„é€»è¾‘

<a id="compression" data-alt="èŠå¤© æ¶ˆæ¯ è®°å¿† å‹ç¼© æ¦‚è§ˆ"></a>
## AIèŠå¤©æ¶ˆæ¯è®°å¿†å‹ç¼©

ç»è¿‡å¤šè½®å¯¹è¯åï¼Œä¼šè¯è®°å¿†ä¼šè¿…é€Ÿè†¨èƒ€å¹¶é€¼è¿‘æ¨¡å‹çš„ä¸Šä¸‹æ–‡çª—å£é™åˆ¶ã€‚ä¸ºé¿å…è¶…é™ä¸æˆæœ¬é£™å‡ï¼Œéœ€è¦å¯¹è®°å¿†è¿›è¡Œå‹ç¼©ä¸æ²»ç†ã€‚å‹ç¼©çš„ç›®æ ‡æ˜¯ç”¨æœ€å°ä¿¡æ¯æŸå¤±ç»´æŒä¸Šä¸‹æ–‡è¿è´¯æ€§ä¸å¯ç”¨æ€§ã€‚ä¸‹é¢ç»™å‡ºå¸¸è§ç­–ç•¥ä¸å–èˆï¼Œå†è¯¦ç»†ä»‹ç» 4 ç§å‹ç¼©æ–¹æ³•ã€‚
- ç»“åˆå®ä½“è®°å¿†ç»´æŠ¤å…³é”®äº‹å®ï¼ˆå¦‚å§“åã€å…¬å¸ã€åå¥½ï¼‰ï¼Œå‡å°‘æ‘˜è¦é—æ¼çš„é‡è¦ä¿¡æ¯ã€‚
- å¯¹è¯å‹ç¼©ï¼šå°†è¾ƒæ—©çš„å†å²å¯¹è¯å‹ç¼©ä¸ºæ‘˜è¦ï¼Œä¿ç•™å…³é”®ä¿¡æ¯ä¸ä¸Šä¸‹æ–‡ï¼Œé™ä½æ€»ä»¤ç‰Œï¼ˆTokenï¼‰å ç”¨ã€‚
- ä¼šè¯å†…çŸ­æœŸï¼šä¼˜å…ˆä½¿ç”¨æ»‘çª—ä¿ç•™ï¼Œå®ç°ç®€å•ã€å¼€é”€ä½ï¼Œä½†å¯èƒ½ä¸¢å¤±æ—©æœŸä¸Šä¸‹æ–‡ã€‚
- æˆæœ¬æ•æ„Ÿï¼šæ ¹æ®ä»¤ç‰Œï¼ˆTokenï¼‰é¢„ç®—è°ƒæ•´æ‘˜è¦è§¦å‘é˜ˆå€¼ä¸ä¿ç•™çª—å£ï¼Œå¸¸è§åšæ³•æ˜¯åœ¨ä¸Šä¸‹æ–‡çª—å£çš„ 70%â€“80% æ—¶å¯åŠ¨å‹ç¼©ã€‚


<a id="entity" data-alt="å®ä½“ è®°å¿† EntityMemory"></a>
### AIè®°å¿†ä¿¡æ¯å®ä½“å‹ç¼©
- é€šè¿‡ä»å¯¹è¯ä¸­æŠ½å–â€œäººç‰©ã€åœ°ç‚¹ã€ç»„ç»‡ã€å…³é”®äº‹å®â€ç­‰å®ä½“ä¿¡æ¯ï¼Œå½¢æˆç»“æ„åŒ–è®°å¿†ï¼Œä¾¿äºé•¿æœŸå¼•ç”¨ä¸æ›´æ–°ã€‚
- å¸¸è§åšæ³•ï¼šåœ¨æ¯è½®å¯¹è¯åæŠ½å–å¹¶åˆå¹¶å®ä½“æ¡£æ¡ˆï¼ˆå¦‚â€œå§“åã€å…¬å¸ã€å…´è¶£â€ç­‰ï¼‰ï¼Œåœ¨æç¤ºè¯ä¸­ä»¥ç»“æ„åŒ–å­—æ®µæ³¨å…¥ä¸Šä¸‹æ–‡ã€‚
  
```python

import os
from typing import Dict, List
from dotenv import load_dotenv
from langchain.globals import set_verbose
from langchain_core.chat_history import BaseChatMessageHistory, InMemoryChatMessageHistory
from langchain_core.messages import BaseMessage, HumanMessage, AIMessage
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_core.callbacks import StdOutCallbackHandler
from langchain.globals import set_verbose, set_debug
from langchain_openai import ChatOpenAI


# ä»å½“å‰æ¨¡å—ç›®å½•åŠ è½½ .env
def load_environment():
    """åŠ è½½ç¯å¢ƒå˜é‡é…ç½®"""
    try:
        load_dotenv(dotenv_path=os.path.join(os.path.dirname(__file__), ".env"), override=False)
        print("âœ… ç¯å¢ƒå˜é‡åŠ è½½æˆåŠŸ")
    except Exception as e:
        print(f"âŒ ç¯å¢ƒå˜é‡åŠ è½½å¤±è´¥: {e}")
        raise

# è·å–é…ç½®çš„è¯­è¨€æ¨¡å‹
def get_llm() -> ChatOpenAI:
    """
    åˆ›å»ºå¹¶é…ç½®è¯­è¨€æ¨¡å‹å®ä¾‹
    
    Returns:
        ChatOpenAI: é…ç½®å¥½çš„è¯­è¨€æ¨¡å‹å®ä¾‹
        
    Raises:
        ValueError: å½“APIå¯†é’¥æœªè®¾ç½®æ—¶æŠ›å‡º
        Exception: å…¶ä»–é…ç½®é”™è¯¯æ—¶æŠ›å‡º
    """
    try:
        api_key = os.getenv("OPENAI_API_KEY")
        if not api_key:
            raise ValueError("OPENAI_API_KEY æœªè®¾ç½®ï¼Œè¯·åœ¨.envæ–‡ä»¶ä¸­é…ç½®")
        
        model = os.getenv("OPENAI_MODEL", "gpt-3.5-turbo")
        base_url = os.getenv("OPENAI_BASE_URL")
        temperature = float(os.getenv("OPENAI_TEMPERATURE", "0.7"))  # é»˜è®¤0.7ä»¥è·å¾—æ›´å¥½çš„å¯¹è¯æ•ˆæœ
        max_tokens = int(os.getenv("OPENAI_MAX_TOKENS", "512"))

        kwargs = {
            "model": model,
            "api_key": api_key,
            "temperature": temperature,
            "max_tokens": max_tokens,
            "timeout": 120,
            "max_retries": 3,
            "request_timeout": 120,
            "base_url": base_url,
            "verbose": True
        }
        
        print(f"âœ… LLMé…ç½®æˆåŠŸ - æ¨¡å‹: {model}, æ¸©åº¦: {temperature}")
        return ChatOpenAI(**kwargs)
    except ValueError as e:
        print(f"âŒ é…ç½®éªŒè¯å¤±è´¥: {e}")
        raise
    except Exception as e:
        print(f"âŒ LLMé…ç½®å¤±è´¥: {e}")
        raise

# ä¼šè¯å­˜å‚¨ - å­˜å‚¨ä¸åŒä¼šè¯çš„å†å²è®°å½•
store: Dict[str, BaseChatMessageHistory] = {}
# å®ä½“å­˜å‚¨ - å­˜å‚¨ä¸åŒä¼šè¯çš„å®ä½“ä¿¡æ¯
entity_store: Dict[str, Dict[str, str]] = {}

# è·å–ä¼šè¯å†å²
def get_session_history(session_id: str) -> BaseChatMessageHistory:
    """
    è·å–æˆ–åˆ›å»ºæŒ‡å®šä¼šè¯çš„å†å²è®°å½•
    
    Args:
        session_id: ä¼šè¯æ ‡è¯†ç¬¦
        
    Returns:
        BaseChatMessageHistory: ä¼šè¯å†å²è®°å½•å®ä¾‹
        
    Raises:
        Exception: ä¼šè¯å†å²è·å–è¿‡ç¨‹ä¸­å‘ç”Ÿçš„å¼‚å¸¸
    """
    try:
        if session_id not in store:
            store[session_id] = InMemoryChatMessageHistory()
            print(f"âœ… åˆ›å»ºæ–°çš„ä¼šè¯å†å² - ä¼šè¯ID: {session_id}")
        else:
            print(f"âœ… åŠ è½½ç°æœ‰ä¼šè¯å†å² - ä¼šè¯ID: {session_id}")
        return store[session_id]
    except Exception as e:
        print(f"âŒ è·å–ä¼šè¯å†å²å¤±è´¥ - ä¼šè¯ID: {session_id}, é”™è¯¯: {e}")
        raise

# æå–å®ä½“çš„æç¤ºæ¨¡æ¿
ENTITY_EXTRACTION_PROMPT = ChatPromptTemplate.from_messages([
    ("system", "ä½ æ˜¯ä¸€ä¸ªå®ä½“æå–åŠ©æ‰‹ã€‚è¯·ä»ä»¥ä¸‹å¯¹è¯ä¸­æå–å®ä½“åŠå…¶ç›¸å…³ä¿¡æ¯ã€‚è¯·ä»¥JSONæ ¼å¼è¾“å‡ºï¼Œå…¶ä¸­é”®æ˜¯å®ä½“åç§°ï¼Œå€¼æ˜¯å…³äºè¯¥å®ä½“çš„ä¿¡æ¯ã€‚å¦‚æœæ²¡æœ‰å®ä½“ï¼Œè¯·è¿”å›ç©ºå¯¹è±¡ã€‚"),
    ("human", "å¯¹è¯: {conversation}")
])

# æå–å®ä½“
def extract_entities(conversation: List[BaseMessage], session_id: str) -> Dict[str, str]:
    """
    ä»å¯¹è¯å†…å®¹ä¸­æå–å…³é”®å®ä½“ä¿¡æ¯
    
    è¯¥æ–¹æ³•ä½¿ç”¨LLMæ¨¡å‹åˆ†æå¯¹è¯æ–‡æœ¬ï¼Œè¯†åˆ«å¹¶æå–å…¶ä¸­çš„äººç‰©ã€åœ°ç‚¹ã€æ—¶é—´ç­‰å®ä½“ä¿¡æ¯ã€‚
    æå–çš„å®ä½“å°†ç”¨äºå¢å¼ºå¯¹è¯è®°å¿†å’Œä¸Šä¸‹æ–‡ç†è§£ã€‚
    
    Args:
        conversation: éœ€è¦åˆ†æçš„å¯¹è¯æ¶ˆæ¯åˆ—è¡¨
        session_id: ä¼šè¯æ ‡è¯†ç¬¦ï¼Œç”¨äºåŒºåˆ†ä¸åŒç”¨æˆ·çš„å®ä½“å­˜å‚¨
        
    Returns:
        Dict[str, str]: æå–çš„å®ä½“å­—å…¸ï¼Œé”®ä¸ºå®ä½“åç§°ï¼Œå€¼ä¸ºå®ä½“ç›¸å…³ä¿¡æ¯
        
    Raises:
        json.JSONDecodeError: å½“LLMå“åº”æ— æ³•è§£æä¸ºæœ‰æ•ˆJSONæ—¶æŠ›å‡º
        Exception: å…¶ä»–å¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿçš„å¼‚å¸¸
    """
    try:
        llm = get_llm()
        
        # æ„å»ºå¯¹è¯å­—ç¬¦ä¸²
        conversation_str = "\n".join([f"{msg.type}: {msg.content}" for msg in conversation])
        
        print(f"ğŸ” å®ä½“æå– - å¯¹è¯å†…å®¹é•¿åº¦: {len(conversation_str)} å­—ç¬¦")
        print(f"ğŸ” å®ä½“æå– - ä¼šè¯ID: {session_id}")
        
        # æå–å®ä½“
        entity_chain = ENTITY_EXTRACTION_PROMPT | llm
        print(f"ğŸ” å®ä½“æå– - å¼€å§‹è°ƒç”¨å®ä½“æå–é“¾")
        handler = StdOutCallbackHandler()
        response = entity_chain.invoke({"conversation": conversation_str}, config={"callbacks": [handler]})
        print(f"ğŸ” å®ä½“æå– - åŸå§‹å“åº”: {response.content}")
        
        try:
            # å°è¯•è§£æJSONå“åº”
            import json
            entities = json.loads(response.content)
            print(f"âœ… å®ä½“æå– - è§£ææˆåŠŸï¼Œå®ä½“æ•°é‡: {len(entities)}")
            print(f"ğŸ” å®ä½“æå– - è§£æåçš„å®ä½“: {entities}")
        except json.JSONDecodeError as e:
            # å¦‚æœè§£æå¤±è´¥ï¼Œä½¿ç”¨ç©ºå­—å…¸
            entities = {}
            print(f"âš ï¸ å®ä½“æå– - JSONè§£æå¤±è´¥: {e}ï¼Œä½¿ç”¨ç©ºå­—å…¸")
            print("âš ï¸ å»ºè®®æ£€æŸ¥LLMå“åº”æ ¼å¼æ˜¯å¦ç¬¦åˆJSONè§„èŒƒ")
        
        # æ›´æ–°å®ä½“å­˜å‚¨
        if session_id not in entity_store:
            entity_store[session_id] = {}
            print(f"âœ… åˆ›å»ºæ–°çš„å®ä½“å­˜å‚¨ - ä¼šè¯ID: {session_id}")
        
        old_entities = entity_store[session_id].copy()
        entity_store[session_id].update(entities)
        
        print(f"âœ… å®ä½“å­˜å‚¨æ›´æ–° - ä¼šè¯ID: {session_id}")
        print(f"ğŸ” å®ä½“å­˜å‚¨æ›´æ–°è¯¦æƒ… - ä» {old_entities} æ›´æ–°ä¸º {entity_store[session_id]}")
        
        return entities
    except Exception as e:
        print(f"âŒ å®ä½“æå–è¿‡ç¨‹å‘ç”Ÿå¼‚å¸¸ - ä¼šè¯ID: {session_id}, é”™è¯¯: {e}")
        print("âš ï¸ è¯·æ£€æŸ¥ç½‘ç»œè¿æ¥ã€APIå¯†é’¥å’Œæ¨¡å‹é…ç½®")
        return {}

# åˆ›å»ºå¸¦å®ä½“è®°å¿†çš„å¯¹è¯é“¾
# ä¿®æ”¹create_entity_aware_chainå‡½æ•°
def  create_entity_aware_chain(session_id: str):
    """åˆ›å»ºèƒ½å¤Ÿè¯†åˆ«å’Œåˆ©ç”¨å®ä½“ä¿¡æ¯çš„å¯¹è¯é“¾"""
    llm = get_llm()
    
    # æ„å»ºåŒ…å«å®ä½“ä¿¡æ¯çš„æç¤ºæ¨¡æ¿
    prompt = ChatPromptTemplate.from_messages([
        ("system", "ä½ æ˜¯ä¸€ä¸ªå‹å¥½çš„AIåŠ©æ‰‹ã€‚ä»¥ä¸‹æ˜¯å¯¹è¯ä¸­æåˆ°çš„é‡è¦å®ä½“ä¿¡æ¯ï¼š\n{entity_info}\nè¯·æ ¹æ®è¿™äº›ä¿¡æ¯å›ç­”é—®é¢˜ã€‚"),
        ("human", "{input}"),
    ])
    
    # è·å–å®ä½“ä¿¡æ¯
    entity_info = "\n".join([f"{entity}: {info}" for entity, info in 
                           entity_store.get(session_id, {}).items()]) or "æš‚æ— å®ä½“ä¿¡æ¯"
    
    print(f"[VERBOSE] åˆ›å»ºå¯¹è¯é“¾ - ä¼šè¯ID: {session_id}")
    print(f"[VERBOSE] åˆ›å»ºå¯¹è¯é“¾ - å®ä½“ä¿¡æ¯: {entity_info}")
    
    # åˆ›å»ºé“¾ - ä¸ä½¿ç”¨bindæ–¹æ³•ï¼Œè€Œæ˜¯åœ¨è°ƒç”¨æ—¶ä¼ é€’å‚æ•°
    chain = prompt | llm
    
    return chain, entity_info

# ä¿®æ”¹mainå‡½æ•°ä¸­çš„è°ƒç”¨éƒ¨åˆ†
def main() -> None:
    try:
        set_debug(True)
        # åŠ è½½ç¯å¢ƒå˜é‡
        load_environment()
        # å¼€å¯LangChainå…¨å±€verboseæ—¥å¿—
    
        # å¦‚éœ€æ›´è¯¦ç»†è°ƒè¯•æ—¥å¿—ï¼Œå¯å¼€å¯ä¸‹é¢è¿™è¡Œï¼ˆè¾“å‡ºæ›´ä¸ºå†—é•¿ï¼‰
        # set_debug(True)
        session_id = "user_123"
        
        # åˆ›å»ºä¼šè¯å†å²
        history = get_session_history(session_id)
        
        # æ·»åŠ åˆå§‹å¯¹è¯
        user_message = "æˆ‘å«å¼ ä¸‰ï¼Œæˆ‘åœ¨å¾®è½¯å·¥ä½œã€‚"
        history.add_user_message(user_message)
        
        # å¤§æ¨¡å‹æå–å®ä½“
        entities = extract_entities(history.messages, session_id)
        print(f"è¯†åˆ«çš„å®ä½“ï¼š{entities}")
        
        # åˆ›å»ºå®ä½“æ„ŸçŸ¥çš„å¯¹è¯é“¾ - è·å–chainå’Œentity_info
        conversation_chain, entity_info = create_entity_aware_chain(session_id)
        
        # è·å–AIå“åº” - ç›´æ¥ä¼ é€’entity_infoå‚æ•° æˆ‘å«å¼ ä¸‰ï¼Œæˆ‘åœ¨å¾®è½¯å·¥ä½œã€‚
        print(f"[VERBOSE] ç¬¬ä¸€è½®å¯¹è¯ - ç”¨æˆ·è¾“å…¥: {user_message}")
        print(f"[VERBOSE] ç¬¬ä¸€è½®å¯¹è¯ - ä¼ é€’çš„å®ä½“ä¿¡æ¯: {entity_info}")
        handler = StdOutCallbackHandler()
        response = conversation_chain.invoke({"input": user_message, "entity_info": entity_info}, config={"callbacks": [handler]})
        ai_response = response.content
        history.add_ai_message(ai_response)
        
        print(f"[VERBOSE] ç¬¬ä¸€è½®å¯¹è¯ - AIå“åº”: {ai_response}")
        print(f"AIå“åº”: {ai_response}")
        
        # ç¬¬äºŒè½®å¯¹è¯æµ‹è¯•å®ä½“è®°å¿†
        second_user_message = "æˆ‘åœ¨å“ªé‡Œå·¥ä½œï¼Ÿ"
        history.add_user_message(second_user_message)
        
        # æ›´æ–°å®ä½“å­˜å‚¨
        extract_entities(history.messages, session_id)
        
        # æ›´æ–°å¯¹è¯é“¾ä»¥åŒ…å«æœ€æ–°å®ä½“ä¿¡æ¯
        conversation_chain, updated_entity_info = create_entity_aware_chain(session_id)
        
        # è·å–ç¬¬äºŒè½®AIå“åº” - ç›´æ¥ä¼ é€’æ›´æ–°çš„entity_infoå‚æ•°
        print(f"[VERBOSE] ç¬¬äºŒè½®å¯¹è¯ - ç”¨æˆ·è¾“å…¥: {second_user_message}")
        print(f"[VERBOSE] ç¬¬äºŒè½®å¯¹è¯ - æ›´æ–°çš„å®ä½“ä¿¡æ¯: {updated_entity_info}")
        handler = StdOutCallbackHandler()
        second_response = conversation_chain.invoke({"input": second_user_message, "entity_info": updated_entity_info}, config={"callbacks": [handler]})
        second_ai_response = second_response.content
        history.add_ai_message(second_ai_response)
        
        print(f"[VERBOSE] ç¬¬äºŒè½®å¯¹è¯ - AIå“åº”: {second_ai_response}")
        print(f"AIå“åº”: {second_ai_response}")
        
        # æ‰“å°æœ€ç»ˆå®ä½“å­˜å‚¨
        print(f"æœ€ç»ˆå®ä½“å­˜å‚¨ï¼š{entity_store.get(session_id, {})}")
        
    except Exception as e:
        print(f"é”™è¯¯: {e}")
        print("è¯·æ£€æŸ¥OPENAI_API_KEYç­‰é…ç½®æ˜¯å¦æ­£ç¡®")

if __name__ == "__main__":
    main()
```

è¦ç‚¹ï¼š
- ä½¿ç”¨ extract_entities é€šè¿‡LLMæç‚¼æˆå®ä½“ï¼Œé”®æ˜¯å®ä½“åç§°ï¼Œå€¼æ˜¯å…³äºè¯¥å®ä½“çš„ä¿¡æ¯ã€‚
- ç”¨ InMemoryChatMessageHistory å­˜å‚¨å®ä½“çš„è®°å¿†ã€‚
- é€šè¿‡æŠŠå®ä½“jsonåŒ–ï¼ŒåµŒå…¥åˆ°ç³»ç»Ÿæç¤ºè¯ä¸­ã€‚

<a id="summary" data-alt="æ‘˜è¦ è®°å¿† ConversationSummary"></a>
### AIè®°å¿†ä¿¡æ¯æ€»ç»“ï¼ˆæ‘˜è¦ï¼‰å‹ç¼©

- å°†è¾ƒæ—©çš„å†å²å¯¹è¯å‹ç¼©ä¸ºæ‘˜è¦ï¼Œä¿ç•™å…³é”®ä¿¡æ¯ä¸ä¸Šä¸‹æ–‡ï¼Œé™ä½æ€»ä»¤ç‰Œï¼ˆTokenï¼‰å ç”¨ã€‚
- ä¸‹æ–‡â€œä»¤ç‰Œå‹ç¼©è®°å¿†â€ç¤ºä¾‹æ¼”ç¤ºäº†åœ¨è¶…è¿‡é˜ˆå€¼æ—¶è‡ªåŠ¨ç”Ÿæˆæ‘˜è¦å¹¶é‡ç½®å†å²ï¼Œä»¥å®ç°ç¨³å®šçš„é•¿å¯¹è¯ã€‚

```python
import os
from dotenv import load_dotenv
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_core.chat_history import InMemoryChatMessageHistory
from langchain_openai import ChatOpenAI



# ä»å½“å‰æ¨¡å—ç›®å½•åŠ è½½ .env
def load_environment():
    """
    åŠ è½½ç¯å¢ƒå˜é‡é…ç½®æ–‡ä»¶
    
    ä»å½“å‰æ¨¡å—ç›®å½•åŠ è½½.envæ–‡ä»¶ï¼Œç”¨äºé…ç½®APIå¯†é’¥ç­‰ç¯å¢ƒå˜é‡ã€‚
    
    Raises:
        FileNotFoundError: å½“.envæ–‡ä»¶ä¸å­˜åœ¨æ—¶æŠ›å‡º
        Exception: å…¶ä»–åŠ è½½ç¯å¢ƒå˜é‡æ—¶å‘ç”Ÿçš„å¼‚å¸¸
    """
    try:
        env_path = os.path.join(os.path.dirname(__file__), ".env")
        print(f"ğŸ”§ åŠ è½½ç¯å¢ƒå˜é‡æ–‡ä»¶: {env_path}")
        load_dotenv(dotenv_path=env_path, override=False)
        print("âœ… ç¯å¢ƒå˜é‡åŠ è½½æˆåŠŸ")
    except FileNotFoundError:
        print("âš ï¸ è­¦å‘Š: .envæ–‡ä»¶æœªæ‰¾åˆ°ï¼Œå°†ä½¿ç”¨ç³»ç»Ÿç¯å¢ƒå˜é‡")
    except Exception as e:
        print(f"âŒ åŠ è½½ç¯å¢ƒå˜é‡å¤±è´¥: {e}")
        raise



# è·å–é…ç½®çš„è¯­è¨€æ¨¡å‹
def get_llm() -> ChatOpenAI:
    """åˆ›å»ºå¹¶é…ç½®è¯­è¨€æ¨¡å‹å®ä¾‹"""
    try:
        api_key = os.getenv("OPENAI_API_KEY")
        if not api_key:
            raise ValueError("OPENAI_API_KEY æœªè®¾ç½®ï¼Œè¯·åœ¨.envæ–‡ä»¶ä¸­é…ç½®")
        
        model = os.getenv("OPENAI_MODEL", "gpt-3.5-turbo")
        base_url = os.getenv("OPENAI_BASE_URL")
        temperature = float(os.getenv("OPENAI_TEMPERATURE", "0.7"))  # é»˜è®¤0.7ä»¥è·å¾—æ›´å¥½çš„å¯¹è¯æ•ˆæœ
        max_tokens = int(os.getenv("OPENAI_MAX_TOKENS", "512"))

        kwargs = {
            "model": model,
            "api_key": api_key,
            "temperature": temperature,
            "max_tokens": max_tokens,
            "timeout": 120,
            "max_retries": 3,
            "request_timeout": 120,
            "verbose": True,
            "base_url": base_url
        }

        print(f"âœ… LLMé…ç½®æˆåŠŸ - æ¨¡å‹: {model}, æ¸©åº¦: {temperature}")
        return ChatOpenAI(**kwargs)
    except Exception as e:
        print(f"âŒ LLMé…ç½®å¤±è´¥: {e}")
        raise

# ä¼šè¯å­˜å‚¨
store = {}

def get_session_history(session_id: str):
    """æ ¹æ®session_idè·å–æˆ–åˆ›å»ºå¯¹åº”çš„èŠå¤©å†å²è®°å½•"""
    if session_id not in store:
        store[session_id] = InMemoryChatMessageHistory()  # ä½¿ç”¨å†…å­˜å­˜å‚¨
        print(f"âœ… åˆ›å»ºæ–°çš„ä¼šè¯å†å² - session_id: {session_id}")
    else:
        print(f"âœ… è·å–ç°æœ‰ä¼šè¯å†å² - session_id: {session_id}, æ¶ˆæ¯æ•°: {len(store[session_id].messages)}")
    return store[session_id]

def create_summary_chain():
    """åˆ›å»ºå¸¦æ‘˜è¦åŠŸèƒ½çš„ä¼šè¯é“¾"""
    # åˆ›å»ºæ¨¡å‹
    llm = get_llm()
    
    # æ„å»ºæç¤ºæ¨¡æ¿ï¼ŒåŒ…å«ç³»ç»ŸæŒ‡ä»¤ã€å†å²æ¶ˆæ¯å’Œç”¨æˆ·è¾“å…¥
    prompt = ChatPromptTemplate.from_messages([
        ("system", "ä½ æ˜¯ä¸€ä¸ªåŠ©æ‰‹ï¼Œèƒ½å¤Ÿç†è§£å¹¶æ€»ç»“å¯¹è¯å†…å®¹ã€‚"),
        MessagesPlaceholder(variable_name="history"),  # å†å²æ¶ˆæ¯å°†åŠ¨æ€æ³¨å…¥äºæ­¤
        ("human", "{input}"),
    ])
    
    # åˆ›å»ºåŸºæœ¬é“¾
    chain = prompt | llm
    
    # ä½¿ç”¨RunnableWithMessageHistoryåŒ…è£…é“¾ï¼Œå®ç°å¯¹è¯å†å²ç®¡ç†
    conversation = RunnableWithMessageHistory(
        runnable=chain,
        get_session_history=get_session_history,
        input_messages_key="input",  # æŒ‡å®šè¾“å…¥ä¿¡æ¯åœ¨é“¾ä¸­çš„key
        history_messages_key="history",  # æŒ‡å®šå†å²ä¿¡æ¯åœ¨æç¤ºæ¨¡æ¿ä¸­çš„key
        verbose=True
    )
    
    return conversation

def main() -> None:
    try:
        load_environment()
        
        # åˆ›å»ºå¸¦å†å²è®°å¿†çš„ä¼šè¯é“¾
        conversation = create_summary_chain()
        
        # æ¨¡æ‹Ÿå¤šè½®å¯¹è¯
        session_id = "test_session"
        
        # ç¬¬ä¸€è½®å¯¹è¯
        response = conversation.invoke(
            {"input": "ä½ å¥½ï¼Œæˆ‘å«å¼ ä¸‰ã€‚"},
            config={"configurable": {"session_id": session_id}}
        )
        print(f"AI: {response.content}")
        
        # ç¬¬äºŒè½®å¯¹è¯
        response = conversation.invoke(
            {"input": "æˆ‘æ˜¯ä¸€åè½¯ä»¶å·¥ç¨‹å¸ˆã€‚"},
            config={"configurable": {"session_id": session_id}}
        )
        print(f"AI: {response.content}")
        
        # ç¬¬ä¸‰è½®å¯¹è¯
        response = conversation.invoke(
            {"input": "æˆ‘å–œæ¬¢ç¼–ç¨‹å’Œæœºå™¨å­¦ä¹ ã€‚"},
            config={"configurable": {"session_id": session_id}}
        )
        print(f"AI: {response.content}")
        
        # ç¬¬å››è½®å¯¹è¯ - æµ‹è¯•è®°å¿†å’Œæ‘˜è¦èƒ½åŠ›
        response = conversation.invoke(
            {"input": "è¯·æ€»ç»“ä¸€ä¸‹æˆ‘ä»¬çš„å¯¹è¯å†…å®¹ã€‚"},
            config={"configurable": {"session_id": session_id}}
        )
        print(f"AIæ‘˜è¦: {response.content}")
        
    except Exception as e:
        print(f"å‘ç”Ÿé”™è¯¯: {e}")
        print("è¯·æ£€æŸ¥ OPENAI_API_KEY ä¸ OPENAI_MODEL æ˜¯å¦å·²é…ç½®ã€‚")


if __name__ == "__main__":
    main()
```

è¦ç‚¹ï¼š
- ç”¨ InMemoryChatMessageHistory å­˜å‚¨èŠå¤©çš„è®°å¿†ã€‚
- ç»è¿‡å‡ è½®çš„å¯¹è¯ï¼Œæœ€åå‘Šè¯‰æ¨¡å‹â€œè¯·æ€»ç»“ä¸€ä¸‹æˆ‘ä»¬çš„å¯¹è¯å†…å®¹ã€‚â€ï¼Œå®ç°å¯¹èŠå¤©è¿‡ç¨‹çš„æ•°æ®è¿›è¡Œå‹ç¼©ã€‚


## LangChain è®°å¿†æ¨¡å—æ€»ç»“

### æ ¸å¿ƒè®°å¿†ç±»å‹ä¸ç»“æ„

```
BaseChatMessageHistoryï¼ˆä¼šè¯æ¶ˆæ¯å†å²ï¼‰
â”œâ”€â”€ InMemoryChatMessageHistoryï¼ˆå†…å­˜ï¼‰
â”œâ”€â”€ FileChatMessageHistoryï¼ˆæ–‡ä»¶ï¼‰
â”œâ”€â”€ RedisChatMessageHistoryï¼ˆRedisï¼‰
â””â”€â”€ SQLChatMessageHistoryï¼ˆSQL/DBï¼‰

Memory Strategiesï¼ˆè®°å¿†ç­–ç•¥ï¼‰
â”œâ”€â”€ Limited Historyï¼ˆæ»‘çª—ä¿ç•™ï¼‰
â”œâ”€â”€ Token Compressionï¼ˆä»¤ç‰Œæ»‘çª—/æ‘˜è¦ï¼‰
â”œâ”€â”€ Summary Memoryï¼ˆæ‘˜è¦è®°å¿†ï¼‰
â”œâ”€â”€ Entity Memoryï¼ˆå®ä½“è®°å¿†ï¼‰
â”œâ”€â”€ VectorStore Memoryï¼ˆå‘é‡å­˜å‚¨è®°å¿†ï¼‰
â””â”€â”€ Key-Value Memoryï¼ˆé”®å€¼è®°å¿†ï¼‰
```

### è®°å¿†ç±»å‹åŠŸèƒ½å¯¹æ¯”

| è®°å¿†ç±»å‹ | ä¸»è¦ç”¨é€” | æŒä¹…æ€§ | ä¸Šä¸‹æ–‡ä¿ç•™ | å¤æ‚åº¦ | é€‚ç”¨åœºæ™¯ |
|----------|----------|--------|------------|--------|----------|
| å†…å­˜å­˜å‚¨ï¼ˆInMemoryï¼‰ | å•ä¼šè¯å¿«é€Ÿè¯»å†™ | å¦ | æœ€è¿‘ä¸Šä¸‹æ–‡ | ä½ | ä¸´æ—¶å¯¹è¯ã€åŸå‹éªŒè¯ |
| æ–‡ä»¶å­˜å‚¨ï¼ˆFileï¼‰ | æœ¬åœ°æŒä¹…åŒ– | æ˜¯ | å…¨é‡å¯æ§ | ä½ | ä¸ªäººé¡¹ç›®ã€å°å‹åº”ç”¨ |
| Redis/SQL å­˜å‚¨ | æœåŠ¡ç«¯æŒä¹…åŒ– | æ˜¯ | å…¨é‡å¯æ§ | ä¸­ | ç”Ÿäº§ç¯å¢ƒã€å¹¶å‘ä¸å¤šç”¨æˆ· |
| å‘é‡å­˜å‚¨è®°å¿† | è¯­ä¹‰æ£€ç´¢é•¿æœŸçŸ¥è¯† | æ˜¯ | ä¸»é¢˜è¯­ä¹‰ä¿ç•™ | ä¸­ | FAQsã€çŸ¥è¯†åº“ã€RAG ç»“åˆ |
| å®ä½“è®°å¿† | å…³é”®äººç‰©/åå¥½/äº‹å® | å¯é€‰ | å…³é”®å®ä½“å¼ºåŒ– | ä¸­ | ä¸ªæ€§åŒ–åŠ©æ‰‹ã€å®¢æˆ·ç”»åƒ |
| æ‘˜è¦è®°å¿† | é•¿å¯¹è¯å‹ç¼© | å¯é€‰ | ä¸»é¢˜ä¸è¦ç‚¹ | ä¸­ | é•¿ä¼šè¯æˆæœ¬æ§åˆ¶ |
| æ»‘çª—ä¿ç•™ | ä¿ç•™æœ€è¿‘ N æ¡ | å¦/æ˜¯ | è¿‘æœŸä¸Šä¸‹æ–‡ | ä½ | ç®€æ´é«˜æ•ˆã€æ— éœ€æ‘˜è¦ |
| ä»¤ç‰Œæ»‘çª—å‹ç¼© | ä»¤ç‰Œé¢„ç®—å†…ä¿ç•™ | å¯é€‰ | è¿‘æœŸ+æ—§æ‘˜è¦ | ä¸­ | æˆæœ¬æ•æ„Ÿã€ä¸Šä¸‹æ–‡å¹³è¡¡ |
| é”®å€¼è®°å¿† | å‚æ•°/é…ç½®/çŠ¶æ€ | å¯é€‰ | ç²¾å‡†å­—æ®µ | ä½ | å·¥å…·è°ƒç”¨ã€æµç¨‹çŠ¶æ€ |

### è®°å¿†é€‰æ‹©æŒ‡å—

1. ä¸´æ—¶å¯¹è¯/åŸå‹ â†’ `InMemoryChatMessageHistory`
   - å•ç”¨æˆ·æˆ–å•ä¼šè¯ã€é€Ÿåº¦ä¼˜å…ˆã€æ— éœ€æŒä¹…åŒ–ã€‚
2. æœ¬åœ°è½»é‡çº§æŒä¹…åŒ– â†’ `FileChatMessageHistory`
   - ä½è¿ç»´ã€æ˜“å¤‡ä»½ã€å¼€å‘æœºæˆ–ä¸ªäººé¡¹ç›®ã€‚
3. ç”Ÿäº§æŒä¹…åŒ–ä¸å¹¶å‘ â†’ `Redis/SQLChatMessageHistory`
   - å¤šç”¨æˆ·ã€å¯é å­˜å‚¨ã€å¯åšå®¡è®¡ä¸ç»Ÿè®¡ã€‚
4. é•¿æœŸçŸ¥è¯†ä¸æ£€ç´¢ â†’ å‘é‡å­˜å‚¨è®°å¿†ï¼ˆç»“åˆ RAGï¼‰
   - æ–‡æ¡£/FAQ/æ‰‹å†Œï¼Œè¯­ä¹‰å¬å›æå‡ä¸Šä¸‹æ–‡è´¨é‡ã€‚
5. ä¸ªæ€§åŒ–ä¸ç”»åƒ â†’ å®ä½“è®°å¿†ï¼ˆåå¥½/èº«ä»½/çº¦æŸï¼‰
   - æå–å¹¶æ›´æ–°å…³é”®å®ä½“ï¼Œå‡å°‘é‡å¤é—®ç­”ã€‚
6. é•¿ä¼šè¯æˆæœ¬æ§åˆ¶ â†’ æ‘˜è¦è®°å¿†æˆ–ä»¤ç‰Œæ»‘çª—å‹ç¼©
   - å®šæœŸæ‘˜è¦æ—§å¯¹è¯ï¼Œä¿ç•™è¿‘æœŸæ¶ˆæ¯ä¸å…³é”®äº‹å®ã€‚
7. å·¥å…·ä¸æµç¨‹å‚æ•° â†’ é”®å€¼è®°å¿†
   - ä¿å­˜ä¼šè¯ä¸Šä¸‹æ–‡ä¸­çš„æŒ‡ä»¤ã€é…ç½®ã€ä¸´æ—¶å˜é‡ã€‚

### æœ€ä½³å®è·µ

- å†å²é”®åä¸€è‡´ï¼š`MessagesPlaceholder("chat_history")` ä¸é“¾è¾“å…¥é”®ä¿æŒä¸€è‡´ã€‚
- ä¼šè¯éš”ç¦»ï¼šä¸ºä¸åŒç”¨æˆ·/ä¼šè¯è®¾ç½®ç‹¬ç«‹ `session_id` å¹¶æ­£ç¡®ä¼ é€’ã€‚
- æ··åˆç­–ç•¥ï¼šæ»‘çª— + æ‘˜è¦ + å®ä½“è®°å¿†è”åˆä½¿ç”¨ï¼Œå…¼é¡¾è¿‘æœŸä¸é•¿æœŸã€‚
- æˆæœ¬æ§åˆ¶ï¼šæŒ‰ä»¤ç‰Œé¢„ç®—è§¦å‘å‹ç¼©ï¼Œé¿å…è¶…ä¸Šä¸‹æ–‡çª—å£ä¸è´¹ç”¨æ¿€å¢ã€‚
- å†—ä½™æ²»ç†ï¼šå»é‡ç³»ç»Ÿæç¤ºä¸è§„åˆ™ã€åˆå¹¶å†—é•¿å›å¤ä¸æ— æ•ˆå™ªå£°ã€‚
- æŒä¹…åŒ–é€‰æ‹©ï¼šæœ¬åœ°ç”¨æ–‡ä»¶ï¼ŒæœåŠ¡ç«¯ç”¨ Redis/SQLï¼Œæ³¨æ„å¹¶å‘ä¸ä¸€è‡´æ€§ã€‚

### ç­–ç•¥ä¸å·¥å…·è¡¥å……

- ä»¤ç‰Œè®¡æ•°ï¼šä½¿ç”¨ `tiktoken` ä¼°ç®—ä¸Šä¸‹æ–‡é•¿åº¦ï¼ŒæŒ‰é¢„ç®—è§¦å‘å‹ç¼©ã€‚
- ä¼šè¯å°è£…ï¼šç”¨ `RunnableWithMessageHistory` é›†æˆé“¾ä¸å†å²ï¼Œç®€åŒ–å¤šè½®å¯¹è¯ã€‚
- æ‘˜è¦ç”Ÿæˆï¼šç³»ç»Ÿæç¤ºçº¦æŸâ€œä¿ç•™å…³é”®äº‹å®ä¸ä»»åŠ¡ä¸Šä¸‹æ–‡â€ï¼Œé¿å…ä¿¡æ¯ä¸¢å¤±ã€‚
- å®ä½“æŠ½å–ï¼šå®šæœŸä»å¯¹è¯æŠ½å–äººç‰©/åå¥½/ä»»åŠ¡çŠ¶æ€å¹¶æ›´æ–°å®ä½“å­˜å‚¨ã€‚
- æ£€ç´¢å¢å¼ºï¼šå‘é‡å­˜å‚¨ç»“åˆè®°å¿†ï¼Œä¼˜å…ˆå¬å›ç›¸å…³çŸ¥è¯†åå†æ³¨å…¥ä¸Šä¸‹æ–‡ã€‚



<a id="limited" data-alt="é™åˆ¶ å†å² æ»‘çª— Limited History"></a>
### èŠå¤©æ¶ˆæ¯è®°å¿†æ»‘çª—ä¿ç•™

```python

import os
from dotenv import load_dotenv
from langchain_core.messages import BaseMessage, HumanMessage, AIMessage
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_openai import ChatOpenAI


# ä»å½“å‰æ¨¡å—ç›®å½•åŠ è½½ .env
def load_environment():
    load_dotenv(dotenv_path=os.path.join(os.path.dirname(__file__), ".env"), override=False)

# è·å–é…ç½®çš„è¯­è¨€æ¨¡å‹
def get_llm() -> ChatOpenAI:
    """
    åˆ›å»ºå¹¶é…ç½®è¯­è¨€æ¨¡å‹å®ä¾‹
    
    Returns:
        ChatOpenAI: é…ç½®å¥½çš„è¯­è¨€æ¨¡å‹å®ä¾‹
        
    Raises:
        ValueError: å½“OPENAI_API_KEYæœªè®¾ç½®æ—¶æŠ›å‡º
        Exception: åˆ›å»ºæ¨¡å‹å®ä¾‹æ—¶å‘ç”Ÿçš„å…¶ä»–å¼‚å¸¸
    """
    try:
        api_key = os.getenv("OPENAI_API_KEY")
        if not api_key:
            raise ValueError("OPENAI_API_KEYç¯å¢ƒå˜é‡æœªè®¾ç½®ï¼Œè¯·æ£€æŸ¥.envæ–‡ä»¶æˆ–ç³»ç»Ÿç¯å¢ƒå˜é‡")
        
        model = os.getenv("OPENAI_MODEL", "gpt-3.5-turbo")
        base_url = os.getenv("OPENAI_BASE_URL")
        temperature = float(os.getenv("OPENAI_TEMPERATURE", "0"))
        max_tokens = int(os.getenv("OPENAI_MAX_TOKENS", "512"))

        print(f"ğŸ¤– åˆ›å»ºè¯­è¨€æ¨¡å‹ - æ¨¡å‹: {model}, æ¸©åº¦: {temperature}, æœ€å¤§ä»¤ç‰Œ: {max_tokens}")
        
        kwargs = {
            "model": model,
            "api_key": api_key,
            "temperature": temperature,
            "max_tokens": max_tokens,
            "timeout": 120,
            "max_retries": 3,
            "request_timeout": 120,
            "verbose": True,
            "base_url": base_url
        }

        return ChatOpenAI(**kwargs)
    except ValueError as e:
        print(f"âŒ é…ç½®é”™è¯¯: {e}")
        raise
    except Exception as e:
        print(f"âŒ åˆ›å»ºè¯­è¨€æ¨¡å‹å¤±è´¥: {e}")
        raise

# è‡ªå®šä¹‰æœ‰é™å†å²è®°å½•ç±»
class LimitedChatMessageHistory:
    """
    é™åˆ¶æ¶ˆæ¯æ•°é‡çš„èŠå¤©å†å²è®°å½•ç±»
    
    Attributes:
        max_messages (int): æœ€å¤§å…è®¸çš„æ¶ˆæ¯æ•°é‡
        _messages (list): å†…éƒ¨å­˜å‚¨çš„æ¶ˆæ¯åˆ—è¡¨
    """
    def __init__(self, max_messages: int = 10):
        """
        åˆå§‹åŒ–æœ‰é™å†å²è®°å½•ç±»
        
        Args:
            max_messages (int): æœ€å¤§å…è®¸çš„æ¶ˆæ¯æ•°é‡ï¼Œé»˜è®¤ä¸º10
            
        Raises:
            ValueError: å½“max_messageså°äºç­‰äº0æ—¶æŠ›å‡º
        """
        try:
            if max_messages <= 0:
                raise ValueError("max_messageså¿…é¡»å¤§äº0")
            
            self._messages = []
            self.max_messages = max_messages
            print(f"âœ… åˆå§‹åŒ–æœ‰é™å†å²è®°å½• - æœ€å¤§æ¶ˆæ¯æ•°: {max_messages}")
        except ValueError as e:
            print(f"âŒ åˆå§‹åŒ–å¤±è´¥: {e}")
            raise
        except Exception as e:
            print(f"âŒ LimitedChatMessageHistoryåˆå§‹åŒ–å¤±è´¥: {e}")
            raise
    
    def add_message(self, message: BaseMessage) -> None:
        """
        æ·»åŠ æ¶ˆæ¯å¹¶ä¿æŒæ¶ˆæ¯æ•°é‡é™åˆ¶
        
        Args:
            message (BaseMessage): è¦æ·»åŠ çš„æ¶ˆæ¯å¯¹è±¡
            
        Raises:
            TypeError: å½“messageä¸æ˜¯BaseMessageç±»å‹æ—¶æŠ›å‡º
        """
        try:
            if not isinstance(message, BaseMessage):
                raise TypeError("messageå¿…é¡»æ˜¯BaseMessageç±»å‹")
            
            self._messages.append(message)
            # è¶…è¿‡æœ€å¤§æ¶ˆæ¯æ•°æ—¶ï¼Œåˆ é™¤æœ€æ—§çš„æ¶ˆæ¯
            if len(self._messages) > self.max_messages:
                self._messages = self._messages[-self.max_messages:]
                print(f"âš ï¸ æ¶ˆæ¯æ•°é‡è¶…è¿‡é™åˆ¶ï¼Œå·²åˆ é™¤æ—§æ¶ˆæ¯ï¼Œå½“å‰ä¿ç•™: {len(self._messages)}æ¡")
        except Exception as e:
            print(f"âŒ æ·»åŠ æ¶ˆæ¯å¤±è´¥: {e}")
            raise
    
    def clear(self) -> None:
        """æ¸…é™¤æ‰€æœ‰æ¶ˆæ¯"""
        try:
            self._messages = []
            print("âœ… å·²æ¸…é™¤æ‰€æœ‰æ¶ˆæ¯")
        except Exception as e:
            print(f"âŒ æ¸…é™¤æ¶ˆæ¯å¤±è´¥: {e}")
            raise
    
    @property
    def messages(self):
        """è·å–æ¶ˆæ¯åˆ—è¡¨"""
        return self._messages
    
    @messages.setter
    def messages(self, value):
        """è®¾ç½®æ¶ˆæ¯åˆ—è¡¨"""
        self._messages = value

# ä½¿ç”¨å¸¦é™åˆ¶çš„å†å²è®°å½•
store = {}

def get_limited_history(session_id: str):
    """
    è·å–æˆ–åˆ›å»ºæœ‰é™å†å²è®°å½•å®ä¾‹
    
    Args:
        session_id (str): ä¼šè¯æ ‡è¯†ç¬¦
        
    Returns:
        LimitedChatMessageHistory: æœ‰é™å†å²è®°å½•å®ä¾‹
        
    Raises:
        ValueError: å½“session_idä¸ºç©ºæˆ–æ— æ•ˆæ—¶æŠ›å‡º
    """
    try:
        if not session_id or not isinstance(session_id, str):
            raise ValueError("session_idä¸èƒ½ä¸ºç©ºä¸”å¿…é¡»æ˜¯å­—ç¬¦ä¸²ç±»å‹")
        
        if session_id not in store:
            print(f"âœ… ä¸ºsession_id {session_id} åˆ›å»ºæ–°çš„LimitedChatMessageHistoryå®ä¾‹")
            # ç›´æ¥ä½¿ç”¨æˆ‘ä»¬è‡ªå®šä¹‰çš„ç±»ï¼Œä¸ç»§æ‰¿ChatMessageHistory
            store[session_id] = LimitedChatMessageHistory(max_messages=4)  # åªä¿ç•™4æ¡æ¶ˆæ¯
        else:
            print(f"âœ… è·å–ç°æœ‰æœ‰é™å†å²è®°å½• - session_id: {session_id}")
        return store[session_id]
    except Exception as e:
        print(f"âŒ è·å–æœ‰é™å†å²è®°å½•å¤±è´¥ - session_id: {session_id}, é”™è¯¯: {e}")
        raise

def main() -> None:
    """
    æ¼”ç¤ºæœ‰é™å†å²è®°å½•åŠŸèƒ½çš„ä¸»å‡½æ•°
    
    è¯¥å‡½æ•°å±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨LimitedChatMessageHistoryç±»æ¥é™åˆ¶èŠå¤©å†å²æ¶ˆæ¯æ•°é‡ï¼Œ
    åŒ…æ‹¬æ·»åŠ æ¶ˆæ¯ã€éªŒè¯æ¶ˆæ¯æ•°é‡é™åˆ¶ç­‰åŠŸèƒ½ã€‚
    
    Raises:
        Exception: æ‰§è¡Œè¿‡ç¨‹ä¸­å‘ç”Ÿçš„ä»»ä½•å¼‚å¸¸
    """
    try:
        # åŠ è½½ç¯å¢ƒå˜é‡
        print("ğŸ”§ åŠ è½½ç¯å¢ƒå˜é‡...")
        load_environment()
        
        session_id = "user_123"
        print(f"ğŸ“ è·å–å†å²è®°å½• - session_id: {session_id}")
        history = get_limited_history(session_id)
        
        # æµ‹è¯•æ·»åŠ æ¶ˆæ¯
        print("\nğŸ“¤ æ·»åŠ æµ‹è¯•æ¶ˆæ¯...")
        history.add_message(HumanMessage(content="ä½ å¥½ï¼Œæˆ‘æ˜¯å¼ ä¸‰"))
        history.add_message(AIMessage(content="ä½ å¥½å¼ ä¸‰ï¼Œæˆ‘æ˜¯AIåŠ©æ‰‹"))
        
        # éªŒè¯æ¶ˆæ¯æ•°é‡
        print(f"\nğŸ“Š å½“å‰æ¶ˆæ¯æ•°é‡: {len(history.messages)}")
        print(f"ğŸ“ æ¶ˆæ¯å†…å®¹: {[msg.content for msg in history.messages]}")
        
        # æµ‹è¯•æ¶ˆæ¯æ•°é‡é™åˆ¶
        print("\nğŸ§ª æµ‹è¯•æ¶ˆæ¯æ•°é‡é™åˆ¶...")
        for i in range(5):
            history.add_message(HumanMessage(content=f"æµ‹è¯•æ¶ˆæ¯ {i}"))
            history.add_message(AIMessage(content=f"æµ‹è¯•å“åº” {i}"))
            print(f"ğŸ“Š æ·»åŠ æ¶ˆæ¯åæ•°é‡: {len(history.messages)}")
            print(f"ğŸ“ ä¿ç•™çš„æ¶ˆæ¯: {[msg.content for msg in history.messages]}")
            
        print("\nâœ… æœ‰é™å†å²è®°å½•æµ‹è¯•å®Œæˆ")
            
    except Exception as e:
        print(f"âŒ é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        print("âš ï¸ è¯·æ£€æŸ¥OPENAI_API_KEYç­‰é…ç½®æ˜¯å¦æ­£ç¡®")


if __name__ == "__main__":
    main()
```

è¦ç‚¹ï¼š
- è¿™æ˜¯ä¸€ç§ä¼šé—æ¼éƒ¨åˆ†å¯¹è¯çš„å‹ç¼©æ–¹æ³•ã€‚
- ä¼˜ç‚¹æ˜¯ä¿ç•™äº†æœ€è¿‘çš„å¯¹è¯å†…å®¹ï¼Œé¿å…äº†ä¿¡æ¯ä¸¢å¤±ã€‚
- é€šè¿‡è®¾ç½®`max_messages`å‚æ•°å¯ä»¥æ§åˆ¶ä¿ç•™æœ€è¿‘å‡ æ¬¡çš„æ¶ˆæ¯æ•°é‡ã€‚

<a id="token-compression-window" data-alt="ä»¤ç‰Œ å‹ç¼© æ»‘çª— Window"></a>
### èŠå¤©æ¶ˆæ¯è®°å¿†ä»¤ç‰Œæ»‘çª—å‹ç¼©ï¼ˆTokenï¼‰

```python
import os
import tiktoken
from typing import Dict, List

from dotenv import load_dotenv
from langchain_core.chat_history import BaseChatMessageHistory, InMemoryChatMessageHistory
from langchain_core.messages import BaseMessage, get_buffer_string
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_openai import ChatOpenAI
from langchain.globals import set_debug



# ä»å½“å‰æ¨¡å—ç›®å½•åŠ è½½ .env
def load_environment():
    load_dotenv(dotenv_path=os.path.join(os.path.dirname(__file__), ".env"), override=False)

# è·å–é…ç½®çš„è¯­è¨€æ¨¡å‹
def get_llm() -> ChatOpenAI:
    """åˆ›å»ºå¹¶é…ç½®è¯­è¨€æ¨¡å‹å®ä¾‹"""
    api_key = os.getenv("OPENAI_API_KEY")
    model = os.getenv("OPENAI_MODEL", "gpt-3.5-turbo")
    base_url = os.getenv("OPENAI_BASE_URL")
    temperature = float(os.getenv("OPENAI_TEMPERATURE", "0"))
    max_tokens = int(os.getenv("OPENAI_MAX_TOKENS", "512"))

    kwargs = {
        "model": model,
        "api_key": api_key,
        "temperature": temperature,
        "max_tokens": max_tokens,
        "timeout": 120,
        "max_retries": 3,
        "request_timeout": 120,
        "verbose": True,
        "base_url": base_url
    }

    return ChatOpenAI(**kwargs)

# ä¼šè¯å­˜å‚¨
store: Dict[str, BaseChatMessageHistory] = {}

# è·å–ä¼šè¯å†å²
def get_session_history(session_id: str) -> BaseChatMessageHistory:
    """
    è·å–æˆ–åˆ›å»ºä¼šè¯å†å²è®°å½•
    
    Args:
        session_id (str): ä¼šè¯æ ‡è¯†ç¬¦
        
    Returns:
        BaseChatMessageHistory: ä¼šè¯å†å²è®°å½•å®ä¾‹
        
    Raises:
        ValueError: å½“session_idä¸ºç©ºæˆ–æ— æ•ˆæ—¶æŠ›å‡º
        Exception: è·å–ä¼šè¯å†å²æ—¶å‘ç”Ÿçš„å…¶ä»–å¼‚å¸¸
    """
    try:
        if not session_id or not isinstance(session_id, str):
            raise ValueError("session_idä¸èƒ½ä¸ºç©ºä¸”å¿…é¡»æ˜¯å­—ç¬¦ä¸²ç±»å‹")
        
        if session_id not in store:
            print(f"âœ… åˆ›å»ºæ–°çš„ä¼šè¯å†å²è®°å½• - session_id: {session_id}")
            store[session_id] = InMemoryChatMessageHistory()
        else:
            print(f"âœ… è·å–ç°æœ‰ä¼šè¯å†å²è®°å½• - session_id: {session_id}")
        return store[session_id]
    except Exception as e:
        print(f"âŒ è·å–ä¼šè¯å†å²å¤±è´¥ - session_id: {session_id}, é”™è¯¯: {e}")
        raise

# è‡ªå®šä¹‰ä»¤ç‰Œè®¡æ•°å‡½æ•°
def count_tokens(messages: List[BaseMessage], model: str = "gpt-3.5-turbo") -> int:
    """
    è®¡ç®—æ¶ˆæ¯çš„ä»¤ç‰Œæ•°é‡
    
    Args:
        messages (List[BaseMessage]): æ¶ˆæ¯åˆ—è¡¨
        model (str): æ¨¡å‹åç§°ï¼Œé»˜è®¤"gpt-3.5-turbo"
        
    Returns:
        int: æ¶ˆæ¯çš„ä»¤ç‰Œæ•°é‡
        
    Raises:
        Exception: ä»¤ç‰Œè®¡æ•°è¿‡ç¨‹ä¸­å‘ç”Ÿçš„å¼‚å¸¸
    """
    try:
        if not messages:
            print("â„¹ï¸ æ¶ˆæ¯åˆ—è¡¨ä¸ºç©ºï¼Œä»¤ç‰Œæ•°ä¸º0")
            return 0
            
        # ä½¿ç”¨tiktokenåº“è¿›è¡Œä»¤ç‰Œè®¡æ•°
        encoding = tiktoken.encoding_for_model(model)
        buffer_string = get_buffer_string(messages)
        token_count = len(encoding.encode(buffer_string))
        print(f"ğŸ”¢ ä»¤ç‰Œè®¡æ•° - æ¨¡å‹: {model}, æ¶ˆæ¯æ•°é‡: {len(messages)}, ä»¤ç‰Œæ•°: {token_count}")
        return token_count
    except KeyError:
        # å¦‚æœæ¨¡å‹ä¸æ”¯æŒï¼Œä½¿ç”¨è¿‘ä¼¼è®¡æ•°
        print(f"âš ï¸ æ¨¡å‹{model}ä¸æ”¯æŒï¼Œä½¿ç”¨è¿‘ä¼¼ä»¤ç‰Œè®¡æ•°")
        approximate_count = sum(len(str(msg.content)) // 4 for msg in messages)
        print(f"ğŸ”¢ è¿‘ä¼¼ä»¤ç‰Œè®¡æ•°: {approximate_count}")
        return approximate_count
    except Exception as e:
        print(f"âŒ ä»¤ç‰Œè®¡æ•°å¤±è´¥: {e}")
        # è¿”å›ä¸€ä¸ªå®‰å…¨çš„é»˜è®¤å€¼
        return sum(len(str(msg.content)) // 4 for msg in messages)

# å®ç°ç®€å•çš„ä»¤ç‰Œå‹ç¼©é€»è¾‘
def compress_messages_if_needed(session_id: str, max_tokens: int = 1000):
    """
    æ£€æŸ¥å¹¶å‹ç¼©ä¼šè¯å†å²æ¶ˆæ¯ï¼Œç¡®ä¿ä»¤ç‰Œæ•°ä¸è¶…è¿‡é™åˆ¶
    
    å½“ä¼šè¯å†å²æ¶ˆæ¯çš„ä»¤ç‰Œæ•°è¶…è¿‡æœ€å¤§é™åˆ¶æ—¶ï¼Œè¯¥æ–¹æ³•ä¼šä¿ç•™æœ€æ–°çš„å‡ æ¡æ¶ˆæ¯ï¼Œ
    å¹¶å¯¹æ—§æ¶ˆæ¯ç”Ÿæˆæ‘˜è¦ï¼Œä»è€Œå®ç°ä»¤ç‰Œå‹ç¼©çš„ç›®çš„ã€‚
    
    Args:
        session_id (str): ä¼šè¯æ ‡è¯†ç¬¦
        max_tokens (int): æœ€å¤§å…è®¸çš„ä»¤ç‰Œæ•°ï¼Œé»˜è®¤1000
        
    Returns:
        bool: æ˜¯å¦æ‰§è¡Œäº†å‹ç¼©æ“ä½œ
        
    Raises:
        Exception: å‹ç¼©è¿‡ç¨‹ä¸­å‘ç”Ÿçš„å¼‚å¸¸
    """
    try:
        history = get_session_history(session_id)
        current_tokens = count_tokens(history.messages)
        
        print(f"ğŸ” ä»¤ç‰Œå‹ç¼©æ£€æŸ¥ - ä¼šè¯ID: {session_id}, å½“å‰ä»¤ç‰Œæ•°: {current_tokens}, æœ€å¤§é™åˆ¶: {max_tokens}")
        
        if current_tokens > max_tokens:
            print(f"âš ï¸ ä»¤ç‰Œæ•°è¶…è¿‡é™åˆ¶ï¼Œå¼€å§‹å‹ç¼©å†å²æ¶ˆæ¯")
            
            # ä¿ç•™æœ€æ–°çš„å‡ æ¡æ¶ˆæ¯
            keep_recent = 2
            if len(history.messages) > keep_recent:
                # æå–è¦æ€»ç»“çš„æ—§æ¶ˆæ¯
                messages_to_summarize = history.messages[:-keep_recent]
                print(f"ğŸ“ éœ€è¦å‹ç¼©çš„æ¶ˆæ¯æ•°é‡: {len(messages_to_summarize)}")
                
                # ç”Ÿæˆæ‘˜è¦
                llm = get_llm()
                summary_prompt = ChatPromptTemplate.from_messages([
                    ("system", "è¯·ä¸ºä»¥ä¸‹å¯¹è¯ç”Ÿæˆç®€æ´çš„æ‘˜è¦ï¼Œä¿ç•™é‡è¦ä¿¡æ¯å’Œä¸Šä¸‹æ–‡ï¼š"),
                    ("human", "{conversation}")
                ])
                
                conversation_str = get_buffer_string(messages_to_summarize)
                summary_chain = summary_prompt | llm
                summary = summary_chain.invoke({"conversation": conversation_str})
                
                # é‡ç½®å†å²ï¼Œä¿ç•™æ‘˜è¦å’Œæœ€æ–°æ¶ˆæ¯
                history.clear()
                history.add_user_message(f"è¿‡å¾€å¯¹è¯æ‘˜è¦ï¼š{summary.content}")
                history.add_messages(history.messages[-keep_recent:])

                compressed_tokens = count_tokens(history.messages)
                print(f"âœ… æˆåŠŸå‹ç¼©å†å²æ¶ˆæ¯ï¼Œå‹ç¼©åä»¤ç‰Œæ•°: {compressed_tokens}")
                return True
            else:
                print(f"â„¹ï¸ æ¶ˆæ¯æ•°é‡ä¸è¶³ï¼Œæ— éœ€å‹ç¼©")
                return False
        else:
            print(f"â„¹ï¸ ä»¤ç‰Œæ•°åœ¨é™åˆ¶èŒƒå›´å†…ï¼Œæ— éœ€å‹ç¼©")
            return False
            
    except Exception as e:
        print(f"âŒ ä»¤ç‰Œå‹ç¼©å¤±è´¥ - ä¼šè¯ID: {session_id}, é”™è¯¯: {e}")
        print("âš ï¸ å‹ç¼©è¿‡ç¨‹ä¸­å‘ç”Ÿå¼‚å¸¸ï¼Œå†å²æ¶ˆæ¯ä¿æŒä¸å˜")
        return False


def create_conversation_chain():
    """
    åˆ›å»ºå¸¦å†å²è®°å¿†å’Œä»¤ç‰Œå‹ç¼©åŠŸèƒ½çš„ä¼šè¯é“¾
    
    Returns:
        RunnableWithMessageHistory: é…ç½®å¥½çš„ä¼šè¯é“¾å®ä¾‹
        
    Raises:
        Exception: åˆ›å»ºä¼šè¯é“¾è¿‡ç¨‹ä¸­å‘ç”Ÿçš„å¼‚å¸¸
    """
    try:
        print("ğŸ”— åˆ›å»ºä¼šè¯é“¾...")
        # åˆ›å»ºæ¨¡å‹
        llm = get_llm()

        # æ„å»ºæç¤ºæ¨¡æ¿
        prompt = ChatPromptTemplate.from_messages([
            ("system", "ä½ æ˜¯ä¸€ä¸ªå‹å¥½çš„AIåŠ©æ‰‹ã€‚"),
            MessagesPlaceholder(variable_name="chat_history"),
            ("human", "{input}"),
        ])

        # åˆ›å»ºåŸºæœ¬é“¾
        chain = prompt | llm

        # ä½¿ç”¨RunnableWithMessageHistoryé›†æˆè®°å¿†
        conversation = RunnableWithMessageHistory(
            chain,
            get_session_history,
            input_messages_key="input",
            history_messages_key="chat_history",
        )
        
        print("âœ… ä¼šè¯é“¾åˆ›å»ºæˆåŠŸ")
        return conversation
    except Exception as e:
        print(f"âŒ åˆ›å»ºä¼šè¯é“¾å¤±è´¥: {e}")
        raise

def main() -> None:
    """
    ä¸»å‡½æ•°ï¼šæ¼”ç¤ºä»¤ç‰Œå‹ç¼©è®°å¿†åŠŸèƒ½
    
    æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨ä»¤ç‰Œå‹ç¼©æ¥ç®¡ç†å¯¹è¯å†å²ï¼ŒåŒ…æ‹¬ï¼š
    1. åˆå§‹åŒ–ç¯å¢ƒ
    2. åˆ›å»ºä¼šè¯é“¾
    3. è¿›è¡Œå¤šè½®å¯¹è¯
    4. æ£€æŸ¥å¹¶å‹ç¼©æ¶ˆæ¯
    5. éªŒè¯è®°å¿†åŠŸèƒ½
    
    Raises:
        Exception: æ¼”ç¤ºè¿‡ç¨‹ä¸­å‘ç”Ÿçš„å¼‚å¸¸
    """
    try:
        print("ğŸš€ å¯åŠ¨ä»¤ç‰Œå‹ç¼©è®°å¿†æ¼”ç¤º...")
        set_debug(True)
        
        # åŠ è½½ç¯å¢ƒå˜é‡
        load_environment()
        
        # åˆ›å»ºä¼šè¯é“¾
        conversation = create_conversation_chain()
        session_id = "user_123"
        print(f"ğŸ“ ä½¿ç”¨ä¼šè¯ID: {session_id}")

        # ç¬¬ä¸€è½®å¯¹è¯
        print("\nğŸ’¬ ç¬¬ä¸€è½®å¯¹è¯...")
        response = conversation.invoke(
            {"input": "ä½ å¥½ï¼Œæˆ‘å«å¼ ä¸‰ã€‚"},
            config={"configurable": {"session_id": session_id}},
            verbose=True
        ) 
        print(f"ğŸ¤– AIå“åº”: {response.content}")
        
        # æ£€æŸ¥å¹¶å‹ç¼©æ¶ˆæ¯
        print("\nğŸ” æ£€æŸ¥ä»¤ç‰Œå‹ç¼©...")
        compress_messages_if_needed(session_id, max_tokens=1000)

        # ç¬¬äºŒè½®å¯¹è¯ï¼Œæ£€æŸ¥è®°å¿†åŠŸèƒ½
        print("\nğŸ’¬ ç¬¬äºŒè½®å¯¹è¯...")
        response = conversation.invoke(
            {"input": "æˆ‘åˆšæ‰å‘Šè¯‰ä½ æˆ‘å«ä»€ä¹ˆåå­—ï¼Ÿ"},
            config={"configurable": {"session_id": session_id}},
            verbose=True
        )
        print(f"ğŸ¤– AIå“åº”: {response.content}")

        # æ‰“å°å½“å‰è®°å¿†å†…å®¹
        print(f"\nğŸ“‹ å½“å‰è®°å¿†å†…å®¹:")
        history_messages = get_session_history(session_id).messages
        for i, message in enumerate(history_messages, 1):
            print(f"  {i}. {message.type}: {message.content}")
            
        print("\nâœ… ä»¤ç‰Œå‹ç¼©è®°å¿†æ¼”ç¤ºå®Œæˆ")

    except Exception as e:
        print(f"âŒ æ¼”ç¤ºè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        print("âš ï¸ è¯·æ£€æŸ¥ä»¥ä¸‹é…ç½®:")
        print("  - OPENAI_API_KEYç¯å¢ƒå˜é‡æ˜¯å¦æ­£ç¡®è®¾ç½®")
        print("  - ç½‘ç»œè¿æ¥æ˜¯å¦æ­£å¸¸")
        print("  - ä¾èµ–åŒ…æ˜¯å¦æ­£ç¡®å®‰è£…")
        raise

if __name__ == "__main__":
    main()
```

è¦ç‚¹ï¼š
- è¿™æ˜¯ä¸€ç§é€šè¿‡æ¯æ¬¡è¾“å…¥å¤§æ¨¡å‹å‰è®¡ç®—ä»¤ç‰Œï¼ˆTokenï¼‰æ•°é‡æ¥è®°å¿†æ€»ç»“ï¼ˆæ‘˜è¦ï¼‰æ–¹å¼å‹ç¼©è®°å¿†ã€‚
- ä¼˜ç‚¹æ˜¯ä¸€å®šä¸ä¼šè¶…å‡ºæœ€å¤§ä»¤ç‰Œï¼ˆTokenï¼‰æ•°é‡ï¼Œä»…æœ€å¤§å¯èƒ½å‹ç¼©è®°å¿†å’Œä¿ç•™æœ€è¿‘çš„æ¶ˆæ¯ã€‚
- é€šè¿‡`count_tokens`æ–¹æ³•è®¡ç®—å½“å‰è®°å¿†çš„ä»¤ç‰Œï¼ˆTokenï¼‰æ•°é‡ï¼Œè¶…è¿‡æœ€å¤§ä»¤ç‰Œï¼ˆTokenï¼‰æ•°é‡æ—¶è¿›è¡Œå‹ç¼©ã€‚
- å‹ç¼©é€»è¾‘ï¼šä¿ç•™æœ€æ–°çš„å‡ æ¡æ¶ˆæ¯ï¼Œç”Ÿæˆå¯¹æ—§æ¶ˆæ¯çš„æ‘˜è¦ï¼Œæ›¿æ¢æ—§æ¶ˆæ¯ã€‚


<a id="qa" data-alt="å¸¸è§ é”™è¯¯ æ’æŸ¥ Q/A"></a>
## å¸¸è§é”™è¯¯ä¸å¿«é€Ÿæ’æŸ¥ (Q/A)

- è®°å¿†æœªç”Ÿæ•ˆï¼šæ ¸å¯¹ <code>MessagesPlaceholder("history")</code> ä¸ <code>input/history</code> é”®åä¸€è‡´ã€‚
- ä¼šè¯ä¸²çº¿ï¼šç¡®ä¿ä¸ºä¸åŒç”¨æˆ·è®¾ç½®ä¸åŒ <code>session_id</code>ï¼Œå¹¶æ­£ç¡®ä¼ é€’åˆ°é…ç½®ä¸­ã€‚

<a id="qa-storage-choice"></a>
### å­˜å‚¨ç±»å‹é€‰æ‹©æŒ‡å—

- å†…å­˜å­˜å‚¨ï¼šä¼šè¯å†…ã€é€Ÿåº¦å¿«ã€ä¸æŒä¹…ï¼›é€‚åˆä¸´æ—¶å¯¹è¯ä¸å°å‹åº”ç”¨ã€‚
- æ–‡ä»¶/æ•°æ®åº“ï¼šè·¨ä¼šè¯ã€å¯æŒä¹…ï¼›æ–‡ä»¶é€‚åˆæœ¬åœ°ä¸è½»é‡çº§ï¼ŒRedis/SQL é€‚åˆç”Ÿäº§ç¯å¢ƒã€‚
- å‘é‡å­˜å‚¨ï¼šç”¨äºè¯­ä¹‰æ£€ç´¢ä¸é•¿æœŸçŸ¥è¯†åº“ï¼Œç»“åˆè®°å¿†æå‡å¬å›è´¨é‡ã€‚

<a id="qa-window-vs-compression"></a>
### æ»‘çª— vs ä»¤ç‰Œå‹ç¼©å¦‚ä½•å–èˆ

- æ»‘çª—ä¿ç•™ï¼šå›ºå®šä¿ç•™æœ€è¿‘ N æ¡æ¶ˆæ¯ï¼Œç®€å•é«˜æ•ˆä½†å¯èƒ½ä¸¢å¤±é•¿æœŸä¸Šä¸‹æ–‡ã€‚
- ä»¤ç‰Œæ»‘çª—å‹ç¼©ï¼šæŒ‰ä»¤ç‰Œé¢„ç®—ä¿ç•™æœ€è¿‘å†…å®¹å¹¶æ‘˜è¦æ—§æ¶ˆæ¯ï¼Œæ›´å¹³è¡¡ä¸Šä¸‹æ–‡ä¸æˆæœ¬ã€‚

<a id="qa-providers"></a>
### ä¸åŒæ¨¡å‹/ä¾›åº”å•†ä¸‹å¯ç”¨è®°å¿†

- OpenAI/DeepSeekï¼šæŒ‰å®˜æ–¹ SDK é…ç½®ï¼Œç¡®ä¿ `max_tokens` ä¸è¶…æ—¶è®¾ç½®åˆç†ã€‚
- Ollama/æœ¬åœ°æ¨¡å‹ï¼šå…³æ³¨ä¸Šä¸‹æ–‡çª—å£å¤§å°ä¸ç”Ÿæˆé€Ÿåº¦ï¼Œé€‚å½“é™ä½æ¸©åº¦ä¸é•¿åº¦ã€‚

<a id="qa-context-bloat"></a>
### é¿å…ä¸Šä¸‹æ–‡çˆ†ç‚¸çš„å®ç”¨å»ºè®®

- æ§åˆ¶æ¶ˆæ¯å†—ä½™ã€åˆå¹¶å†—é•¿å›å¤ï¼›è®¾ç½®å†å²ä¸Šé™ä¸å®šæœŸæ‘˜è¦ç­–ç•¥ã€‚
- å¯¹ç³»ç»Ÿæç¤ºä¸è§„åˆ™åšå»é‡ä¸å‹ç¼©ï¼Œé¿å…é‡å¤æ³¨å…¥ã€‚

<a id="qa-eval"></a>
### å‹ç¼©æŸå¤±çš„è¯„ä¼°ä¸ç›‘æ§

- å¯¹æ‘˜è¦å¬å›ç‡ã€å…³é”®å®ä½“å®Œæ•´æ€§ã€å¯¹è¯ä¸€è‡´æ€§åšæœ€å°è¯„æµ‹ã€‚
- ç›‘æ§ä¸Šä¸‹æ–‡é•¿åº¦ä¸å“åº”è´¨é‡ï¼Œå¿…è¦æ—¶åˆ‡æ¢/æ··åˆå‹ç¼©ç­–ç•¥ã€‚


<a id="links"></a>
### è¯¦ç»†ä»£ç å’Œæ–‡æ¡£

- å®Œæ•´ä»£ç ï¼šæŸ¥çœ‹ [GitHub ä»“åº“](https://github.com/kakaCat/langchain-learn/tree/04-memory/01_01_in_memory_history_demo.py)
- é¡¹ç›®ç»“æ„ï¼šå‚è€ƒä»“åº“ä¸­çš„ `README.md`
- LangChain æ–‡æ¡£ï¼šhttps://python.langchain.com/
- LangChain OpenAI é›†æˆï¼ˆPython API ç´¢å¼•ï¼‰ï¼šhttps://api.python.langchain.com/
- DeepSeek API æ–‡æ¡£ï¼šhttps://api-docs.deepseek.com/
- OpenAI API æ–‡æ¡£ï¼šhttps://platform.openai.com/docs/api-reference
- Azure OpenAI æ–‡æ¡£ï¼šhttps://learn.microsoft.com/azure/ai-services/openai/
- Ollama æ–‡æ¡£ï¼šhttps://ollama.com/docs


<a id="references" data-alt="å¼•ç”¨ å‚è€ƒ æ–‡çŒ® é“¾æ¥"></a>
## å‚è€ƒèµ„æ–™

- LangChain Memory æŒ‡å—ï¼šhttps://python.langchain.com/docs/modules/memory/
- RunnableWithMessageHistoryï¼ˆè¡¨è¾¾å¼è¯­è¨€è®°å¿†ï¼‰ï¼šhttps://python.langchain.com/docs/expression_language/memory/
- ChatMessageHistoryï¼ˆèŠå¤©æ¶ˆæ¯è®°å¿†ï¼‰ï¼šhttps://python.langchain.com/docs/modules/memory/chat_messages/
- é¡¹ç›®ç¤ºä¾‹ç›®å½•ï¼š`./04-memory/`ã€`./11-token-compression/`
- ç›¸å…³æ•™ç¨‹ï¼š[`langchain-chatbot-tutorial.md`](./langchain-chatbot-tutorial.md)ã€[`langchain-prompt-templates-tutorial.md`](./langchain-prompt-templates-tutorial.md)


<a id="summary-final" data-alt="æ€»ç»“ æ”¶å°¾ æœ€ä½³å®è·µ"></a>
## æ€»ç»“

