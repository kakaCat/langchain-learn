#!/usr/bin/env python3
"""
13 - å¤šè½®æ¾„æ¸…å¯¹è¯ (Multi-Round Clarification)

æ¼”ç¤ºå¦‚ä½•å®ç°å¤šè½®å¯¹è¯å¼æ¾„æ¸…ï¼š
1. æ”¯æŒå¤šè½®è¿­ä»£æ¾„æ¸…
2. åŸºäºä¸Šä¸€è½®å›ç­”ç”Ÿæˆä¸‹ä¸€è½®é—®é¢˜
3. åŠ¨æ€è°ƒæ•´æ¾„æ¸…ç­–ç•¥
4. è‡ªåŠ¨åˆ¤æ–­ä½•æ—¶åœæ­¢æ¾„æ¸…

æŠ€æœ¯ç‰¹ç‚¹ï¼š
- Iterative Clarificationï¼ˆè¿­ä»£å¼æ¾„æ¸…ï¼‰
- Context-Aware Question Generationï¼ˆä¸Šä¸‹æ–‡æ„ŸçŸ¥é—®é¢˜ç”Ÿæˆï¼‰
- Adaptive Questioning Strategyï¼ˆè‡ªé€‚åº”æé—®ç­–ç•¥ï¼‰
- Automatic Stopping Criterionï¼ˆè‡ªåŠ¨åœæ­¢æ¡ä»¶ï¼‰

é€‚ç”¨åœºæ™¯ï¼š
- å¤æ‚éœ€æ±‚åˆ†æ
- ä¸ªæ€§åŒ–å’¨è¯¢æœåŠ¡
- æ™ºèƒ½å®¢æœå¯¹è¯
- åŒ»ç–—è¯Šæ–­åŠ©æ‰‹
"""

from __future__ import annotations

import json
import os
from datetime import datetime
from typing import Dict, List, Literal, Optional

from dotenv import load_dotenv
from langchain_core.messages import HumanMessage
from langchain_openai import ChatOpenAI
from langchain_ollama import ChatOllama
from langgraph.graph import END, StateGraph
from pydantic import BaseModel, Field, ValidationError


def load_environment() -> None:
    load_dotenv(
        dotenv_path=os.path.join(os.path.dirname(__file__), ".env"), override=False
    )


def get_llm(model: Optional[str] = None, temperature: float = 0.3) -> object:
    provider = os.getenv("LLM_PROVIDER", "").lower()
    use_ollama = provider in {"ollama", "local"} or not os.getenv("OPENAI_API_KEY")

    if use_ollama:
        model_name = model or os.getenv("OLLAMA_MODEL", "llama3.1:8b")
        base_url = os.getenv("OLLAMA_BASE_URL", "http://localhost:11434")
        return ChatOllama(model=model_name, base_url=base_url, temperature=temperature)
    else:
        api_key = os.getenv("OPENAI_API_KEY")
        base_url = os.getenv("OPENAI_BASE_URL")
        model_name = model or os.getenv("OPENAI_MODEL", "gpt-4o-mini")
        return ChatOpenAI(
            model=model_name,
            api_key=api_key,
            base_url=base_url,
            temperature=temperature,
            max_tokens=1500,
        )


# ============================================================================
# æ•°æ®æ¨¡å‹
# ============================================================================


class Question(BaseModel):
    """å•ä¸ªé—®é¢˜"""

    question: str
    reason: str
    question_type: Literal["scope", "preference", "constraint", "context"] = "context"
    options: Optional[List[str]] = None


class ClarificationRound(BaseModel):
    """å•è½®æ¾„æ¸…"""

    round_number: int
    questions: List[Question]
    answers: Dict[str, str] = Field(default_factory=dict)
    timestamp: str = Field(default_factory=lambda: datetime.now().isoformat())


class ContinueAssessment(BaseModel):
    """ç»§ç»­æ¾„æ¸…çš„è¯„ä¼°"""

    should_continue: bool
    reason: str
    completeness: float = Field(ge=0.0, le=1.0)  # éœ€æ±‚å®Œæ•´åº¦ 0-1


class MultiRoundState(BaseModel):
    """å¤šè½®å¯¹è¯çŠ¶æ€"""

    user_request: str
    clarification_rounds: List[ClarificationRound] = Field(default_factory=list)
    current_round: int = 0
    max_rounds: int = Field(default=3)
    should_continue_clarification: bool = True
    requirement_summary: Optional[str] = None
    execution_plan: List[str] = Field(default_factory=list)
    final_output: Optional[str] = None


# ============================================================================
# è¾…åŠ©å‡½æ•°
# ============================================================================


def parse_json_safely(
    llm, prompt: str, target_model: type[BaseModel]
) -> Optional[BaseModel]:
    """å®‰å…¨è§£æ JSON"""
    try:
        response = llm.invoke([HumanMessage(content=prompt)])
        content = response.content.strip()

        if "```json" in content:
            content = content.split("```json")[1].split("```")[0].strip()
        elif "```" in content:
            content = content.split("```")[1].split("```")[0].strip()

        data = json.loads(content)
        return target_model(**data)
    except Exception as e:
        print(f"[è­¦å‘Š] JSON è§£æå¤±è´¥ï¼š{e}")
        return None


# ============================================================================
# èŠ‚ç‚¹å®ç°
# ============================================================================


def generate_questions_node(state: MultiRoundState) -> MultiRoundState:
    """ç”Ÿæˆæœ¬è½®é—®é¢˜"""
    if state.current_round >= state.max_rounds:
        state.should_continue_clarification = False
        return state

    llm = get_llm()

    # æ”¶é›†ä¹‹å‰çš„å¯¹è¯å†å²
    history = []
    for r in state.clarification_rounds:
        for q, a in r.answers.items():
            if a != "ï¼ˆè·³è¿‡ï¼‰":
                history.append(f"Q{r.round_number}: {q}\nA{r.round_number}: {a}")

    history_text = "\n\n".join(history) if history else "ï¼ˆæ— å†å²å¯¹è¯ï¼‰"

    prompt = f"""
ç”¨æˆ·åŸå§‹éœ€æ±‚ï¼š{state.user_request}

å·²æœ‰å¯¹è¯å†å²ï¼š
{history_text}

å½“å‰æ˜¯ç¬¬ {state.current_round + 1} è½®æ¾„æ¸…ï¼ˆå…±æœ€å¤š {state.max_rounds} è½®ï¼‰ã€‚

è¯·åŸºäºå½“å‰ä¿¡æ¯ç”Ÿæˆ 1-2 ä¸ªé—®é¢˜ã€‚è¦æ±‚ï¼š
1. åŸºäºå·²æœ‰å›ç­”æ·±å…¥æŒ–æ˜
2. é¿å…é‡å¤å·²é—®è¿‡çš„é—®é¢˜
3. èšç„¦æœ€å…³é”®çš„ä¿¡æ¯ç¼ºå£

è¾“å‡º JSONï¼š

{{
  "questions": [
    {{
      "question": "å…·ä½“é—®é¢˜",
      "reason": "ä¸ºä»€ä¹ˆé—®è¿™ä¸ª",
      "question_type": "scope|preference|constraint|context",
      "options": ["é€‰é¡¹1", "é€‰é¡¹2"]
    }}
  ]
}}

å¦‚æœä¿¡æ¯å·²ç»è¶³å¤Ÿï¼Œè¿”å›ç©ºæ•°ç»„ {{"questions": []}}
"""

    result = parse_json_safely(llm, prompt, type("TempModel", (BaseModel,), {
        "__annotations__": {"questions": List[Question]},
        "questions": Field(default_factory=list)
    }))

    questions = result.questions if result else []

    if not questions:
        print(f"\n[ç¬¬{state.current_round + 1}è½®] æ— éœ€è¿›ä¸€æ­¥æ¾„æ¸…")
        state.should_continue_clarification = False
        return state

    # åˆ›å»ºæ–°ä¸€è½®
    new_round = ClarificationRound(
        round_number=state.current_round + 1, questions=questions
    )
    state.clarification_rounds.append(new_round)

    print(f"\n[ç¬¬{state.current_round + 1}è½®] ç”Ÿæˆ {len(questions)} ä¸ªé—®é¢˜")

    return state


def ask_questions_node(state: MultiRoundState) -> MultiRoundState:
    """å‘ç”¨æˆ·æé—®"""
    if not state.clarification_rounds:
        return state

    current_round = state.clarification_rounds[-1]

    print("\n" + "=" * 80)
    print(f"ğŸ’¬ ç¬¬ {current_round.round_number} è½®æ¾„æ¸…")
    print("=" * 80)

    answers = {}
    for i, q in enumerate(current_round.questions, 1):
        print(f"\né—®é¢˜ {i} [{q.question_type}]:")
        print(f"  {q.question}")
        print(f"  ğŸ’¡ {q.reason}")

        if q.options:
            print(f"  é€‰é¡¹ï¼š")
            for j, opt in enumerate(q.options, 1):
                print(f"    {j}. {opt}")
            user_input = input(f"\n  æ‚¨çš„é€‰æ‹©ï¼ˆ1-{len(q.options)} æˆ–è‡ªå®šä¹‰ï¼‰[å›è½¦è·³è¿‡]: ").strip()

            if user_input.isdigit() and 1 <= int(user_input) <= len(q.options):
                answers[q.question] = q.options[int(user_input) - 1]
            elif user_input:
                answers[q.question] = user_input
            else:
                answers[q.question] = "ï¼ˆè·³è¿‡ï¼‰"
        else:
            user_input = input("  æ‚¨çš„å›ç­” [å›è½¦è·³è¿‡]: ").strip()
            answers[q.question] = user_input if user_input else "ï¼ˆè·³è¿‡ï¼‰"

    current_round.answers = answers
    state.current_round += 1

    print(f"\nâœ“ ç¬¬ {current_round.round_number} è½®å®Œæˆ")
    return state


def assess_continuation_node(state: MultiRoundState) -> MultiRoundState:
    """è¯„ä¼°æ˜¯å¦ç»§ç»­æ¾„æ¸…"""
    if state.current_round >= state.max_rounds:
        print(f"\n[è¯„ä¼°] è¾¾åˆ°æœ€å¤§è½®æ•° ({state.max_rounds})ï¼Œåœæ­¢æ¾„æ¸…")
        state.should_continue_clarification = False
        return state

    llm = get_llm()

    # æ”¶é›†æ‰€æœ‰å¯¹è¯
    all_qa = []
    for r in state.clarification_rounds:
        for q, a in r.answers.items():
            if a != "ï¼ˆè·³è¿‡ï¼‰":
                all_qa.append(f"- {q} â†’ {a}")

    qa_text = "\n".join(all_qa) if all_qa else "ï¼ˆæ— æœ‰æ•ˆå›ç­”ï¼‰"

    prompt = f"""
åŸå§‹éœ€æ±‚ï¼š{state.user_request}

å·²æ¾„æ¸…ä¿¡æ¯ï¼š
{qa_text}

å½“å‰è½®æ¬¡ï¼š{state.current_round}/{state.max_rounds}

è¯·è¯„ä¼°æ˜¯å¦éœ€è¦ç»§ç»­æ¾„æ¸…ã€‚è¾“å‡º JSONï¼š

{{
  "should_continue": false,
  "reason": "è¯„ä¼°ç†ç”±",
  "completeness": 0.85
}}

è¯„ä¼°æ ‡å‡†ï¼š
- completeness >= 0.80 æ—¶å»ºè®®åœæ­¢
- å…³é”®ä¿¡æ¯å·²æ”¶é›†å®Œæ¯•å»ºè®®åœæ­¢
- ç”¨æˆ·é¢‘ç¹è·³è¿‡é—®é¢˜å»ºè®®åœæ­¢
"""

    result = parse_json_safely(llm, prompt, ContinueAssessment)

    if not result:
        # å›é€€
        result = ContinueAssessment(
            should_continue=state.current_round < state.max_rounds,
            reason="é»˜è®¤ç­–ç•¥",
            completeness=0.5,
        )

    state.should_continue_clarification = result.should_continue

    print(f"\n[è¯„ä¼°] å®Œæ•´åº¦ï¼š{result.completeness:.0%}")
    print(f"  {result.reason}")
    print(f"  ç»§ç»­æ¾„æ¸…ï¼š{result.should_continue}")

    return state


def summarize_requirements_node(state: MultiRoundState) -> MultiRoundState:
    """æ€»ç»“éœ€æ±‚"""
    llm = get_llm()

    # æ”¶é›†æ‰€æœ‰æ¾„æ¸…ä¿¡æ¯
    all_info = []
    for r in state.clarification_rounds:
        for q, a in r.answers.items():
            if a != "ï¼ˆè·³è¿‡ï¼‰":
                all_info.append(f"- {q} â†’ {a}")

    clarifications = "\n".join(all_info) if all_info else "ï¼ˆæ— æ¾„æ¸…ä¿¡æ¯ï¼‰"

    prompt = f"""
åŸå§‹éœ€æ±‚ï¼š{state.user_request}

æ¾„æ¸…ä¿¡æ¯ï¼š
{clarifications}

è¯·æ•´åˆæ‰€æœ‰ä¿¡æ¯ï¼Œç”Ÿæˆå®Œæ•´çš„éœ€æ±‚æè¿°ï¼ˆ2-3 æ®µï¼‰ã€‚
"""

    response = llm.invoke([HumanMessage(content=prompt)])
    state.requirement_summary = response.content.strip()

    print("\n" + "=" * 80)
    print("ğŸ“‹ éœ€æ±‚æ€»ç»“")
    print("=" * 80)
    print(state.requirement_summary)

    return state


def plan_and_execute_node(state: MultiRoundState) -> MultiRoundState:
    """åˆ¶å®šè®¡åˆ’å¹¶æ‰§è¡Œ"""
    llm = get_llm()

    requirement = state.requirement_summary or state.user_request

    # åˆ¶å®šè®¡åˆ’
    plan_prompt = f"""
éœ€æ±‚ï¼š{requirement}

è¯·åˆ¶å®šæ‰§è¡Œè®¡åˆ’ï¼ˆ3-5 æ­¥ï¼‰ã€‚
åªè¾“å‡º JSON æ•°ç»„ï¼š["æ­¥éª¤1", "æ­¥éª¤2", ...]
"""

    response = llm.invoke([HumanMessage(content=plan_prompt)])
    content = response.content.strip()

    if "```json" in content:
        content = content.split("```json")[1].split("```")[0].strip()
    elif "```" in content:
        content = content.split("```")[1].split("```")[0].strip()

    try:
        steps = json.loads(content)
        state.execution_plan = [str(s) for s in steps]
    except:
        state.execution_plan = [requirement]

    print(f"\n[è®¡åˆ’] {len(state.execution_plan)} ä¸ªæ­¥éª¤")
    for i, step in enumerate(state.execution_plan, 1):
        print(f"  {i}. {step}")

    # æ‰§è¡Œè®¡åˆ’ï¼ˆç®€åŒ–ç‰ˆï¼‰
    exec_prompt = f"""
éœ€æ±‚ï¼š{requirement}

è®¡åˆ’ï¼š{chr(10).join(f'{i+1}. {s}' for i, s in enumerate(state.execution_plan))}

è¯·æ€»ç»“æ‰§è¡Œç»“æœï¼ˆ3-4 æ®µï¼‰ã€‚
"""

    final_response = llm.invoke([HumanMessage(content=exec_prompt)])
    state.final_output = final_response.content.strip()

    print("\n" + "=" * 80)
    print("âœ… æ‰§è¡Œç»“æœ")
    print("=" * 80)
    print(state.final_output)

    return state


# ============================================================================
# å·¥ä½œæµæ„å»º
# ============================================================================


def create_multi_round_workflow():
    """åˆ›å»ºå¤šè½®æ¾„æ¸…å·¥ä½œæµ"""
    graph = StateGraph(MultiRoundState)

    # æ·»åŠ èŠ‚ç‚¹
    graph.add_node("generate", generate_questions_node)
    graph.add_node("ask", ask_questions_node)
    graph.add_node("assess", assess_continuation_node)
    graph.add_node("summarize", summarize_requirements_node)
    graph.add_node("execute", plan_and_execute_node)

    # å®šä¹‰æµç¨‹
    graph.set_entry_point("generate")

    # æ¡ä»¶åˆ†æ”¯ï¼šå¦‚æœç”Ÿæˆäº†é—®é¢˜ï¼Œåˆ™è¯¢é—®ç”¨æˆ·
    graph.add_conditional_edges(
        "generate",
        lambda s: "ask" if s.clarification_rounds and s.clarification_rounds[-1].questions else "summarize",
        {"ask": "ask", "summarize": "summarize"},
    )

    graph.add_edge("ask", "assess")

    # æ¡ä»¶åˆ†æ”¯ï¼šæ˜¯å¦ç»§ç»­æ¾„æ¸…
    graph.add_conditional_edges(
        "assess",
        lambda s: "generate" if s.should_continue_clarification else "summarize",
        {"generate": "generate", "summarize": "summarize"},
    )

    graph.add_edge("summarize", "execute")
    graph.add_edge("execute", END)

    return graph.compile()


# ============================================================================
# ä¸»å‡½æ•°
# ============================================================================


def run_demo():
    """è¿è¡Œæ¼”ç¤º"""
    load_environment()
    workflow = create_multi_round_workflow()

    print("\n" + "=" * 80)
    print("å¤šè½®æ¾„æ¸…å¯¹è¯ Agent æ¼”ç¤º")
    print("=" * 80)
    print("\næç¤ºï¼šAgent ä¼šè¿›è¡Œæœ€å¤š 3 è½®æ¾„æ¸…ï¼ŒåŸºäºæ‚¨çš„å›ç­”é€æ­¥ç»†åŒ–éœ€æ±‚ã€‚\n")

    user_input = input("è¯·è¾“å…¥æ‚¨çš„éœ€æ±‚ï¼ˆå›è½¦ä½¿ç”¨é»˜è®¤ï¼‰: ").strip()
    if not user_input:
        user_input = "å¸®æˆ‘è®¾è®¡ä¸€ä¸ªå­¦ä¹ è®¡åˆ’"
        print(f"ä½¿ç”¨é»˜è®¤éœ€æ±‚ï¼š{user_input}")

    state = MultiRoundState(user_request=user_input, max_rounds=3)

    try:
        final_state = workflow.invoke(state)

        print("\n" + "=" * 80)
        print("ğŸ“Š å¯¹è¯ç»Ÿè®¡")
        print("=" * 80)
        print(f"æ€»è½®æ•°ï¼š{len(final_state.clarification_rounds)}")

        for r in final_state.clarification_rounds:
            valid_answers = sum(1 for a in r.answers.values() if a != "ï¼ˆè·³è¿‡ï¼‰")
            print(
                f"\nç¬¬ {r.round_number} è½®ï¼š"
                f"\n  é—®é¢˜æ•°ï¼š{len(r.questions)}"
                f"\n  æœ‰æ•ˆå›ç­”ï¼š{valid_answers}/{len(r.answers)}"
            )

    except Exception as e:
        print(f"\næ‰§è¡Œå‡ºé”™ï¼š{e}")
        import traceback

        traceback.print_exc()


if __name__ == "__main__":
    run_demo()
