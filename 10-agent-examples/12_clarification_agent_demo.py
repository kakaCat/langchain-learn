#!/usr/bin/env python3
"""
12 - æ™ºèƒ½æ¾„æ¸… Agent (Intelligent Clarification Agent)

æ¼”ç¤ºå¦‚ä½•æ„å»ºä¸»åŠ¨æé—®çš„æ™ºèƒ½ Agentï¼š
1. è‡ªåŠ¨æ£€æµ‹éœ€æ±‚æ˜¯å¦éœ€è¦æ¾„æ¸…
2. ç”Ÿæˆç»“æ„åŒ–çš„æ¾„æ¸…é—®é¢˜
3. åŸºäºç”¨æˆ·åé¦ˆè°ƒæ•´æ‰§è¡Œç­–ç•¥
4. æ”¯æŒé—®é¢˜ç±»å‹åˆ†ç±»å’Œç´§è¿«æ€§è¯„ä¼°

æŠ€æœ¯ç‰¹ç‚¹ï¼š
- Proactive Questioningï¼ˆä¸»åŠ¨æé—®ï¼‰
- Structured Question Generationï¼ˆç»“æ„åŒ–é—®é¢˜ç”Ÿæˆï¼‰
- Context-Aware Clarificationï¼ˆä¸Šä¸‹æ–‡æ„ŸçŸ¥æ¾„æ¸…ï¼‰
- Adaptive Executionï¼ˆè‡ªé€‚åº”æ‰§è¡Œï¼‰

é€‚ç”¨åœºæ™¯ï¼š
- éœ€æ±‚æ¨¡ç³Šçš„ä»»åŠ¡
- å¤šé¢†åŸŸäº¤å‰ä¸»é¢˜
- ä¸ªæ€§åŒ–æ¨èç³»ç»Ÿ
- æ™ºèƒ½å®¢æœå’ŒåŠ©æ‰‹
"""

from __future__ import annotations

import json
import os
from datetime import datetime
from typing import Dict, List, Literal, Optional

from dotenv import load_dotenv
from langchain_core.messages import HumanMessage, SystemMessage
from langchain_openai import ChatOpenAI
from langchain_ollama import ChatOllama
from langgraph.graph import END, StateGraph
from pydantic import BaseModel, Field, ValidationError


def load_environment() -> None:
    load_dotenv(
        dotenv_path=os.path.join(os.path.dirname(__file__), ".env"), override=False
    )


def get_llm(model: Optional[str] = None, temperature: float = 0.2) -> object:
    provider = os.getenv("LLM_PROVIDER", "").lower()
    use_ollama = provider in {"ollama", "local"} or not os.getenv("OPENAI_API_KEY")

    if use_ollama:
        model_name = model or os.getenv("OLLAMA_MODEL", "llama3.1:8b")
        base_url = os.getenv("OLLAMA_BASE_URL", "http://localhost:11434")
        return ChatOllama(model=model_name, base_url=base_url, temperature=temperature)
    else:
        api_key = os.getenv("OPENAI_API_KEY")
        base_url = os.getenv("OPENAI_BASE_URL")
        model_name = model or os.getenv("OPENAI_MODEL", "gpt-4o-mini")
        return ChatOpenAI(
            model=model_name,
            api_key=api_key,
            base_url=base_url,
            temperature=temperature,
            max_tokens=1500,
        )


# ============================================================================
# æ•°æ®æ¨¡å‹
# ============================================================================


class ClarificationQuestion(BaseModel):
    """æ¾„æ¸…é—®é¢˜"""

    question: str
    reason: str
    question_type: Literal["scope", "preference", "constraint", "context"] = "context"
    options: Optional[List[str]] = None


class ClarificationNeed(BaseModel):
    """æ¾„æ¸…éœ€æ±‚"""

    need_clarification: bool
    questions: List[ClarificationQuestion] = Field(default_factory=list)
    reasoning: str
    urgency: Literal["high", "medium", "low"] = "medium"


class ClarificationResponse(BaseModel):
    """ç”¨æˆ·å›ç­”"""

    answers: Dict[str, str]
    timestamp: str = Field(default_factory=lambda: datetime.now().isoformat())


class AgentState(BaseModel):
    """Agent çŠ¶æ€"""

    user_request: str
    clarification_need: Optional[ClarificationNeed] = None
    clarification_responses: List[ClarificationResponse] = Field(default_factory=list)
    refined_request: Optional[str] = None
    execution_plan: List[str] = Field(default_factory=list)
    results: List[str] = Field(default_factory=list)
    final_output: Optional[str] = None


# ============================================================================
# è¾…åŠ©å‡½æ•°
# ============================================================================


def parse_json_safely(
    llm, prompt: str, target_model: type[BaseModel], max_retries: int = 2
) -> Optional[BaseModel]:
    """å®‰å…¨è§£æ JSON"""
    for attempt in range(max_retries):
        try:
            response = llm.invoke([HumanMessage(content=prompt)])
            content = response.content.strip()

            # æå– JSON
            if "```json" in content:
                content = content.split("```json")[1].split("```")[0].strip()
            elif "```" in content:
                content = content.split("```")[1].split("```")[0].strip()

            data = json.loads(content)
            return target_model(**data)

        except (json.JSONDecodeError, ValidationError) as e:
            if attempt < max_retries - 1:
                prompt += f"\n\nè§£æå¤±è´¥ï¼š{e}\nè¯·ä¸¥æ ¼æŒ‰ç…§ JSON æ ¼å¼è¾“å‡ºï¼Œä¸è¦æ·»åŠ ä»»ä½•å…¶ä»–æ–‡å­—ã€‚"
            else:
                print(f"[è­¦å‘Š] JSON è§£æå¤±è´¥ï¼š{e}")
                return None

    return None


# ============================================================================
# èŠ‚ç‚¹å®ç°
# ============================================================================


def detect_clarification_node(state: AgentState) -> AgentState:
    """æ£€æµ‹æ˜¯å¦éœ€è¦æ¾„æ¸…"""
    if state.clarification_responses:
        # å·²ç»æ¾„æ¸…è¿‡ï¼Œè·³è¿‡
        return state

    llm = get_llm()

    prompt = f"""
ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½åŠ©æ‰‹ï¼Œæ­£åœ¨åˆ†æç”¨æˆ·éœ€æ±‚ã€‚

ç”¨æˆ·éœ€æ±‚ï¼š{state.user_request}

è¯·åˆ¤æ–­è¿™ä¸ªéœ€æ±‚æ˜¯å¦éœ€è¦æ¾„æ¸…ã€‚è€ƒè™‘ï¼š
1. éœ€æ±‚æ˜¯å¦æ¨¡ç³Šæˆ–æœ‰å¤šç§ç†è§£ï¼Ÿ
2. æ˜¯å¦ç¼ºå°‘å…³é”®ä¿¡æ¯ï¼Ÿ
3. æ˜¯å¦æœ‰æŠ€æœ¯é€‰å‹ç­‰å†³ç­–ç‚¹ï¼Ÿ

è¾“å‡º JSONï¼š

{{
  "need_clarification": true,
  "questions": [
    {{
      "question": "æ‚¨å¸Œæœ›é‡ç‚¹å…³æ³¨å“ªä¸ªæ–¹é¢ï¼Ÿ",
      "reason": "éœ€æ±‚è¿‡äºå®½æ³›",
      "question_type": "scope",
      "options": ["é€‰é¡¹1", "é€‰é¡¹2"]
    }}
  ],
  "reasoning": "ä¸ºä»€ä¹ˆéœ€è¦æ¾„æ¸…",
  "urgency": "high"
}}

question_type: scope | preference | constraint | context
urgency: high | medium | low

æ³¨æ„ï¼š
- åªåœ¨çœŸæ­£éœ€è¦æ—¶è®¾ç½® need_clarification=true
- é—®é¢˜æ•°é‡ 1-3 ä¸ª
- å¦‚æœéœ€æ±‚å·²ç»æ¸…æ™°ï¼Œè¿”å› need_clarification=false
"""

    result = parse_json_safely(llm, prompt, ClarificationNeed)

    if not result:
        # å›é€€
        result = ClarificationNeed(
            need_clarification=False, questions=[], reasoning="è§£æå¤±è´¥ï¼Œé»˜è®¤ä¸éœ€è¦æ¾„æ¸…"
        )

    state.clarification_need = result

    if result.need_clarification:
        print(f"\n[æ£€æµ‹] éœ€è¦æ¾„æ¸…ï¼š{result.reasoning}")
        print(f"  é—®é¢˜æ•°ï¼š{len(result.questions)}, ç´§è¿«æ€§ï¼š{result.urgency}")
    else:
        print(f"\n[æ£€æµ‹] éœ€æ±‚æ˜ç¡®ï¼š{result.reasoning}")

    return state


def ask_user_node(state: AgentState) -> AgentState:
    """å‘ç”¨æˆ·æé—®"""
    if not (state.clarification_need and state.clarification_need.need_clarification):
        return state

    if state.clarification_responses:
        # å·²ç»é—®è¿‡äº†
        return state

    clarification = state.clarification_need

    print("\n" + "=" * 80)
    print("ğŸ¤” Agent éœ€è¦æ‚¨çš„å¸®åŠ©")
    print("=" * 80)
    print(f"\nåŸå› ï¼š{clarification.reasoning}")
    print(f"ç´§è¿«æ€§ï¼š{clarification.urgency.upper()}\n")

    answers = {}
    for i, q in enumerate(clarification.questions, 1):
        print(f"\né—®é¢˜ {i}/{len(clarification.questions)} [{q.question_type}]:")
        print(f"  {q.question}")
        print(f"  â†’ {q.reason}")

        if q.options:
            print(f"  å¯é€‰é¡¹ï¼š")
            for j, opt in enumerate(q.options, 1):
                print(f"    {j}. {opt}")
            user_input = input(f"\n  æ‚¨çš„é€‰æ‹©ï¼ˆ1-{len(q.options)} æˆ–è‡ªå®šä¹‰ï¼‰[å›è½¦è·³è¿‡]: ").strip()

            if user_input.isdigit() and 1 <= int(user_input) <= len(q.options):
                answers[q.question] = q.options[int(user_input) - 1]
            elif user_input:
                answers[q.question] = user_input
            else:
                answers[q.question] = "ï¼ˆè·³è¿‡ï¼‰"
        else:
            user_input = input("  æ‚¨çš„å›ç­” [å›è½¦è·³è¿‡]: ").strip()
            answers[q.question] = user_input if user_input else "ï¼ˆè·³è¿‡ï¼‰"

    response = ClarificationResponse(answers=answers)
    state.clarification_responses.append(response)

    print("\nâœ“ æ„Ÿè°¢æ‚¨çš„åé¦ˆï¼")
    return state


def refine_request_node(state: AgentState) -> AgentState:
    """åŸºäºæ¾„æ¸…ç»“æœç²¾ç‚¼éœ€æ±‚"""
    if not state.clarification_responses:
        state.refined_request = state.user_request
        return state

    # æ•´åˆæ¾„æ¸…ä¿¡æ¯
    clarifications = []
    for resp in state.clarification_responses:
        for q, a in resp.answers.items():
            if a != "ï¼ˆè·³è¿‡ï¼‰":
                clarifications.append(f"- {q} -> {a}")

    if clarifications:
        clarification_text = "\n".join(clarifications)
        state.refined_request = (
            f"{state.user_request}\n\nç”¨æˆ·æ¾„æ¸…ï¼š\n{clarification_text}"
        )
        print(f"\n[ç²¾ç‚¼] æ›´æ–°åçš„éœ€æ±‚ï¼š")
        print(state.refined_request)
    else:
        state.refined_request = state.user_request

    return state


def plan_execution_node(state: AgentState) -> AgentState:
    """åˆ¶å®šæ‰§è¡Œè®¡åˆ’"""
    llm = get_llm()

    request = state.refined_request or state.user_request

    prompt = f"""
éœ€æ±‚ï¼š{request}

è¯·åˆ¶å®šæ‰§è¡Œè®¡åˆ’ï¼Œåˆ†è§£ä¸º 3-5 ä¸ªæ­¥éª¤ã€‚
åªè¾“å‡º JSON æ•°ç»„ï¼š

["æ­¥éª¤1æè¿°", "æ­¥éª¤2æè¿°", ...]

æ³¨æ„ï¼šåŸºäºç”¨æˆ·çš„æ¾„æ¸…ä¿¡æ¯æ¥ä¼˜åŒ–è®¡åˆ’ã€‚
"""

    messages = [HumanMessage(content=prompt)]
    response = llm.invoke(messages)
    content = response.content.strip()

    # æå– JSON
    if "```json" in content:
        content = content.split("```json")[1].split("```")[0].strip()
    elif "```" in content:
        content = content.split("```")[1].split("```")[0].strip()

    try:
        steps = json.loads(content)
        state.execution_plan = [str(s) for s in steps]
    except:
        # å›é€€ï¼šæŒ‰è¡Œåˆ†å‰²
        state.execution_plan = [
            line.strip() for line in content.split("\n") if line.strip()
        ][:5]

    print(f"\n[è§„åˆ’] æ‰§è¡Œè®¡åˆ’ï¼š")
    for i, step in enumerate(state.execution_plan, 1):
        print(f"  {i}. {step}")

    return state


def execute_plan_node(state: AgentState) -> AgentState:
    """æ‰§è¡Œè®¡åˆ’"""
    llm = get_llm()

    request = state.refined_request or state.user_request

    for i, step in enumerate(state.execution_plan, 1):
        prompt = f"""
æ€»ä½“éœ€æ±‚ï¼š{request}
å½“å‰æ­¥éª¤ï¼š{step}

è¯·æ‰§è¡Œè¿™ä¸ªæ­¥éª¤ï¼Œç»™å‡ºç»“æœï¼ˆ2-3 å¥è¯ï¼‰ã€‚
"""

        response = llm.invoke([HumanMessage(content=prompt)])
        result = response.content.strip()

        state.results.append(f"æ­¥éª¤{i}: {result}")
        print(f"\n[æ‰§è¡Œ] æ­¥éª¤ {i}: {result[:100]}...")

    return state


def finalize_node(state: AgentState) -> AgentState:
    """ç”Ÿæˆæœ€ç»ˆè¾“å‡º"""
    llm = get_llm()

    results_text = "\n".join(state.results)
    request = state.refined_request or state.user_request

    prompt = f"""
ç”¨æˆ·éœ€æ±‚ï¼š{request}

æ‰§è¡Œç»“æœï¼š
{results_text}

è¯·ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Šï¼ˆ2-3 æ®µï¼‰ã€‚
"""

    response = llm.invoke([HumanMessage(content=prompt)])
    state.final_output = response.content.strip()

    print("\n" + "=" * 80)
    print("âœ“ æœ€ç»ˆæŠ¥å‘Š")
    print("=" * 80)
    print(state.final_output)

    return state


# ============================================================================
# å·¥ä½œæµæ„å»º
# ============================================================================


def create_clarification_workflow():
    """åˆ›å»ºæ¾„æ¸…å·¥ä½œæµ"""
    graph = StateGraph(AgentState)

    # æ·»åŠ èŠ‚ç‚¹
    graph.add_node("detect", detect_clarification_node)
    graph.add_node("ask", ask_user_node)
    graph.add_node("refine", refine_request_node)
    graph.add_node("plan", plan_execution_node)
    graph.add_node("execute", execute_plan_node)
    graph.add_node("finalize", finalize_node)

    # å®šä¹‰æµç¨‹
    graph.set_entry_point("detect")

    # æ¡ä»¶åˆ†æ”¯ï¼šæ˜¯å¦éœ€è¦æ¾„æ¸…
    graph.add_conditional_edges(
        "detect",
        lambda s: "ask" if (s.clarification_need and s.clarification_need.need_clarification) else "refine",
        {"ask": "ask", "refine": "refine"},
    )

    graph.add_edge("ask", "refine")
    graph.add_edge("refine", "plan")
    graph.add_edge("plan", "execute")
    graph.add_edge("execute", "finalize")
    graph.add_edge("finalize", END)

    return graph.compile()


# ============================================================================
# ä¸»å‡½æ•°
# ============================================================================


def run_demo():
    """è¿è¡Œæ¼”ç¤º"""
    load_environment()
    workflow = create_clarification_workflow()

    print("\n" + "=" * 80)
    print("æ™ºèƒ½æ¾„æ¸… Agent æ¼”ç¤º")
    print("=" * 80)

    user_input = input("\nè¯·è¾“å…¥æ‚¨çš„éœ€æ±‚ï¼ˆå›è½¦ä½¿ç”¨é»˜è®¤ï¼‰: ").strip()
    if not user_input:
        user_input = "ç ”ç©¶ AI"
        print(f"ä½¿ç”¨é»˜è®¤éœ€æ±‚ï¼š{user_input}")

    state = AgentState(user_request=user_input)

    try:
        final_state = workflow.invoke(state)

        print("\n" + "=" * 80)
        print("æ‰§è¡Œæ€»ç»“")
        print("=" * 80)

        if final_state.clarification_responses:
            print("\næ¾„æ¸…è¿‡ç¨‹ï¼š")
            for resp in final_state.clarification_responses:
                for q, a in resp.answers.items():
                    if a != "ï¼ˆè·³è¿‡ï¼‰":
                        print(f"  Q: {q}")
                        print(f"  A: {a}")

        print(f"\næ‰§è¡Œæ­¥éª¤ï¼š{len(final_state.execution_plan)}")
        print(f"å®Œæˆç»“æœï¼š{len(final_state.results)}")

    except Exception as e:
        print(f"\næ‰§è¡Œå‡ºé”™ï¼š{e}")
        import traceback

        traceback.print_exc()


if __name__ == "__main__":
    run_demo()
