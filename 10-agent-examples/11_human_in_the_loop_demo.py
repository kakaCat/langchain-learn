#!/usr/bin/env python3
"""
11 - Human-in-the-Loop (äººæœºååŒ) åŸºç¡€ç¤ºä¾‹

æ¼”ç¤ºå¦‚ä½•åœ¨ LangGraph å·¥ä½œæµä¸­å®ç°äººæœºååŒï¼š
1. Agent æ‰§è¡Œè¿‡ç¨‹ä¸­æš‚åœå¹¶è¯·æ±‚äººç±»è¾“å…¥
2. åŸºäºäººç±»åé¦ˆè°ƒæ•´æ‰§è¡Œè·¯å¾„
3. ä½¿ç”¨ interrupt_before/after æœºåˆ¶å®ç°æ–­ç‚¹
4. çŠ¶æ€æŒä¹…åŒ–å’Œæ¢å¤

é€‚ç”¨åœºæ™¯ï¼š
- éœ€è¦äººå·¥å®¡æ ¸çš„å†³ç­–ç‚¹
- ä¸ç¡®å®šæ€§é«˜çš„ä»»åŠ¡
- éœ€è¦ç”¨æˆ·æˆæƒçš„æ“ä½œ
- æ•™å­¦å’Œæ¼”ç¤ºåœºæ™¯
"""

from __future__ import annotations

import os
from typing import Annotated, Literal, Optional

from dotenv import load_dotenv
from langchain_core.messages import HumanMessage
from langchain_openai import ChatOpenAI
from langchain_ollama import ChatOllama
from langgraph.checkpoint.memory import MemorySaver
from langgraph.graph import END, StateGraph
from pydantic import BaseModel, Field
from typing_extensions import TypedDict


def load_environment() -> None:
    load_dotenv(
        dotenv_path=os.path.join(os.path.dirname(__file__), ".env"), override=True
    )


def get_llm(model: Optional[str] = None, temperature: float = 0.7) -> object:
    provider = os.getenv("LLM_PROVIDER", "").lower()
    use_ollama = provider in {"ollama", "local"} or not os.getenv("OPENAI_API_KEY")

    if use_ollama:
        model_name = model or os.getenv("OLLAMA_MODEL", "llama3.1:8b")
        base_url = os.getenv("OLLAMA_BASE_URL", "http://localhost:11434")
        return ChatOllama(model=model_name, base_url=base_url, temperature=temperature)
    else:
        api_key = os.getenv("OPENAI_API_KEY")
        base_url = os.getenv("OPENAI_BASE_URL")
        model_name = model or os.getenv("OPENAI_MODEL", "gpt-4o-mini")
        return ChatOpenAI(
            model=model_name,
            api_key=api_key,
            base_url=base_url,
            temperature=temperature,
            max_tokens=1000,
        )


# ============================================================================
# çŠ¶æ€å®šä¹‰
# ============================================================================


class WorkflowState(TypedDict):
    """å·¥ä½œæµçŠ¶æ€"""

    user_request: str
    plan: list[str]
    current_step: int
    step_results: list[str]
    human_feedback: Optional[str]
    should_continue: bool
    final_output: Optional[str]


# ============================================================================
# èŠ‚ç‚¹å®ç°
# ============================================================================


def create_plan_node(state: WorkflowState) -> WorkflowState:
    """åˆ›å»ºæ‰§è¡Œè®¡åˆ’"""
    llm = get_llm()

    prompt = f"""
ç”¨æˆ·éœ€æ±‚ï¼š{state['user_request']}

è¯·å°†è¿™ä¸ªéœ€æ±‚åˆ†è§£ä¸º 3-5 ä¸ªå¯æ‰§è¡Œçš„æ­¥éª¤ã€‚
åªè¾“å‡ºæ­¥éª¤åˆ—è¡¨ï¼Œæ¯è¡Œä¸€ä¸ªæ­¥éª¤ï¼Œæ ¼å¼å¦‚ä¸‹ï¼š
1. æ­¥éª¤æè¿°
2. æ­¥éª¤æè¿°
...
"""

    response = llm.invoke([HumanMessage(content=prompt)])
    content = response.content.strip()

    # è§£ææ­¥éª¤
    steps = []
    for line in content.split("\n"):
        line = line.strip()
        if line and (line[0].isdigit() or line.startswith("-")):
            # å»é™¤åºå·
            step = line.split(".", 1)[-1].strip() if "." in line else line[1:].strip()
            if step:
                steps.append(step)

    state["plan"] = steps if steps else [state["user_request"]]
    state["current_step"] = 0
    state["step_results"] = []

    print(f"\nâœ“ ç”Ÿæˆæ‰§è¡Œè®¡åˆ’ï¼š")
    for i, step in enumerate(state["plan"], 1):
        print(f"  {i}. {step}")

    return state


def execute_step_node(state: WorkflowState) -> WorkflowState:
    """æ‰§è¡Œå½“å‰æ­¥éª¤"""
    if state["current_step"] >= len(state["plan"]):
        state["should_continue"] = False
        return state

    llm = get_llm()
    current_step = state["plan"][state["current_step"]]

    prompt = f"""
æ€»ä½“ä»»åŠ¡ï¼š{state['user_request']}
å½“å‰æ­¥éª¤ï¼š{current_step}

{'ä¹‹å‰çš„æ­¥éª¤ç»“æœï¼š' + chr(10).join(f'- {r}' for r in state['step_results']) if state['step_results'] else ''}

{'ç”¨æˆ·åé¦ˆï¼š' + state['human_feedback'] if state.get('human_feedback') else ''}

è¯·æ‰§è¡Œå½“å‰æ­¥éª¤å¹¶ç»™å‡ºç»“æœï¼ˆ1-2 å¥è¯ï¼‰ã€‚
"""

    response = llm.invoke([HumanMessage(content=prompt)])
    result = response.content.strip()

    state["step_results"].append(f"æ­¥éª¤ {state['current_step'] + 1}: {result}")
    state["current_step"] += 1

    print(f"\nâœ“ æ‰§è¡Œæ­¥éª¤ {state['current_step']}/{len(state['plan'])}")
    print(f"  ç»“æœï¼š{result}")

    # æ¸…é™¤å·²ä½¿ç”¨çš„åé¦ˆ
    state["human_feedback"] = None

    return state


def request_human_input_node(state: WorkflowState) -> WorkflowState:
    """è¯·æ±‚äººç±»è¾“å…¥ï¼ˆè¿™ä¸ªèŠ‚ç‚¹ä¼šè§¦å‘ä¸­æ–­ï¼‰"""
    print("\n" + "=" * 80)
    print("ğŸ¤š Agent è¯·æ±‚äººç±»åé¦ˆ")
    print("=" * 80)
    print(f"\nå·²å®Œæˆæ­¥éª¤ï¼š{state['current_step']}/{len(state['plan'])}")
    print(f"å½“å‰è¿›å±•ï¼š")
    for result in state["step_results"]:
        print(f"  - {result}")

    print(
        "\næç¤ºï¼šAgent å°†æš‚åœç­‰å¾…æ‚¨çš„è¾“å…¥ã€‚\n"
        "æ‚¨å¯ä»¥ï¼š\n"
        "  1. æä¾›åé¦ˆå»ºè®®\n"
        "  2. è¦æ±‚ä¿®æ”¹æŸä¸ªæ­¥éª¤\n"
        "  3. æ‰¹å‡†ç»§ç»­æ‰§è¡Œï¼ˆè¾“å…¥ 'continue' æˆ–ç›´æ¥å›è½¦ï¼‰\n"
        "  4. ç»ˆæ­¢æ‰§è¡Œï¼ˆè¾“å…¥ 'stop'ï¼‰"
    )

    # æ³¨æ„ï¼šå®é™…çš„è¾“å…¥ä¼šåœ¨ workflow.stream() çš„å¾ªç¯ä¸­å¤„ç†
    # è¿™é‡Œåªæ˜¯å±•ç¤ºæç¤ºä¿¡æ¯
    return state


def finalize_node(state: WorkflowState) -> WorkflowState:
    """ç”Ÿæˆæœ€ç»ˆè¾“å‡º"""
    llm = get_llm()

    results_str = "\n".join(state["step_results"])

    prompt = f"""
ä»»åŠ¡ï¼š{state['user_request']}

æ‰§è¡Œè¿‡ç¨‹ï¼š
{results_str}

è¯·æ€»ç»“æ‰§è¡Œç»“æœï¼Œç»™å‡ºç®€æ´çš„æœ€ç»ˆæŠ¥å‘Šï¼ˆ2-3 æ®µï¼‰ã€‚
"""

    response = llm.invoke([HumanMessage(content=prompt)])
    state["final_output"] = response.content.strip()

    print("\n" + "=" * 80)
    print("âœ“ ä»»åŠ¡å®Œæˆ")
    print("=" * 80)
    print(state["final_output"])

    return state


# ============================================================================
# æ¡ä»¶åˆ¤æ–­
# ============================================================================


def should_continue(state: WorkflowState) -> Literal["execute", "request_input", "finalize"]:
    """åˆ¤æ–­ä¸‹ä¸€æ­¥æ“ä½œ"""
    # å¦‚æœæ‰€æœ‰æ­¥éª¤å®Œæˆ
    if state["current_step"] >= len(state["plan"]):
        return "finalize"

    # æ¯ 2 ä¸ªæ­¥éª¤è¯·æ±‚ä¸€æ¬¡äººç±»è¾“å…¥
    if state["current_step"] > 0 and state["current_step"] % 2 == 0:
        return "request_input"

    return "execute"


# ============================================================================
# å·¥ä½œæµæ„å»º
# ============================================================================


def create_hitl_workflow():
    """åˆ›å»º Human-in-the-Loop å·¥ä½œæµ"""
    graph = StateGraph(WorkflowState)

    # æ·»åŠ èŠ‚ç‚¹
    graph.add_node("plan", create_plan_node)
    graph.add_node("execute", execute_step_node)
    graph.add_node("request_input", request_human_input_node)
    graph.add_node("finalize", finalize_node)

    # å®šä¹‰æµç¨‹
    graph.set_entry_point("plan")
    graph.add_edge("plan", "execute")

    # æ¡ä»¶åˆ†æ”¯
    graph.add_conditional_edges(
        "execute",
        should_continue,
        {
            "execute": "execute",  # ç»§ç»­æ‰§è¡Œ
            "request_input": "request_input",  # è¯·æ±‚è¾“å…¥
            "finalize": "finalize",  # å®Œæˆ
        },
    )

    # äººç±»è¾“å…¥åç»§ç»­æ‰§è¡Œ
    graph.add_edge("request_input", "execute")
    graph.add_edge("finalize", END)

    # ä½¿ç”¨ MemorySaver æ”¯æŒçŠ¶æ€æŒä¹…åŒ–
    # interrupt_before æŒ‡å®šåœ¨å“ªä¸ªèŠ‚ç‚¹å‰æš‚åœ
    return graph.compile(
        checkpointer=MemorySaver(), interrupt_before=["request_input"]
    )


# ============================================================================
# äº¤äº’å¼æ‰§è¡Œ
# ============================================================================


def run_interactive_demo():
    """è¿è¡Œäº¤äº’å¼æ¼”ç¤º"""
    load_environment()
    workflow = create_hitl_workflow()

    print("\n" + "=" * 80)
    print("Human-in-the-Loop äº¤äº’å¼æ¼”ç¤º")
    print("=" * 80)

    user_request = input("\nè¯·è¾“å…¥ä»»åŠ¡ï¼ˆå›è½¦ä½¿ç”¨é»˜è®¤ï¼‰: ").strip()
    if not user_request:
        user_request = "åˆ¶å®šä¸€ä¸ªå­¦ä¹  Python çš„è®¡åˆ’"
        print(f"ä½¿ç”¨é»˜è®¤ä»»åŠ¡ï¼š{user_request}")

    # åˆå§‹çŠ¶æ€
    initial_state = {
        "user_request": user_request,
        "plan": [],
        "current_step": 0,
        "step_results": [],
        "human_feedback": None,
        "should_continue": True,
        "final_output": None,
    }

    # é…ç½®ï¼ˆç”¨äºçŠ¶æ€æŒä¹…åŒ–ï¼‰
    config = {"configurable": {"thread_id": "demo-thread-1"}}

    # å¼€å§‹æ‰§è¡Œ
    print("\nå¼€å§‹æ‰§è¡Œ...")

    try:
        # ç¬¬ä¸€æ¬¡æ‰§è¡Œï¼ˆç›´åˆ°é‡åˆ° interruptï¼‰
        for event in workflow.stream(initial_state, config):
            if "__interrupt__" in event:
                # é‡åˆ°ä¸­æ–­ç‚¹
                print("\nâ¸ å·¥ä½œæµå·²æš‚åœ")

                # è·å–å½“å‰çŠ¶æ€
                current_state = workflow.get_state(config)
                print(f"\nå½“å‰èŠ‚ç‚¹ï¼š{current_state.next}")
                print(f"å¾…æ‰§è¡ŒèŠ‚ç‚¹ï¼š{list(current_state.next)}")

                # è¯·æ±‚ç”¨æˆ·è¾“å…¥
                user_input = input("\næ‚¨çš„åé¦ˆï¼ˆå›è½¦ç»§ç»­ï¼‰: ").strip()

                if user_input.lower() == "stop":
                    print("\nç”¨æˆ·ç»ˆæ­¢æ‰§è¡Œ")
                    break

                # æ›´æ–°çŠ¶æ€ï¼ˆæ·»åŠ äººç±»åé¦ˆï¼‰
                if user_input and user_input.lower() != "continue":
                    # è·å–å½“å‰çŠ¶æ€å€¼
                    state_values = current_state.values
                    state_values["human_feedback"] = user_input
                    # æ›´æ–°çŠ¶æ€
                    workflow.update_state(config, state_values)
                    print(f"\nâœ“ å·²è®°å½•æ‚¨çš„åé¦ˆï¼š{user_input}")

                # ç»§ç»­æ‰§è¡Œ
                print("\nç»§ç»­æ‰§è¡Œ...")
                for event2 in workflow.stream(None, config):
                    if "__interrupt__" in event2:
                        # å†æ¬¡é‡åˆ°ä¸­æ–­
                        break

        # è·å–æœ€ç»ˆçŠ¶æ€
        final_state = workflow.get_state(config)
        if final_state.values.get("final_output"):
            print("\nä»»åŠ¡å·²å®Œæˆï¼")

    except Exception as e:
        print(f"\næ‰§è¡Œå‡ºé”™ï¼š{e}")
        import traceback

        traceback.print_exc()


# ============================================================================
# è‡ªåŠ¨æ¼”ç¤ºï¼ˆç”¨äºæµ‹è¯•ï¼‰
# ============================================================================


def run_automated_demo():
    """è¿è¡Œè‡ªåŠ¨æ¼”ç¤ºï¼ˆæ¨¡æ‹Ÿç”¨æˆ·è¾“å…¥ï¼‰"""
    load_environment()
    workflow = create_hitl_workflow()

    print("\n" + "=" * 80)
    print("Human-in-the-Loop è‡ªåŠ¨æ¼”ç¤º")
    print("=" * 80)

    initial_state = {
        "user_request": "åˆ¶å®šä¸€ä¸ªå‘¨æœ«å­¦ä¹ è®¡åˆ’",
        "plan": [],
        "current_step": 0,
        "step_results": [],
        "human_feedback": None,
        "should_continue": True,
        "final_output": None,
    }

    config = {"configurable": {"thread_id": "auto-demo-1"}}

    # æ¨¡æ‹Ÿçš„ç”¨æˆ·åé¦ˆ
    simulated_feedbacks = [
        "è¯·å¢åŠ æ›´å¤šå®è·µé¡¹ç›®",
        "æ—¶é—´å®‰æ’è¦æ›´çµæ´»",
    ]
    feedback_index = 0

    try:
        for event in workflow.stream(initial_state, config):
            if "__interrupt__" in event:
                print("\nâ¸ å·¥ä½œæµæš‚åœï¼ˆè‡ªåŠ¨æ¨¡å¼ï¼‰")

                # æ¨¡æ‹Ÿç”¨æˆ·è¾“å…¥
                if feedback_index < len(simulated_feedbacks):
                    feedback = simulated_feedbacks[feedback_index]
                    print(f"[æ¨¡æ‹Ÿç”¨æˆ·è¾“å…¥]: {feedback}")

                    current_state = workflow.get_state(config)
                    state_values = current_state.values
                    state_values["human_feedback"] = feedback
                    workflow.update_state(config, state_values)

                    feedback_index += 1
                else:
                    print("[æ¨¡æ‹Ÿç”¨æˆ·è¾“å…¥]: continue")

                # ç»§ç»­æ‰§è¡Œ
                print("ç»§ç»­æ‰§è¡Œ...")
                for event2 in workflow.stream(None, config):
                    if "__interrupt__" in event2:
                        break

    except Exception as e:
        print(f"\næ‰§è¡Œå‡ºé”™ï¼š{e}")
        import traceback

        traceback.print_exc()


# ============================================================================
# ä¸»å…¥å£
# ============================================================================


if __name__ == "__main__":
    import sys

    if len(sys.argv) > 1 and sys.argv[1] == "--auto":
        run_automated_demo()
    else:
        run_interactive_demo()
