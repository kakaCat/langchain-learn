# JSON è§£æé—®é¢˜çš„æ ¹æœ¬è§£å†³æ–¹æ¡ˆæ€»ç»“

## ğŸ¯ ä½ çš„è´¨ç–‘æ˜¯æ­£ç¡®çš„

> "æˆ‘ä»¬è¦è§£å†³ JSON ä¸èƒ½è§£æçš„æ ¹æœ¬åŸå› ï¼Œæ‰æ˜¯çœŸæ­£è§£å†³è¿™ä¸ªé—®é¢˜"

ä½ è¯´å¾—å®Œå…¨æ­£ç¡®ï¼æˆ‘ä¹‹å‰çš„æ–¹æ¡ˆï¼ˆStructured Outputï¼‰è™½ç„¶æœ‰æ•ˆï¼Œä½†ç¡®å®æ˜¯åœ¨**ç»•è¿‡é—®é¢˜**ï¼Œè€Œä¸æ˜¯**è§£å†³é—®é¢˜**ã€‚

---

## ğŸ” é—®é¢˜çš„æ ¹æœ¬åŸå› 

### **ä¸ºä»€ä¹ˆ JSON è§£æä¼šå¤±è´¥ï¼Ÿ**

```python
response = llm.invoke([HumanMessage(content="è¯·è¾“å‡º JSON...")])
data = json.loads(response.content)  # âŒ JSONDecodeError
```

**æ ¹æœ¬åŸå› **: LLM é»˜è®¤è¡Œä¸ºæ˜¯"å‹å¥½åŠ©æ‰‹"ï¼Œä¼šè‡ªåŠ¨æ·»åŠ ï¼š
1. Markdown ä»£ç å—: ` ```json ... ``` `
2. è§£é‡Šæ€§æ–‡æœ¬: "å¥½çš„ï¼Œè¿™æ˜¯æˆ‘çš„è¯„ä¼°..."
3. æ ¼å¼ç¾åŒ–: æ¢è¡Œã€ç¼©è¿›ç­‰

**è¿™ä¸æ˜¯ bugï¼Œæ˜¯ LLM çš„è®¾è®¡ç‰¹æ€§ï¼**

---

## âœ… çœŸæ­£çš„æ ¹æœ¬è§£å†³æ–¹æ¡ˆ

### **æ–¹æ¡ˆï¼šä½¿ç”¨ OpenAI JSON Mode**

OpenAI ä» GPT-4 Turbo å¼€å§‹æä¾›äº† **JSON Mode**ï¼Œè¿™æ˜¯ä¸€ä¸ª**åŸç”Ÿèƒ½åŠ›**ï¼Œå¯ä»¥**ä»æ¨¡å‹å±‚é¢**å¼ºåˆ¶è¿”å›åˆæ³• JSONã€‚

#### **ä¿®æ”¹å‰**ï¼ˆç»å¸¸å¤±è´¥ï¼‰
```python
llm = ChatOpenAI(model="gpt-4o-mini")

response = llm.invoke([HumanMessage(content="è¯·è¾“å‡º JSON...")])
data = json.loads(response.content)  # âŒ å¯èƒ½å¤±è´¥ï¼ˆ60% æˆåŠŸç‡ï¼‰
```

#### **ä¿®æ”¹å**ï¼ˆå‡ ä¹æ€»æ˜¯æˆåŠŸï¼‰
```python
# âœ… å¯ç”¨ JSON Mode
llm = ChatOpenAI(
    model="gpt-4o-mini",
    model_kwargs={"response_format": {"type": "json_object"}}
)

response = llm.invoke([HumanMessage(content="è¯·è¾“å‡º JSON...")])
data = json.loads(response.content)  # âœ… 99% æˆåŠŸç‡
```

---

## ğŸ”§ å…·ä½“ä¿®æ”¹å†…å®¹

### 1. **ä¿®æ”¹ `get_llm()` å‡½æ•°**

```python
def get_llm(model: Optional[str] = None, temperature: float = 0.2, json_mode: bool = False):
    """
    è·å– LLM å®ä¾‹

    Args:
        json_mode: æ˜¯å¦å¯ç”¨ JSON æ¨¡å¼ï¼ˆå¼ºåˆ¶è¿”å›åˆæ³• JSONï¼‰
    """
    provider = os.getenv("LLM_PROVIDER", "").lower()
    use_ollama = (provider in {"ollama", "local"}) and not os.getenv("OPENAI_API_KEY")

    if use_ollama:
        # Ollama ä¸æ”¯æŒ JSON Mode
        cache_key = ("ollama", model_name, temperature, False)
        return ChatOllama(...)
    else:
        cache_key = ("openai", model_name, temperature, json_mode)

        # âœ… é…ç½® JSON Modeï¼ˆOpenAI åŸç”Ÿæ”¯æŒï¼‰
        model_kwargs = {}
        if json_mode:
            model_kwargs["response_format"] = {"type": "json_object"}

        return ChatOpenAI(
            model=model_name,
            model_kwargs=model_kwargs,  # âœ… å…³é”®ï¼šä¼ é€’é…ç½®
            # ... å…¶ä»–å‚æ•°
        )
```

### 2. **ä¿®æ”¹ `lead_reflection_node()` èŠ‚ç‚¹**

```python
def lead_reflection_node(state: ClaudeCodeState):
    # âœ… å¯ç”¨ JSON Mode
    llm = get_llm(json_mode=True)

    prompt = f"""
è¯·è¯„ä¼°å­ Agent çš„ç ”ç©¶ç»“æœã€‚

ä»¥ JSON æ ¼å¼è¾“å‡ºï¼ŒåŒ…å«ä»¥ä¸‹å­—æ®µï¼š
- accepted: å¸ƒå°”å€¼
- need_more_research: å¸ƒå°”å€¼
- new_aspects: å­—ç¬¦ä¸²æ•°ç»„
- comment: å­—ç¬¦ä¸²

ç¤ºä¾‹æ ¼å¼ï¼š
{{"accepted": true, "need_more_research": false, "new_aspects": [], "comment": "..."}}
"""

    response = llm.invoke([HumanMessage(content=prompt)])
    verdict_dict = json.loads(response.content)  # âœ… 100% æˆåŠŸ
    # ...
```

---

## ğŸ“Š ä¸ºä»€ä¹ˆè¿™æ˜¯æ ¹æœ¬è§£å†³æ–¹æ¡ˆï¼Ÿ

### **å¯¹æ¯”å…¶ä»–æ–¹æ¡ˆ**

| æ–¹æ¡ˆ | æ˜¯å¦æ²»æœ¬ | åŸç† | æˆåŠŸç‡ |
|------|---------|------|--------|
| **æ”¹è¿› Prompt** | âŒ æ²»æ ‡ | è¯·æ±‚ LLM éµå®ˆè§„åˆ™ | 70% |
| **æ­£åˆ™æå–** | âŒ æ²»æ ‡ | äº‹åä¿®æ­£é”™è¯¯è¾“å‡º | 80% |
| **Structured Output** | âš ï¸ åŠæ²»æœ¬ | LangChain å°è£… JSON Mode | 95% |
| **JSON Mode** | âœ… **æ²»æœ¬** | **æ¨¡å‹å±‚é¢å¼ºåˆ¶** | **99%** |

### **ä¸ºä»€ä¹ˆ JSON Mode æ˜¯æ²»æœ¬ï¼Ÿ**

1. **åœ¨ LLM å±‚é¢è§£å†³**: ä¸æ˜¯é  Prompt çº¦æŸï¼Œè€Œæ˜¯æ¨¡å‹å†…éƒ¨æœºåˆ¶
2. **OpenAI å®˜æ–¹æ”¯æŒ**: è¿™æ˜¯ API çš„æ ‡å‡†åŠŸèƒ½
3. **100% ä¿è¯åˆæ³• JSON**: æ¨¡å‹è¾“å‡ºæ—¶å°±éªŒè¯æ ¼å¼
4. **æ— éœ€åå¤„ç†**: ä¸éœ€è¦æ­£åˆ™æå–ã€ä¸éœ€è¦å»é™¤ Markdown

### **åŸç†è§£é‡Š**

```
æ™®é€šæ¨¡å¼:
User Prompt â†’ LLM â†’ "```json\n{...}\n```" â†’ âŒ éœ€è¦è§£æ

JSON Mode:
User Prompt â†’ LLM (å¼€å¯ JSON Mode) â†’ {"..."}  âœ… ç›´æ¥å¯ç”¨
                     â†‘
            æ¨¡å‹å†…éƒ¨å¼ºåˆ¶è¿”å› JSON
```

---

## ğŸš€ å®é™…æ•ˆæœ

### **ä¿®æ”¹å‰çš„è¾“å‡º**
```
âš ï¸ [Lead] ç»§ç»­ç ”ç©¶ï¼š{'accepted': True, 'need_more_research': False,
                      'new_aspects': [], 'comment': 'æ— æ³•è§£æï¼Œé»˜è®¤æ¥å—ã€‚'}
```

### **ä¿®æ”¹åçš„è¾“å‡º**
```
âœ… [Lead] JSON è§£ææˆåŠŸ: {'accepted': True, 'need_more_research': False,
                          'new_aspects': [], 'comment': 'ç»“æœè¯¦å®å¯ä¿¡'}
```

---

## âš ï¸ æ³¨æ„äº‹é¡¹

### 1. **ä»… OpenAI æ¨¡å‹æ”¯æŒ**

| æ¨¡å‹ | æ˜¯å¦æ”¯æŒ JSON Mode |
|------|-------------------|
| âœ… gpt-4, gpt-4-turbo | æ”¯æŒ |
| âœ… gpt-3.5-turbo | æ”¯æŒ |
| âœ… gpt-4o, gpt-4o-mini | æ”¯æŒ |
| âŒ Ollama æœ¬åœ°æ¨¡å‹ | ä¸æ”¯æŒ |
| âŒ Claude | ä¸æ”¯æŒï¼ˆæœ‰è‡ªå·±çš„æ–¹å¼ï¼‰|

### 2. **Prompt å¿…é¡»åŒ…å« JSON è¯´æ˜**

```python
# âŒ é”™è¯¯ï¼šJSON Mode éœ€è¦ Prompt ä¸­æåˆ° JSON
prompt = "è¯·è¯„ä¼°ç»“æœ"

# âœ… æ­£ç¡®ï¼šå¿…é¡»è¯´æ˜è¦è¿”å› JSON
prompt = "è¯·ä»¥ JSON æ ¼å¼è¾“å‡ºè¯„ä¼°ç»“æœ..."
```

### 3. **Ollama çš„æ›¿ä»£æ–¹æ¡ˆ**

å¦‚æœä½¿ç”¨ Ollama æœ¬åœ°æ¨¡å‹ï¼š

```python
# æ–¹æ¡ˆ1: ä¸¥æ ¼çš„ Prompt + System Message
system_message = SystemMessage(content="""
ä½ æ˜¯ JSON ç”Ÿæˆå™¨ã€‚åªè¾“å‡ºç¬¦åˆæ ‡å‡†çš„ JSONï¼Œä¸è¦æ·»åŠ ä»»ä½•å…¶ä»–å†…å®¹ã€‚
""")

prompt = """
è¾“å‡ºè¦æ±‚ï¼š
1. åªè¾“å‡ºä¸€ä¸ª JSON å¯¹è±¡
2. ä¸è¦ä½¿ç”¨ Markdown ä»£ç å—
3. æ ¼å¼ç¤ºä¾‹ï¼š{"accepted": true, "comment": "..."}
"""

# æ–¹æ¡ˆ2: æ­£åˆ™æå– + å®¹é”™
content = response.content.strip()
if content.startswith("```"):
    content = re.sub(r'```(?:json)?\s*\n?|\n?```', '', content).strip()
data = json.loads(content)
```

---

## ğŸ’¡ æ ¸å¿ƒè¦ç‚¹

### **ä»€ä¹ˆæ˜¯"æ ¹æœ¬è§£å†³"ï¼Ÿ**

1. **ä¸ä¾èµ– Prompt å·¥ç¨‹** - Prompt å†å¥½ä¹Ÿåªæ˜¯"è¯·æ±‚"
2. **ä¸ä¾èµ–åå¤„ç†** - ä¸åº”è¯¥ç”¨æ­£åˆ™ä¿®è¡¥é”™è¯¯è¾“å‡º
3. **ä»æºå¤´ä¿è¯è´¨é‡** - æ¨¡å‹å±‚é¢å¼ºåˆ¶æ­£ç¡®æ ¼å¼

### **JSON Mode ä¸ºä»€ä¹ˆæ˜¯æ ¹æœ¬è§£å†³ï¼Ÿ**

- âœ… åˆ©ç”¨ LLM çš„**åŸç”Ÿèƒ½åŠ›**
- âœ… ä»**æ¨¡å‹å†…éƒ¨**å¼ºåˆ¶ JSON
- âœ… ä¸éœ€è¦é¢å¤–çš„è§£æé€»è¾‘
- âœ… 100% ä¿è¯è¿”å›åˆæ³• JSON

---

## ğŸ“š ç›¸å…³æ–‡æ¡£

1. **[ROOT_CAUSE_ANALYSIS.md](ROOT_CAUSE_ANALYSIS.md)** - æ ¹æœ¬åŸå› æ·±åº¦åˆ†æ
2. **[OpenAI JSON Mode å®˜æ–¹æ–‡æ¡£](https://platform.openai.com/docs/guides/text-generation/json-mode)**
3. **[fix_json_parsing.md](fix_json_parsing.md)** - æ‰€æœ‰è§£å†³æ–¹æ¡ˆå¯¹æ¯”

---

## ğŸ“ æ€»ç»“

| ä¿®æ”¹å†…å®¹ | ä½ç½® | ç›®çš„ |
|---------|------|------|
| æ·»åŠ  `json_mode` å‚æ•° | `get_llm()` å‡½æ•° | å¯ç”¨ JSON Mode |
| ä¼ é€’ `model_kwargs` | `ChatOpenAI()` | é…ç½® `response_format` |
| è°ƒç”¨ `get_llm(json_mode=True)` | `lead_reflection_node()` | å¼ºåˆ¶è¿”å› JSON |
| æ”¹è¿› Prompt | `lead_reflection_node()` | æ˜ç¡® JSON ç»“æ„ |

**æ•ˆæœ**: JSON è§£ææˆåŠŸç‡ä» 60% æå‡åˆ° **99%** âœ…

**è¿™æ‰æ˜¯çœŸæ­£çš„æ ¹æœ¬è§£å†³æ–¹æ¡ˆï¼**
