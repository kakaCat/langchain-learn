# JSON è§£æå¤±è´¥çš„æ ¹æœ¬åŸå› åˆ†æ

## ğŸ¯ é—®é¢˜æœ¬è´¨

ä½ è¯´å¾—å¯¹ï¼æˆ‘ä¹‹å‰çš„æ–¹æ¡ˆæ˜¯**ç»•è¿‡é—®é¢˜**ï¼Œè€Œä¸æ˜¯**è§£å†³é—®é¢˜**ã€‚

è®©æˆ‘ä»¬åˆ†æ JSON è§£æå¤±è´¥çš„**æ ¹æœ¬åŸå› **ã€‚

---

## ğŸ” æ ¹æœ¬åŸå› åˆ†æ

### **ç°è±¡**
```python
response = llm.invoke([HumanMessage(content=prompt)])
verdict = json.loads(response.content)  # âŒ JSONDecodeError
```

### **æ ¹æœ¬åŸå› **

#### åŸå› 1: **Prompt æŒ‡ä»¤ä¸å¤Ÿæ˜ç¡®** â­ æœ€å¸¸è§

ç°æœ‰ Prompt:
```python
prompt = """
è¯·è¯„ä¼°ï¼š
1. è¯¥ç»“æœæ˜¯å¦å¯ä¿¡...
2. æ˜¯å¦éœ€è¦è¿½åŠ ç ”ç©¶...

è¾“å‡º JSONï¼š
{
  "accepted": true,
  "need_more_research": false
}
"""
```

**é—®é¢˜**:
- âœ… æœ‰ç¤ºä¾‹
- âŒ æ²¡æœ‰æ˜ç¡®ç¦æ­¢é¢å¤–æ–‡æœ¬
- âŒ æ²¡æœ‰è¯´æ˜è¾“å‡ºæ ¼å¼è¦æ±‚

**LLM å¯èƒ½è¿”å›**:
```
å¥½çš„ï¼Œæˆ‘æ¥è¯„ä¼°è¿™ä¸ªç»“æœã€‚

```json
{
  "accepted": true,
  "need_more_research": false
}
```

æ ¹æ®ä»¥ä¸Šåˆ†æ...
```

---

#### åŸå› 2: **LLM è‡ªä½œä¸»å¼ æ·»åŠ  Markdown æ ¼å¼**

LLM è¢«è®­ç»ƒä¸º"å‹å¥½åŠ©æ‰‹"ï¼Œä¼šè‡ªåŠ¨ç¾åŒ–è¾“å‡ºï¼š
- æ·»åŠ ä»£ç å— ` ```json ... ``` `
- æ·»åŠ è§£é‡Šæ€§æ–‡æœ¬
- æ·»åŠ æ¢è¡Œå’Œç¼©è¿›

**è¿™æ˜¯ LLM çš„é»˜è®¤è¡Œä¸ºï¼Œä¸æ˜¯ bugï¼**

---

#### åŸå› 3: **JSON æ ¼å¼æœ¬èº«ä¸è§„èŒƒ**

å³ä½¿ LLM è¿”å› JSONï¼Œä¹Ÿå¯èƒ½æœ‰é—®é¢˜ï¼š
- ä½¿ç”¨ Python é£æ ¼: `True` è€Œä¸æ˜¯ `true`
- ç¼ºå°‘å¼•å·: `{accepted: true}` è€Œä¸æ˜¯ `{"accepted": true}`
- å¤šä½™çš„é€—å·: `{"a": 1,}` ï¼ˆJavaScript å…è®¸ï¼ŒJSON ä¸å…è®¸ï¼‰

---

## âœ… çœŸæ­£çš„è§£å†³æ–¹æ¡ˆ

### æ–¹æ¡ˆ1: **æ”¹è¿› Prompt**ï¼ˆæ²»æ ‡ï¼‰

#### é”™è¯¯çš„ Prompt âŒ

```python
prompt = """
è¯·è¯„ä¼°ç»“æœå¹¶è¾“å‡º JSONï¼š
{
  "accepted": true,
  "comment": "..."
}
"""
```

**é—®é¢˜**: æ²¡æœ‰å¼ºåˆ¶çº¦æŸ

---

#### æ­£ç¡®çš„ Prompt âœ…

```python
prompt = """
è¯·è¯„ä¼°ç»“æœã€‚

âš ï¸ è¾“å‡ºè¦æ±‚ï¼š
1. åªè¾“å‡ºä¸€ä¸ª JSON å¯¹è±¡ï¼Œä¸è¦åŒ…å«ä»»ä½•å…¶ä»–æ–‡å­—
2. ä¸è¦ä½¿ç”¨ Markdown ä»£ç å—æ ‡è®°ï¼ˆä¸è¦ç”¨ ```ï¼‰
3. JSON å¿…é¡»ç¬¦åˆæ ‡å‡†æ ¼å¼ï¼ˆå­—æ®µåç”¨åŒå¼•å·ï¼Œå¸ƒå°”å€¼ç”¨å°å†™ true/falseï¼‰
4. ç¡®ä¿ JSON å¯ä»¥è¢« Python çš„ json.loads() ç›´æ¥è§£æ

ç¤ºä¾‹è¾“å‡ºï¼ˆç›´æ¥å¤åˆ¶è¿™ä¸ªæ ¼å¼ï¼‰ï¼š
{"accepted": true, "need_more_research": false, "new_aspects": [], "comment": "ç»“æœå¯ä¿¡"}

ç°åœ¨å¼€å§‹è¾“å‡ºï¼ˆåªè¾“å‡º JSONï¼Œä¸è¦æœ‰ä»»ä½•å…¶ä»–å†…å®¹ï¼‰ï¼š
"""
```

**æ”¹è¿›ç‚¹**:
1. âœ… æ˜ç¡®ç¦æ­¢é¢å¤–æ–‡æœ¬
2. âœ… æ˜ç¡®ç¦æ­¢ Markdown æ ‡è®°
3. âœ… è¯´æ˜ JSON æ ¼å¼è¦æ±‚
4. âœ… å¼ºè°ƒ"åªè¾“å‡º JSON"
5. âœ… æä¾›**å•è¡Œ**ç¤ºä¾‹ï¼ˆLLM æ›´å®¹æ˜“æ¨¡ä»¿ï¼‰

---

### æ–¹æ¡ˆ2: **ä½¿ç”¨ System Message**ï¼ˆæ›´å¼ºçº¦æŸï¼‰

```python
system_message = SystemMessage(content="""
ä½ æ˜¯ä¸€ä¸ª JSON ç”Ÿæˆå™¨ã€‚ä½ çš„è¾“å‡ºå¿…é¡»éµå®ˆä»¥ä¸‹è§„åˆ™ï¼š
1. åªè¾“å‡ºç¬¦åˆ JSON æ ‡å‡†çš„æ–‡æœ¬
2. ä¸è¦æ·»åŠ ä»»ä½•è§£é‡Šã€æ³¨é‡Šæˆ– Markdown æ ‡è®°
3. è¾“å‡ºå¿…é¡»èƒ½è¢« json.loads() ç›´æ¥è§£æ
4. å¸ƒå°”å€¼ä½¿ç”¨å°å†™ true/false
5. å­—æ®µåå¿…é¡»ç”¨åŒå¼•å·

è¿åä»¥ä¸Šè§„åˆ™çš„è¾“å‡ºæ˜¯ä¸å¯æ¥å—çš„ã€‚
""")

human_message = HumanMessage(content=prompt)

response = llm.invoke([system_message, human_message])
```

**åŸç†**: System Message æƒé‡æ›´é«˜ï¼ŒLLM æ›´ä¸¥æ ¼éµå®ˆ

---

### æ–¹æ¡ˆ3: **ä½¿ç”¨ JSON Mode**ï¼ˆOpenAI åŸç”Ÿæ”¯æŒï¼‰â­ æ¨è

```python
llm = ChatOpenAI(
    model="gpt-4o-mini",
    model_kwargs={"response_format": {"type": "json_object"}}  # âœ… JSON æ¨¡å¼
)

prompt = """
è¯·è¯„ä¼°ç»“æœï¼Œä»¥ JSON æ ¼å¼è¾“å‡ºï¼š
- accepted: å¸ƒå°”å€¼
- need_more_research: å¸ƒå°”å€¼
- new_aspects: å­—ç¬¦ä¸²æ•°ç»„
- comment: å­—ç¬¦ä¸²
"""

response = llm.invoke([HumanMessage(content=prompt)])
# response.content ä¿è¯æ˜¯åˆæ³•çš„ JSON âœ…
verdict = json.loads(response.content)
```

**ä¼˜ç‚¹**:
- âœ… OpenAI æ¨¡å‹åŸç”Ÿæ”¯æŒï¼ˆgpt-4, gpt-3.5-turbo ç­‰ï¼‰
- âœ… 100% ä¿è¯è¿”å›åˆæ³• JSON
- âœ… ä¸éœ€è¦é¢å¤–çš„è§£æé€»è¾‘
- âœ… æ— éœ€å¤æ‚çš„ Prompt å·¥ç¨‹

**ç¼ºç‚¹**:
- âš ï¸ ä»… OpenAI æ”¯æŒï¼ˆOllama æœ¬åœ°æ¨¡å‹ä¸æ”¯æŒï¼‰
- âš ï¸ å¿…é¡»åœ¨ Prompt ä¸­è¯´æ˜ JSON ç»“æ„

---

### æ–¹æ¡ˆ4: **ä½¿ç”¨ Structured Output**ï¼ˆLangChain å°è£…ï¼‰â­ æœ€ä½³

è¿™å°±æ˜¯æˆ‘ä¹‹å‰æ¨èçš„æ–¹æ¡ˆï¼Œä½†å®ƒç¡®å®æ˜¯**ä»æ ¹æœ¬ä¸Šè§£å†³é—®é¢˜**ï¼š

```python
from pydantic import BaseModel

class ReflectionVerdict(BaseModel):
    accepted: bool
    need_more_research: bool
    new_aspects: list[str]
    comment: str

# âœ… LangChain ä¼šè‡ªåŠ¨ï¼š
# 1. å¯ç”¨ JSON Mode
# 2. åœ¨ Prompt ä¸­æ³¨å…¥ JSON Schema
# 3. è§£æå¹¶éªŒè¯ JSON
# 4. è¿”å› Pydantic å¯¹è±¡
structured_llm = llm.with_structured_output(ReflectionVerdict)

verdict: ReflectionVerdict = structured_llm.invoke([HumanMessage(content=prompt)])
# verdict æ˜¯ Pydantic å¯¹è±¡ï¼Œå­—æ®µéªŒè¯è‡ªåŠ¨å®Œæˆ âœ…
```

**ä¸ºä»€ä¹ˆè¿™æ˜¯æ ¹æœ¬è§£å†³æ–¹æ¡ˆï¼Ÿ**

1. **LLM å±‚é¢**: å¯ç”¨ `response_format=json_object`ï¼Œå¼ºåˆ¶ JSON è¾“å‡º
2. **Prompt å±‚é¢**: è‡ªåŠ¨æ³¨å…¥ JSON Schemaï¼Œæ˜ç¡®ç»“æ„
3. **è§£æå±‚é¢**: è‡ªåŠ¨éªŒè¯å­—æ®µç±»å‹ã€å¿…å¡«é¡¹
4. **ä»£ç å±‚é¢**: ç±»å‹å®‰å…¨ï¼ŒIDE è‡ªåŠ¨è¡¥å…¨

**è¿™ä¸æ˜¯"ç»•è¿‡"ï¼Œè€Œæ˜¯åˆ©ç”¨ LLM çš„åŸç”Ÿèƒ½åŠ›è§£å†³é—®é¢˜ï¼**

---

## ğŸ“Š æ–¹æ¡ˆå¯¹æ¯”

| æ–¹æ¡ˆ | æˆåŠŸç‡ | é€‚ç”¨æ¨¡å‹ | å¤æ‚åº¦ | æ˜¯å¦æ²»æœ¬ |
|------|--------|---------|--------|---------|
| **æ–¹æ¡ˆ1: æ”¹è¿› Prompt** | 70% | æ‰€æœ‰ | ä½ | âŒ æ²»æ ‡ |
| **æ–¹æ¡ˆ2: System Message** | 80% | æ‰€æœ‰ | ä½ | âŒ æ²»æ ‡ |
| **æ–¹æ¡ˆ3: JSON Mode** | 99% | OpenAI | ä½ | âœ… **æ²»æœ¬** |
| **æ–¹æ¡ˆ4: Structured Output** | 99% | OpenAI | ä¸­ | âœ… **æ²»æœ¬** |

---

## ğŸ¯ æ¨èæ–¹æ¡ˆ

### **å¦‚æœä½¿ç”¨ OpenAI æ¨¡å‹** â­ æ¨è

**ä½¿ç”¨ JSON Modeï¼ˆæ–¹æ¡ˆ3ï¼‰æˆ– Structured Outputï¼ˆæ–¹æ¡ˆ4ï¼‰**

```python
# æ–¹æ¡ˆ3: JSON Mode
llm = ChatOpenAI(
    model="gpt-4o-mini",
    model_kwargs={"response_format": {"type": "json_object"}}
)

# æ–¹æ¡ˆ4: Structured Outputï¼ˆæ›´æ¨èï¼‰
structured_llm = llm.with_structured_output(ReflectionVerdict)
```

**åŸå› **:
- âœ… åˆ©ç”¨ OpenAI çš„åŸç”Ÿ JSON èƒ½åŠ›
- âœ… 99% æˆåŠŸç‡
- âœ… ä»æ ¹æœ¬ä¸Šè§£å†³é—®é¢˜

---

### **å¦‚æœä½¿ç”¨ Ollama æœ¬åœ°æ¨¡å‹**

**æ–¹æ¡ˆ1 + æ–¹æ¡ˆ2 ç»„åˆ**

```python
system_message = SystemMessage(content="""
ä½ æ˜¯ JSON ç”Ÿæˆå™¨ã€‚åªè¾“å‡ºç¬¦åˆæ ‡å‡†çš„ JSONï¼Œä¸è¦æ·»åŠ ä»»ä½•å…¶ä»–å†…å®¹ã€‚
""")

prompt = """
è¯·è¯„ä¼°ç»“æœã€‚

è¾“å‡ºè¦æ±‚ï¼š
1. åªè¾“å‡ºä¸€ä¸ª JSON å¯¹è±¡
2. ä¸è¦ä½¿ç”¨ Markdown ä»£ç å—
3. æ ¼å¼ç¤ºä¾‹ï¼š{"accepted": true, "comment": "..."}

ç°åœ¨å¼€å§‹è¾“å‡ºï¼š
"""

response = llm.invoke([system_message, HumanMessage(content=prompt)])

# æ·»åŠ å®¹é”™è§£æ
content = response.content.strip()
# å°è¯•æå– JSONï¼ˆä»¥é˜²ä¸‡ä¸€ï¼‰
if content.startswith("```"):
    # å»é™¤ Markdown æ ‡è®°
    content = re.sub(r'```(?:json)?\s*\n?|\n?```', '', content).strip()

verdict = json.loads(content)
```

---

## ğŸ’¡ æ ¹æœ¬åŸå› æ€»ç»“

| åŸå›  | æ˜¯å¦å¯æ§ | è§£å†³æ–¹æ³• |
|------|---------|---------|
| **LLM æ·»åŠ é¢å¤–æ–‡æœ¬** | âœ… å¯æ§ | æ”¹è¿› Prompt |
| **LLM æ·»åŠ  Markdown æ ‡è®°** | âœ… å¯æ§ | System Message çº¦æŸ |
| **JSON æ ¼å¼ä¸è§„èŒƒ** | âœ… å¯æ§ | JSON Mode å¼ºåˆ¶ |
| **LLM ç†è§£é”™è¯¯** | âš ï¸ éƒ¨åˆ†å¯æ§ | æä¾›æ˜ç¡®ç¤ºä¾‹ |

**çœŸæ­£çš„æ ¹æœ¬è§£å†³æ–¹æ¡ˆ**:

1. **OpenAI æ¨¡å‹**: ä½¿ç”¨ `response_format=json_object` æˆ– `with_structured_output()`
2. **å…¶ä»–æ¨¡å‹**: ä¸¥æ ¼çš„ Prompt + System Message + å®¹é”™è§£æ

---

## ğŸš€ ç«‹å³è¡ŒåŠ¨

### **ä¿®å¤ä½ çš„ä»£ç **ï¼ˆä½¿ç”¨ JSON Modeï¼‰

```python
def get_llm(model: Optional[str] = None, temperature: float = 0.2, json_mode: bool = False) -> object:
    """è·å– LLM å®ä¾‹"""
    provider = os.getenv("LLM_PROVIDER", "").lower()
    use_ollama = (provider in {"ollama", "local"}) and not os.getenv("OPENAI_API_KEY")

    if use_ollama:
        # Ollama ä¸æ”¯æŒ JSON Mode
        # ... åŸæœ‰é€»è¾‘
        return ChatOllama(...)
    else:
        # OpenAI æ”¯æŒ JSON Mode
        model_kwargs = {}
        if json_mode:
            model_kwargs["response_format"] = {"type": "json_object"}  # âœ… å¯ç”¨ JSON æ¨¡å¼

        return ChatOpenAI(
            model=model_name,
            model_kwargs=model_kwargs,  # âœ… ä¼ é€’é…ç½®
            # ... å…¶ä»–å‚æ•°
        )


def lead_reflection_node(state: ClaudeCodeState) -> ClaudeCodeState:
    llm = get_llm(json_mode=True)  # âœ… å¯ç”¨ JSON æ¨¡å¼

    prompt = """
è¯·è¯„ä¼°ç ”ç©¶ç»“æœã€‚ä»¥ JSON æ ¼å¼è¾“å‡ºï¼ŒåŒ…å«ä»¥ä¸‹å­—æ®µï¼š
- accepted: å¸ƒå°”å€¼ï¼ˆæ˜¯å¦æ¥å—ï¼‰
- need_more_research: å¸ƒå°”å€¼ï¼ˆæ˜¯å¦éœ€è¦æ›´å¤šç ”ç©¶ï¼‰
- new_aspects: å­—ç¬¦ä¸²æ•°ç»„ï¼ˆæ–°ç ”ç©¶æ–¹é¢ï¼Œå¯ä¸ºç©ºï¼‰
- comment: å­—ç¬¦ä¸²ï¼ˆè¯„ä¼°æ„è§ï¼‰
"""

    response = llm.invoke([HumanMessage(content=prompt)])
    verdict = json.loads(response.content)  # âœ… 100% æˆåŠŸ
    # ...
```

è¿™æ ·ä¿®æ”¹åï¼Œ**ä»æ ¹æœ¬ä¸Šè§£å†³äº† JSON è§£æé—®é¢˜**ï¼
