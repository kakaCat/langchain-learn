#!/usr/bin/env python3
"""
é«˜çº§ RAG æŠ€æœ¯ Demo
å®ç°æ£€ç´¢å¢å¼ºç”Ÿæˆçš„é«˜çº§ä¼˜åŒ–æŠ€æœ¯
"""

import json
import time
import hashlib
from typing import List, Dict, Any, Optional, Tuple, Callable
from dataclasses import dataclass, field
from enum import Enum
import numpy as np
from collections import defaultdict, deque
import re
from datetime import datetime, timedelta


class DocumentType(Enum):
    """æ–‡æ¡£ç±»å‹æšä¸¾"""
    TEXT = "text"
    CODE = "code"
    MARKDOWN = "markdown"
    PDF = "pdf"
    HTML = "html"


class RetrievalStrategy(Enum):
    """æ£€ç´¢ç­–ç•¥æšä¸¾"""
    SEMANTIC = "semantic"  # è¯­ä¹‰æ£€ç´¢
    KEYWORD = "keyword"    # å…³é”®è¯æ£€ç´¢
    HYBRID = "hybrid"      # æ··åˆæ£€ç´¢
    MULTI_HOP = "multi_hop" # å¤šè·³æ£€ç´¢


class ChunkingStrategy(Enum):
    """åˆ†å—ç­–ç•¥æšä¸¾"""
    FIXED_SIZE = "fixed_size"      # å›ºå®šå¤§å°åˆ†å—
    SENTENCE = "sentence"          # æŒ‰å¥å­åˆ†å—
    PARAGRAPH = "paragraph"        # æŒ‰æ®µè½åˆ†å—
    SEMANTIC = "semantic"          # è¯­ä¹‰åˆ†å—
    OVERLAP = "overlap"            # é‡å åˆ†å—


class EmbeddingModel:
    """åµŒå…¥æ¨¡å‹æ¨¡æ‹Ÿç±»"""
    
    def __init__(self, model_name: str = "text-embedding-ada-002"):
        self.model_name = model_name
        self.dimension = 1536
    
    def embed(self, text: str) -> List[float]:
        """ç”Ÿæˆæ–‡æœ¬åµŒå…¥å‘é‡"""
        # æ¨¡æ‹ŸåµŒå…¥ç”Ÿæˆ
        np.random.seed(hash(text) % 2**32)
        return list(np.random.randn(self.dimension))
    
    def similarity(self, vec1: List[float], vec2: List[float]) -> float:
        """è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦"""
        vec1 = np.array(vec1)
        vec2 = np.array(vec2)
        return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))


class DocumentChunk:
    """æ–‡æ¡£åˆ†å—"""
    
    def __init__(self, content: str, doc_id: str, chunk_id: str, 
                 chunk_index: int, metadata: Dict[str, Any] = None):
        self.content = content
        self.doc_id = doc_id
        self.chunk_id = chunk_id
        self.chunk_index = chunk_index
        self.metadata = metadata or {}
        self.embedding = None
        self.score = 0.0
        
    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸"""
        return {
            "content": self.content,
            "doc_id": self.doc_id,
            "chunk_id": self.chunk_id,
            "chunk_index": self.chunk_index,
            "metadata": self.metadata,
            "score": self.score
        }


class DocumentProcessor:
    """æ–‡æ¡£å¤„ç†å™¨"""
    
    def __init__(self):
        self.embedding_model = EmbeddingModel()
    
    def chunk_document(self, content: str, doc_id: str, 
                      strategy: ChunkingStrategy = ChunkingStrategy.FIXED_SIZE,
                      chunk_size: int = 500, overlap: int = 50) -> List[DocumentChunk]:
        """æ–‡æ¡£åˆ†å—å¤„ç†"""
        chunks = []
        
        if strategy == ChunkingStrategy.FIXED_SIZE:
            # å›ºå®šå¤§å°åˆ†å—
            for i in range(0, len(content), chunk_size - overlap):
                chunk_content = content[i:i + chunk_size]
                if chunk_content.strip():
                    chunk_id = f"{doc_id}_chunk_{i//chunk_size}"
                    chunk = DocumentChunk(
                        content=chunk_content,
                        doc_id=doc_id,
                        chunk_id=chunk_id,
                        chunk_index=i//chunk_size,
                        metadata={
                            "strategy": strategy.value,
                            "chunk_size": chunk_size,
                            "overlap": overlap
                        }
                    )
                    chunks.append(chunk)
        
        elif strategy == ChunkingStrategy.SENTENCE:
            # æŒ‰å¥å­åˆ†å—
            sentences = re.split(r'[.!?ã€‚ï¼ï¼Ÿ]+', content)
            sentences = [s.strip() for s in sentences if s.strip()]
            
            for i, sentence in enumerate(sentences):
                chunk_id = f"{doc_id}_sentence_{i}"
                chunk = DocumentChunk(
                    content=sentence,
                    doc_id=doc_id,
                    chunk_id=chunk_id,
                    chunk_index=i,
                    metadata={
                        "strategy": strategy.value,
                        "sentence_index": i
                    }
                )
                chunks.append(chunk)
        
        return chunks
    
    def generate_embeddings(self, chunks: List[DocumentChunk]) -> List[DocumentChunk]:
        """ä¸ºåˆ†å—ç”ŸæˆåµŒå…¥å‘é‡"""
        for chunk in chunks:
            chunk.embedding = self.embedding_model.embed(chunk.content)
        return chunks


class VectorStore:
    """å‘é‡å­˜å‚¨"""
    
    def __init__(self):
        self.chunks: Dict[str, DocumentChunk] = {}
        self.embeddings: List[List[float]] = []
        self.chunk_ids: List[str] = []
        self.access_stats = defaultdict(int)
        self.last_access = {}
    
    def add_chunks(self, chunks: List[DocumentChunk]):
        """æ·»åŠ åˆ†å—åˆ°å‘é‡å­˜å‚¨"""
        for chunk in chunks:
            if chunk.chunk_id not in self.chunks:
                self.chunks[chunk.chunk_id] = chunk
                if chunk.embedding:
                    self.embeddings.append(chunk.embedding)
                    self.chunk_ids.append(chunk.chunk_id)
    
    def semantic_search(self, query: str, top_k: int = 5, 
                       strategy: RetrievalStrategy = RetrievalStrategy.SEMANTIC) -> List[DocumentChunk]:
        """è¯­ä¹‰æœç´¢"""
        embedding_model = EmbeddingModel()
        query_embedding = embedding_model.embed(query)
        
        # è®¡ç®—ç›¸ä¼¼åº¦
        similarities = []
        for i, chunk_embedding in enumerate(self.embeddings):
            similarity = embedding_model.similarity(query_embedding, chunk_embedding)
            similarities.append((similarity, self.chunk_ids[i]))
        
        # æ’åºå¹¶è¿”å›top_k
        similarities.sort(reverse=True)
        
        results = []
        for similarity, chunk_id in similarities[:top_k]:
            chunk = self.chunks[chunk_id]
            chunk.score = similarity
            
            # æ›´æ–°è®¿é—®ç»Ÿè®¡
            self.access_stats[chunk_id] += 1
            self.last_access[chunk_id] = datetime.now()
            
            results.append(chunk)
        
        return results
    
    def keyword_search(self, query: str, top_k: int = 5) -> List[DocumentChunk]:
        """å…³é”®è¯æœç´¢"""
        query_words = set(query.lower().split())
        
        scores = []
        for chunk_id, chunk in self.chunks.items():
            content_words = set(chunk.content.lower().split())
            
            # è®¡ç®—å…³é”®è¯åŒ¹é…åº¦
            intersection = query_words.intersection(content_words)
            score = len(intersection) / len(query_words) if query_words else 0
            
            scores.append((score, chunk_id))
        
        # æ’åºå¹¶è¿”å›top_k
        scores.sort(reverse=True)
        
        results = []
        for score, chunk_id in scores[:top_k]:
            chunk = self.chunks[chunk_id]
            chunk.score = score
            results.append(chunk)
        
        return results
    
    def hybrid_search(self, query: str, top_k: int = 5, 
                     semantic_weight: float = 0.7) -> List[DocumentChunk]:
        """æ··åˆæœç´¢"""
        semantic_results = self.semantic_search(query, top_k * 2)
        keyword_results = self.keyword_search(query, top_k * 2)
        
        # åˆå¹¶ç»“æœå¹¶é‡æ–°æ’åº
        all_results = {}
        
        for chunk in semantic_results:
            all_results[chunk.chunk_id] = {
                "chunk": chunk,
                "semantic_score": chunk.score,
                "keyword_score": 0.0
            }
        
        for chunk in keyword_results:
            if chunk.chunk_id in all_results:
                all_results[chunk.chunk_id]["keyword_score"] = chunk.score
            else:
                all_results[chunk.chunk_id] = {
                    "chunk": chunk,
                    "semantic_score": 0.0,
                    "keyword_score": chunk.score
                }
        
        # è®¡ç®—ç»¼åˆåˆ†æ•°
        scored_results = []
        for data in all_results.values():
            combined_score = (data["semantic_score"] * semantic_weight + 
                            data["keyword_score"] * (1 - semantic_weight))
            data["chunk"].score = combined_score
            scored_results.append((combined_score, data["chunk"]))
        
        # æ’åºå¹¶è¿”å›top_k
        scored_results.sort(reverse=True)
        return [chunk for _, chunk in scored_results[:top_k]]


class Reranker:
    """é‡æ’åºå™¨"""
    
    @staticmethod
    def diversity_rerank(chunks: List[DocumentChunk], top_k: int = 5) -> List[DocumentChunk]:
        """å¤šæ ·æ€§é‡æ’åº"""
        if not chunks:
            return []
        
        # ç®€å•å¤šæ ·æ€§é‡æ’åºï¼šé€‰æ‹©ä¸åŒæ–‡æ¡£çš„åˆ†å—
        selected = []
        doc_seen = set()
        
        for chunk in chunks:
            if chunk.doc_id not in doc_seen:
                selected.append(chunk)
                doc_seen.add(chunk.doc_id)
            
            if len(selected) >= top_k:
                break
        
        # å¦‚æœå¤šæ ·æ€§ä¸è¶³ï¼Œè¡¥å……é«˜åˆ†åˆ†å—
        if len(selected) < top_k:
            for chunk in chunks:
                if chunk not in selected:
                    selected.append(chunk)
                if len(selected) >= top_k:
                    break
        
        return selected
    
    @staticmethod
    def recency_rerank(chunks: List[DocumentChunk], vector_store: VectorStore, 
                      top_k: int = 5) -> List[DocumentChunk]:
        """æ—¶æ•ˆæ€§é‡æ’åº"""
        # æ ¹æ®æœ€åè®¿é—®æ—¶é—´æ’åº
        chunks_with_time = []
        for chunk in chunks:
            last_access = vector_store.last_access.get(chunk.chunk_id, datetime.min)
            chunks_with_time.append((last_access, chunk))
        
        # æŒ‰æ—¶é—´å€’åºæ’åˆ—
        chunks_with_time.sort(reverse=True)
        return [chunk for _, chunk in chunks_with_time[:top_k]]


class QueryExpander:
    """æŸ¥è¯¢æ‰©å±•å™¨"""
    
    @staticmethod
    def synonym_expansion(query: str) -> List[str]:
        """åŒä¹‰è¯æ‰©å±•"""
        synonyms = {
            "AI": ["äººå·¥æ™ºèƒ½", "æœºå™¨å­¦ä¹ ", "æ·±åº¦å­¦ä¹ "],
            "å¼€å‘": ["ç¼–ç¨‹", "ç¼–å†™", "åˆ›å»º"],
            "æ¡†æ¶": ["åº“", "å·¥å…·åŒ…", "å¹³å°"],
            "å­¦ä¹ ": ["æŒæ¡", "äº†è§£", "ç ”ç©¶"],
            "æŠ€æœ¯": ["æ–¹æ³•", "æŠ€å·§", "æ‰‹æ®µ"]
        }
        
        expanded_queries = [query]
        words = query.split()
        
        for i, word in enumerate(words):
            if word in synonyms:
                for synonym in synonyms[word]:
                    new_words = words.copy()
                    new_words[i] = synonym
                    expanded_queries.append(' '.join(new_words))
        
        return expanded_queries
    
    @staticmethod
    def contextual_expansion(query: str, context_chunks: List[DocumentChunk]) -> str:
        """ä¸Šä¸‹æ–‡æ‰©å±•"""
        if not context_chunks:
            return query
        
        # ä»ç›¸å…³åˆ†å—ä¸­æå–å…³é”®è¯
        all_content = ' '.join(chunk.content for chunk in context_chunks[:3])
        words = re.findall(r'\b\w+\b', all_content.lower())
        
        # ç»Ÿè®¡è¯é¢‘
        word_freq = defaultdict(int)
        for word in words:
            if len(word) > 2:  # è¿‡æ»¤çŸ­è¯
                word_freq[word] += 1
        
        # é€‰æ‹©é«˜é¢‘è¯
        top_words = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)[:3]
        expanded_query = query
        
        for word, _ in top_words:
            if word not in query.lower():
                expanded_query += f" {word}"
        
        return expanded_query


class AdvancedRAGSystem:
    """é«˜çº§ RAG ç³»ç»Ÿ"""
    
    def __init__(self):
        self.document_processor = DocumentProcessor()
        self.vector_store = VectorStore()
        self.reranker = Reranker()
        self.query_expander = QueryExpander()
        self.query_history = deque(maxlen=100)
    
    def add_document(self, content: str, doc_id: str, 
                    chunking_strategy: ChunkingStrategy = ChunkingStrategy.FIXED_SIZE):
        """æ·»åŠ æ–‡æ¡£åˆ°ç³»ç»Ÿ"""
        print(f"ğŸ“„ å¤„ç†æ–‡æ¡£: {doc_id}")
        
        # æ–‡æ¡£åˆ†å—
        chunks = self.document_processor.chunk_document(
            content, doc_id, chunking_strategy
        )
        print(f"  ç”Ÿæˆåˆ†å—: {len(chunks)} ä¸ª")
        
        # ç”ŸæˆåµŒå…¥
        chunks_with_embeddings = self.document_processor.generate_embeddings(chunks)
        print(f"  ç”ŸæˆåµŒå…¥: å®Œæˆ")
        
        # æ·»åŠ åˆ°å‘é‡å­˜å‚¨
        self.vector_store.add_chunks(chunks_with_embeddings)
        print(f"  å­˜å‚¨å®Œæˆ: æ€»å…±æœ‰ {len(self.vector_store.chunks)} ä¸ªåˆ†å—")
    
    def search(self, query: str, strategy: RetrievalStrategy = RetrievalStrategy.HYBRID,
              top_k: int = 5, use_reranking: bool = True, 
              use_query_expansion: bool = True) -> Dict[str, Any]:
        """é«˜çº§æœç´¢"""
        print(f"ğŸ” æ‰§è¡Œæœç´¢: {query}")
        print(f"  ç­–ç•¥: {strategy.value}, top_k: {top_k}")
        
        # è®°å½•æŸ¥è¯¢å†å²
        self.query_history.append({
            "query": query,
            "timestamp": datetime.now().isoformat(),
            "strategy": strategy.value
        })
        
        # æŸ¥è¯¢æ‰©å±•
        expanded_query = query
        if use_query_expansion and strategy != RetrievalStrategy.KEYWORD:
            expanded_queries = self.query_expander.synonym_expansion(query)
            expanded_query = expanded_queries[0]  # ä½¿ç”¨ç¬¬ä¸€ä¸ªæ‰©å±•æŸ¥è¯¢
            print(f"  æŸ¥è¯¢æ‰©å±•: {expanded_query}")
        
        # æ‰§è¡Œæœç´¢
        if strategy == RetrievalStrategy.SEMANTIC:
            results = self.vector_store.semantic_search(expanded_query, top_k * 2)
        elif strategy == RetrievalStrategy.KEYWORD:
            results = self.vector_store.keyword_search(expanded_query, top_k * 2)
        else:  # HYBRID or MULTI_HOP
            results = self.vector_store.hybrid_search(expanded_query, top_k * 2)
        
        print(f"  åˆæ­¥ç»“æœ: {len(results)} ä¸ªåˆ†å—")
        
        # é‡æ’åº
        if use_reranking and results:
            results = self.reranker.diversity_rerank(results, top_k)
            print(f"  é‡æ’åºå: {len(results)} ä¸ªåˆ†å—")
        
        # å‡†å¤‡è¿”å›ç»“æœ
        result_chunks = results[:top_k]
        
        return {
            "query": query,
            "expanded_query": expanded_query,
            "strategy": strategy.value,
            "results": [chunk.to_dict() for chunk in result_chunks],
            "total_results": len(result_chunks),
            "search_time": datetime.now().isoformat()
        }
    
    def multi_hop_search(self, query: str, max_hops: int = 3, 
                        top_k_per_hop: int = 3) -> Dict[str, Any]:
        """å¤šè·³æ£€ç´¢"""
        print(f"ğŸ”„ æ‰§è¡Œå¤šè·³æ£€ç´¢: {query}")
        
        current_query = query
        all_results = []
        hop_results = []
        
        for hop in range(max_hops):
            print(f"  ç¬¬ {hop + 1} è·³æ£€ç´¢...")
            
            # æ‰§è¡Œæœç´¢
            results = self.search(
                current_query, 
                RetrievalStrategy.HYBRID, 
                top_k_per_hop, 
                use_reranking=True,
                use_query_expansion=True
            )
            
            hop_results.append({
                "hop": hop + 1,
                "query": current_query,
                "results": results["results"]
            })
            
            # æ”¶é›†ç»“æœ
            all_results.extend(results["results"])
            
            # ç”Ÿæˆä¸‹ä¸€è·³æŸ¥è¯¢
            if hop < max_hops - 1 and results["results"]:
                # ä»ç»“æœä¸­æå–ä¿¡æ¯ç”Ÿæˆæ–°æŸ¥è¯¢
                context_content = ' '.join(
                    result["content"] for result in results["results"][:2]
                )
                current_query = self.query_expander.contextual_expansion(
                    query, 
                    [DocumentChunk(**result) for result in results["results"][:2]]
                )
                print(f"  ä¸‹ä¸€è·³æŸ¥è¯¢: {current_query}")
            else:
                break
        
        # å»é‡å’Œæ’åº
        unique_results = {}
        for result in all_results:
            if result["chunk_id"] not in unique_results:
                unique_results[result["chunk_id"]] = result
            elif result["score"] > unique_results[result["chunk_id"]]["score"]:
                unique_results[result["chunk_id"]] = result
        
        final_results = sorted(
            unique_results.values(), 
            key=lambda x: x["score"], 
            reverse=True
        )[:top_k_per_hop * max_hops]
        
        return {
            "original_query": query,
            "max_hops": max_hops,
            "hop_results": hop_results,
            "final_results": final_results,
            "total_unique_results": len(final_results)
        }
    
    def get_system_stats(self) -> Dict[str, Any]:
        """è·å–ç³»ç»Ÿç»Ÿè®¡ä¿¡æ¯"""
        return {
            "total_chunks": len(self.vector_store.chunks),
            "total_documents": len(set(chunk.doc_id for chunk in self.vector_store.chunks.values())),
            "total_queries": len(self.query_history),
            "most_accessed_chunks": sorted(
                self.vector_store.access_stats.items(), 
                key=lambda x: x[1], 
                reverse=True
            )[:5],
            "recent_queries": list(self.query_history)[-5:]
        }


def demo_advanced_rag():
    """æ¼”ç¤ºé«˜çº§ RAG åŠŸèƒ½"""
    print("ğŸš€ å¯åŠ¨é«˜çº§ RAG ç³»ç»Ÿæ¼”ç¤º")
    print("=" * 50)
    
    # åˆ›å»ºç³»ç»Ÿå®ä¾‹
    rag_system = AdvancedRAGSystem()
    
    # æ·»åŠ ç¤ºä¾‹æ–‡æ¡£
    sample_documents = [
        {
            "id": "doc_ai_intro",
            "content": """
äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œæ—¨åœ¨åˆ›é€ èƒ½å¤Ÿæ‰§è¡Œé€šå¸¸éœ€è¦äººç±»æ™ºèƒ½çš„ä»»åŠ¡çš„æœºå™¨ã€‚
æœºå™¨å­¦ä¹ æ˜¯AIçš„ä¸€ä¸ªå­é›†ï¼Œå®ƒä½¿è®¡ç®—æœºèƒ½å¤Ÿåœ¨æ²¡æœ‰æ˜ç¡®ç¼–ç¨‹çš„æƒ…å†µä¸‹å­¦ä¹ å’Œæ”¹è¿›ã€‚
æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªå­é¢†åŸŸï¼Œä½¿ç”¨ç¥ç»ç½‘ç»œæ¨¡æ‹Ÿäººè„‘çš„å·¥ä½œæ–¹å¼ã€‚
            """
        },
        {
            "id": "doc_rag_tech", 
            "content": """
æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æ˜¯ä¸€ç§ç»“åˆæ£€ç´¢ç³»ç»Ÿå’Œç”Ÿæˆæ¨¡å‹çš„æŠ€æœ¯ã€‚
RAGç³»ç»Ÿé¦–å…ˆä»çŸ¥è¯†åº“ä¸­æ£€ç´¢ç›¸å…³ä¿¡æ¯ï¼Œç„¶åä½¿ç”¨è¿™äº›ä¿¡æ¯æ¥ç”Ÿæˆæ›´å‡†ç¡®å’Œç›¸å…³çš„å›ç­”ã€‚
é«˜çº§RAGæŠ€æœ¯åŒ…æ‹¬å¤šè·³æ£€ç´¢ã€æŸ¥è¯¢æ‰©å±•ã€é‡æ’åºç­‰ä¼˜åŒ–æ–¹æ³•ã€‚
            """
        },
        {
            "id": "doc_langchain",
            "content": """
LangChainæ˜¯ä¸€ä¸ªç”¨äºå¼€å‘ç”±è¯­è¨€æ¨¡å‹é©±åŠ¨çš„åº”ç”¨ç¨‹åºçš„æ¡†æ¶ã€‚
å®ƒæä¾›äº†é“¾å¼è°ƒç”¨ã€ä»£ç†ã€è®°å¿†ç­‰ç»„ä»¶ï¼Œç®€åŒ–äº†å¤æ‚AIåº”ç”¨çš„å¼€å‘ã€‚
LangChainæ”¯æŒå¤šç§è¯­è¨€æ¨¡å‹å’Œå‘é‡æ•°æ®åº“çš„é›†æˆã€‚
            """
        }
    ]
    
    # å¤„ç†æ–‡æ¡£
    for doc in sample_documents:
        rag_system.add_document(doc["content"], doc["id"])
    
    print("\nğŸ“Š ç³»ç»Ÿç»Ÿè®¡ä¿¡æ¯:")
    stats = rag_system.get_system_stats()
    print(f"   æ€»æ–‡æ¡£æ•°: {stats['total_documents']}")
    print(f"   æ€»åˆ†å—æ•°: {stats['total_chunks']}")
    
    print("\nğŸ” æ¼”ç¤ºä¸åŒæ£€ç´¢ç­–ç•¥:")
    
    # è¯­ä¹‰æ£€ç´¢æ¼”ç¤º
    print("\n1. è¯­ä¹‰æ£€ç´¢:")
    semantic_results = rag_system.search(
        "ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½", 
        RetrievalStrategy.SEMANTIC,
        top_k=3
    )
    print(f"   æ‰¾åˆ° {semantic_results['total_results']} ä¸ªç›¸å…³ç»“æœ")
    for i, result in enumerate(semantic_results["results"][:2], 1):
        print(f"   {i}. åˆ†æ•°: {result['score']:.3f}")
        print(f"      å†…å®¹: {result['content'][:100]}...")
    
    # æ··åˆæ£€ç´¢æ¼”ç¤º
    print("\n2. æ··åˆæ£€ç´¢:")
    hybrid_results = rag_system.search(
        "RAGæŠ€æœ¯çš„å·¥ä½œåŸç†", 
        RetrievalStrategy.HYBRID,
        top_k=3
    )
    print(f"   æ‰¾åˆ° {hybrid_results['total_results']} ä¸ªç›¸å…³ç»“æœ")
    for i, result in enumerate(hybrid_results["results"][:2], 1):
        print(f"   {i}. åˆ†æ•°: {result['score']:.3f}")
        print(f"      å†…å®¹: {result['content'][:100]}...")
    
    # å¤šè·³æ£€ç´¢æ¼”ç¤º
    print("\n3. å¤šè·³æ£€ç´¢:")
    multi_hop_results = rag_system.multi_hop_search(
        "AIå’ŒRAGçš„å…³ç³»",
        max_hops=2,
        top_k_per_hop=2
    )
    print(f"   ç»è¿‡ {len(multi_hop_results['hop_results'])} è·³æ£€ç´¢")
    print(f"   æ‰¾åˆ° {multi_hop_results['total_unique_results']} ä¸ªå”¯ä¸€ç»“æœ")
    
    for hop in multi_hop_results["hop_results"]:
        print(f"   ç¬¬{hop['hop']}è·³æŸ¥è¯¢: {hop['query']}")
    
    print("\nâœ… é«˜çº§ RAG æ¼”ç¤ºå®Œæˆ!")


if __name__ == "__main__":
    demo_advanced_rag()